{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "## Predict whether a mammogram mass is benign or malignant\n",
    "\n",
    "We'll be using the \"mammographic masses\" public dataset from the UCI repository (source: https://archive.ics.uci.edu/ml/datasets/Mammographic+Mass)\n",
    "\n",
    "This data contains 961 instances of masses detected in mammograms, and contains the following attributes:\n",
    "\n",
    "\n",
    "   1. BI-RADS assessment: 1 to 5 (ordinal)  \n",
    "   2. Age: patient's age in years (integer)\n",
    "   3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)\n",
    "   4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)\n",
    "   5. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)\n",
    "   6. Severity: benign=0 or malignant=1 (binominal)\n",
    "   \n",
    "BI-RADS is an assesment of how confident the severity classification is; it is not a \"predictive\" attribute and so we will discard it. The age, shape, margin, and density attributes are the features that we will build our model with, and \"severity\" is the classification we will attempt to predict based on those attributes.\n",
    "\n",
    "Although \"shape\" and \"margin\" are nominal data types, which sklearn typically doesn't deal with well, they are close enough to ordinal that we shouldn't just discard them. The \"shape\" for example is ordered increasingly from round to irregular.\n",
    "\n",
    "A lot of unnecessary anguish and surgery arises from false positives arising from mammogram results. If we can build a better way to interpret them through supervised machine learning, it could improve a lot of lives.\n",
    "\n",
    "## Your assignment\n",
    "\n",
    "Apply several different supervised machine learning techniques to this data set, and see which one yields the highest accuracy as measured with K-Fold cross validation (K=10). Apply:\n",
    "\n",
    "* Decision tree\n",
    "* Random forest\n",
    "* KNN\n",
    "* Naive Bayes\n",
    "* SVM\n",
    "* Logistic Regression\n",
    "* And, as a bonus challenge, a neural network using Keras.\n",
    "\n",
    "The data needs to be cleaned; many rows contain missing data, and there may be erroneous data identifiable as outliers as well.\n",
    "\n",
    "Remember some techniques such as SVM also require the input data to be normalized first.\n",
    "\n",
    "Many techniques also have \"hyperparameters\" that need to be tuned. Once you identify a promising approach, see if you can make it even better by tuning its hyperparameters.\n",
    "\n",
    "I was able to achieve over 80% accuracy - can you beat that?\n",
    "\n",
    "Below I've set up an outline of a notebook for this project, with some guidance and hints. If you're up for a real challenge, try doing this project from scratch in a new, clean notebook!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's begin: prepare your data\n",
    "\n",
    "Start by importing the mammographic_masses.data.txt file into a Pandas dataframe (hint: use read_csv) and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you use the optional parmaters in read_csv to convert missing data (indicated by a ?) into NaN, and to add the appropriate column names (BI_RADS, age, shape, margin, density, and severity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bi_rads</th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bi_rads   age  shape  margin  density  severity\n",
       "0      5.0  67.0    3.0     5.0      3.0         1\n",
       "1      4.0  43.0    1.0     1.0      NaN         1\n",
       "2      5.0  58.0    4.0     5.0      3.0         1\n",
       "3      4.0  28.0    1.0     1.0      3.0         0\n",
       "4      5.0  74.0    1.0     5.0      NaN         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"mammographic_masses.data.txt\",\n",
    "                 na_values=\"?\",\n",
    "                 names=(\"bi_rads\", \"age\", \"shape\", \"margin\", \"density\", \"severity\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate whether the data needs cleaning; your model is only as good as the data it's given. Hint: use describe() on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bi_rads</th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>956.000000</td>\n",
       "      <td>930.000000</td>\n",
       "      <td>913.000000</td>\n",
       "      <td>885.000000</td>\n",
       "      <td>961.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>4.348279</td>\n",
       "      <td>55.487448</td>\n",
       "      <td>2.721505</td>\n",
       "      <td>2.796276</td>\n",
       "      <td>2.910734</td>\n",
       "      <td>0.463059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.783031</td>\n",
       "      <td>14.480131</td>\n",
       "      <td>1.242792</td>\n",
       "      <td>1.566546</td>\n",
       "      <td>0.380444</td>\n",
       "      <td>0.498893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bi_rads         age       shape      margin     density    severity\n",
       "count  959.000000  956.000000  930.000000  913.000000  885.000000  961.000000\n",
       "mean     4.348279   55.487448    2.721505    2.796276    2.910734    0.463059\n",
       "std      1.783031   14.480131    1.242792    1.566546    0.380444    0.498893\n",
       "min      0.000000   18.000000    1.000000    1.000000    1.000000    0.000000\n",
       "25%      4.000000   45.000000    2.000000    1.000000    3.000000    0.000000\n",
       "50%      4.000000   57.000000    3.000000    3.000000    3.000000    0.000000\n",
       "75%      5.000000   66.000000    4.000000    4.000000    3.000000    1.000000\n",
       "max     55.000000   96.000000    4.000000    5.000000    4.000000    1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few missing values in the data set. Before we just drop every row that's missing data, let's make sure we don't bias our data in doing so. Does there appear to be any sort of correlation to what sort of data has missing fields? If there were, we'd have to try and go back and fill that data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bi_rads</th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>bi_rads</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003303</td>\n",
       "      <td>0.120896</td>\n",
       "      <td>0.094349</td>\n",
       "      <td>-0.013383</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>age</td>\n",
       "      <td>-0.003303</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013204</td>\n",
       "      <td>-0.016582</td>\n",
       "      <td>-0.021193</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>shape</td>\n",
       "      <td>0.120896</td>\n",
       "      <td>-0.013204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228499</td>\n",
       "      <td>0.033791</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>margin</td>\n",
       "      <td>0.094349</td>\n",
       "      <td>-0.016582</td>\n",
       "      <td>0.228499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>density</td>\n",
       "      <td>-0.013383</td>\n",
       "      <td>-0.021193</td>\n",
       "      <td>0.033791</td>\n",
       "      <td>0.216018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>severity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           bi_rads       age     shape    margin   density  severity\n",
       "bi_rads   1.000000 -0.003303  0.120896  0.094349 -0.013383       NaN\n",
       "age      -0.003303  1.000000 -0.013204 -0.016582 -0.021193       NaN\n",
       "shape     0.120896 -0.013204  1.000000  0.228499  0.033791       NaN\n",
       "margin    0.094349 -0.016582  0.228499  1.000000  0.216018       NaN\n",
       "density  -0.013383 -0.021193  0.033791  0.216018  1.000000       NaN\n",
       "severity       NaN       NaN       NaN       NaN       NaN       NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nulls = df.isnull().astype(int)\n",
    "df_nulls.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "axes = plt.gca()\n",
    "df_nulls.plot(kind=\"line\", y=\"shape\",ax=axes)\n",
    "df_nulls.plot(kind=\"line\", y=\"margin\",ax=axes)\n",
    "df_nulls.plot(kind=\"line\", y=\"density\",ax=axes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the missing data seems randomly distributed, go ahead and drop rows with missing data. Hint: use dropna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(961, 6)\n",
      "(830, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df_cleaned = df.dropna()\n",
    "print(df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next you'll need to convert the Pandas dataframes into numpy arrays that can be used by scikit_learn. Create an array that extracts only the feature data we want to work with (age, shape, margin, and density) and another array that contains the classes (severity). You'll also need an array of the feature name labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "df_features = df_cleaned[[\"age\", \"shape\", \"margin\", \"density\"]]\n",
    "feature_names = df_features.columns.to_numpy()\n",
    "features = df_features.to_numpy()\n",
    "classes = df_cleaned[\"severity\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our models require the input data to be normalized, so go ahead and normalize the attribute data. Hint: use preprocessing.StandardScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "#print(scaler.mean_)\n",
    "scaled_features = scaler.transform(features)\n",
    "#print(scaled_features[:5])\n",
    "#print(features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Before moving to K-Fold cross validation and random forests, start by creating a single train/test split of our data. Set aside 75% for training, and 25% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611\n",
      "219\n"
     ]
    }
   ],
   "source": [
    "mask = np.random.rand(len(classes)) < 0.75\n",
    "x_train, x_test = scaled_features[mask], scaled_features[~mask]\n",
    "y_train, y_test = classes[mask], classes[~mask]\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a DecisionTreeClassifier and fit it to your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = tree.DecisionTreeClassifier()\n",
    "dt_classifier = dt_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the resulting decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(129.07510728433098, 211.7178947368421, 'X[2] <= -0.838\\ngini = 0.499\\nsamples = 611\\nvalue = [322, 289]'),\n",
       " Text(61.37508802816901, 200.2736842105263, 'X[0] <= 0.595\\ngini = 0.206\\nsamples = 240\\nvalue = [212, 28]'),\n",
       " Text(29.61919014084507, 188.82947368421054, 'X[0] <= -1.11\\ngini = 0.156\\nsamples = 211\\nvalue = [193, 18]'),\n",
       " Text(4.715492957746479, 177.38526315789474, 'X[0] <= -1.451\\ngini = 0.03\\nsamples = 66\\nvalue = [65, 1]'),\n",
       " Text(2.3577464788732394, 165.94105263157894, 'gini = 0.0\\nsamples = 43\\nvalue = [43, 0]'),\n",
       " Text(7.073239436619718, 165.94105263157894, 'X[0] <= -1.383\\ngini = 0.083\\nsamples = 23\\nvalue = [22, 1]'),\n",
       " Text(4.715492957746479, 154.49684210526317, 'X[1] <= -1.032\\ngini = 0.219\\nsamples = 8\\nvalue = [7, 1]'),\n",
       " Text(2.3577464788732394, 143.05263157894737, 'gini = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(7.073239436619718, 143.05263157894737, 'X[3] <= -1.185\\ngini = 0.375\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(4.715492957746479, 131.60842105263157, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(9.430985915492958, 131.60842105263157, 'gini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(9.430985915492958, 154.49684210526317, 'gini = 0.0\\nsamples = 15\\nvalue = [15, 0]'),\n",
       " Text(54.522887323943664, 177.38526315789474, 'X[1] <= -0.227\\ngini = 0.207\\nsamples = 145\\nvalue = [128, 17]'),\n",
       " Text(38.313380281690144, 165.94105263157894, 'X[0] <= -0.838\\ngini = 0.183\\nsamples = 137\\nvalue = [123, 14]'),\n",
       " Text(21.219718309859154, 154.49684210526317, 'X[0] <= -0.974\\ngini = 0.33\\nsamples = 24\\nvalue = [19, 5]'),\n",
       " Text(16.504225352112677, 143.05263157894737, 'X[1] <= -1.032\\ngini = 0.165\\nsamples = 11\\nvalue = [10, 1]'),\n",
       " Text(14.146478873239436, 131.60842105263157, 'X[0] <= -1.042\\ngini = 0.32\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(11.788732394366196, 120.16421052631578, 'gini = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(16.504225352112677, 120.16421052631578, 'gini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(18.861971830985915, 131.60842105263157, 'gini = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(25.935211267605634, 143.05263157894737, 'X[3] <= -1.185\\ngini = 0.426\\nsamples = 13\\nvalue = [9, 4]'),\n",
       " Text(23.577464788732392, 131.60842105263157, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(28.292957746478873, 131.60842105263157, 'X[1] <= -1.032\\ngini = 0.444\\nsamples = 12\\nvalue = [8, 4]'),\n",
       " Text(23.577464788732392, 120.16421052631578, 'X[0] <= -0.906\\ngini = 0.375\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(21.219718309859154, 108.72, 'gini = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(25.935211267605634, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(33.00845070422535, 120.16421052631578, 'X[0] <= -0.906\\ngini = 0.469\\nsamples = 8\\nvalue = [5, 3]'),\n",
       " Text(30.65070422535211, 108.72, 'gini = 0.5\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(35.36619718309859, 108.72, 'gini = 0.375\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(55.407042253521126, 154.49684210526317, 'X[0] <= 0.117\\ngini = 0.147\\nsamples = 113\\nvalue = [104, 9]'),\n",
       " Text(44.797183098591546, 143.05263157894737, 'X[0] <= -0.497\\ngini = 0.094\\nsamples = 81\\nvalue = [77, 4]'),\n",
       " Text(42.43943661971831, 131.60842105263157, 'gini = 0.0\\nsamples = 29\\nvalue = [29, 0]'),\n",
       " Text(47.154929577464785, 131.60842105263157, 'X[0] <= -0.087\\ngini = 0.142\\nsamples = 52\\nvalue = [48, 4]'),\n",
       " Text(44.797183098591546, 120.16421052631578, 'X[0] <= -0.156\\ngini = 0.193\\nsamples = 37\\nvalue = [33, 4]'),\n",
       " Text(40.08169014084507, 108.72, 'X[0] <= -0.36\\ngini = 0.128\\nsamples = 29\\nvalue = [27, 2]'),\n",
       " Text(37.72394366197183, 97.27578947368421, 'X[3] <= -1.185\\ngini = 0.231\\nsamples = 15\\nvalue = [13, 2]'),\n",
       " Text(35.36619718309859, 85.83157894736843, 'gini = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(40.08169014084507, 85.83157894736843, 'X[1] <= -1.032\\ngini = 0.26\\nsamples = 13\\nvalue = [11, 2]'),\n",
       " Text(35.36619718309859, 74.38736842105263, 'X[0] <= -0.428\\ngini = 0.198\\nsamples = 9\\nvalue = [8, 1]'),\n",
       " Text(33.00845070422535, 62.943157894736856, 'gini = 0.375\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(37.72394366197183, 62.943157894736856, 'gini = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(44.797183098591546, 74.38736842105263, 'X[0] <= -0.428\\ngini = 0.375\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(42.43943661971831, 62.943157894736856, 'gini = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(47.154929577464785, 62.943157894736856, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(42.43943661971831, 97.27578947368421, 'gini = 0.0\\nsamples = 14\\nvalue = [14, 0]'),\n",
       " Text(49.51267605633803, 108.72, 'X[1] <= -1.032\\ngini = 0.375\\nsamples = 8\\nvalue = [6, 2]'),\n",
       " Text(47.154929577464785, 97.27578947368421, 'gini = 0.278\\nsamples = 6\\nvalue = [5, 1]'),\n",
       " Text(51.87042253521127, 97.27578947368421, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(49.51267605633803, 120.16421052631578, 'gini = 0.0\\nsamples = 15\\nvalue = [15, 0]'),\n",
       " Text(66.0169014084507, 143.05263157894737, 'X[0] <= 0.39\\ngini = 0.264\\nsamples = 32\\nvalue = [27, 5]'),\n",
       " Text(63.65915492957746, 131.60842105263157, 'X[0] <= 0.322\\ngini = 0.401\\nsamples = 18\\nvalue = [13, 5]'),\n",
       " Text(61.30140845070422, 120.16421052631578, 'X[0] <= 0.254\\ngini = 0.375\\nsamples = 16\\nvalue = [12, 4]'),\n",
       " Text(58.943661971830984, 108.72, 'X[3] <= -1.185\\ngini = 0.408\\nsamples = 14\\nvalue = [10, 4]'),\n",
       " Text(56.585915492957746, 97.27578947368421, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(61.30140845070422, 97.27578947368421, 'X[0] <= 0.185\\ngini = 0.426\\nsamples = 13\\nvalue = [9, 4]'),\n",
       " Text(56.585915492957746, 85.83157894736843, 'X[1] <= -1.032\\ngini = 0.375\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(54.22816901408451, 74.38736842105263, 'gini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(58.943661971830984, 74.38736842105263, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(66.0169014084507, 85.83157894736843, 'X[1] <= -1.032\\ngini = 0.444\\nsamples = 9\\nvalue = [6, 3]'),\n",
       " Text(63.65915492957746, 74.38736842105263, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(68.37464788732395, 74.38736842105263, 'gini = 0.469\\nsamples = 8\\nvalue = [5, 3]'),\n",
       " Text(63.65915492957746, 108.72, 'gini = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(66.0169014084507, 120.16421052631578, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(68.37464788732395, 131.60842105263157, 'gini = 0.0\\nsamples = 14\\nvalue = [14, 0]'),\n",
       " Text(70.73239436619718, 165.94105263157894, 'X[0] <= -1.042\\ngini = 0.469\\nsamples = 8\\nvalue = [5, 3]'),\n",
       " Text(68.37464788732395, 154.49684210526317, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(73.09014084507042, 154.49684210526317, 'X[0] <= -0.769\\ngini = 0.408\\nsamples = 7\\nvalue = [5, 2]'),\n",
       " Text(70.73239436619718, 143.05263157894737, 'gini = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(75.44788732394366, 143.05263157894737, 'X[0] <= -0.599\\ngini = 0.5\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(73.09014084507042, 131.60842105263157, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(77.8056338028169, 131.60842105263157, 'X[0] <= -0.224\\ngini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(75.44788732394366, 120.16421052631578, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(80.16338028169014, 120.16421052631578, 'X[0] <= 0.151\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(77.8056338028169, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(82.52112676056338, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(93.13098591549296, 188.82947368421054, 'X[1] <= -1.032\\ngini = 0.452\\nsamples = 29\\nvalue = [19, 10]'),\n",
       " Text(87.23661971830985, 177.38526315789474, 'X[0] <= 1.311\\ngini = 0.498\\nsamples = 15\\nvalue = [8, 7]'),\n",
       " Text(84.87887323943661, 165.94105263157894, 'X[0] <= 1.106\\ngini = 0.463\\nsamples = 11\\nvalue = [4, 7]'),\n",
       " Text(82.52112676056338, 154.49684210526317, 'X[3] <= -2.611\\ngini = 0.494\\nsamples = 9\\nvalue = [4, 5]'),\n",
       " Text(80.16338028169014, 143.05263157894737, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(84.87887323943661, 143.05263157894737, 'X[0] <= 0.663\\ngini = 0.469\\nsamples = 8\\nvalue = [3, 5]'),\n",
       " Text(82.52112676056338, 131.60842105263157, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(87.23661971830985, 131.60842105263157, 'X[0] <= 0.765\\ngini = 0.49\\nsamples = 7\\nvalue = [3, 4]'),\n",
       " Text(84.87887323943661, 120.16421052631578, 'gini = 0.5\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(89.59436619718309, 120.16421052631578, 'gini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(87.23661971830985, 154.49684210526317, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(89.59436619718309, 165.94105263157894, 'gini = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(99.02535211267606, 177.38526315789474, 'X[0] <= 1.208\\ngini = 0.337\\nsamples = 14\\nvalue = [11, 3]'),\n",
       " Text(94.30985915492957, 165.94105263157894, 'X[0] <= 1.072\\ngini = 0.165\\nsamples = 11\\nvalue = [10, 1]'),\n",
       " Text(91.95211267605633, 154.49684210526317, 'X[0] <= 1.004\\ngini = 0.278\\nsamples = 6\\nvalue = [5, 1]'),\n",
       " Text(89.59436619718309, 143.05263157894737, 'gini = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(94.30985915492957, 143.05263157894737, 'X[1] <= -0.227\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(91.95211267605633, 131.60842105263157, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(96.66760563380282, 131.60842105263157, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(96.66760563380282, 154.49684210526317, 'gini = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(103.74084507042254, 165.94105263157894, 'X[3] <= -1.185\\ngini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(101.3830985915493, 154.49684210526317, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(106.09859154929578, 154.49684210526317, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(196.77512654049295, 200.2736842105263, 'X[1] <= -0.227\\ngini = 0.417\\nsamples = 371\\nvalue = [110, 261]'),\n",
       " Text(129.08661971830986, 188.82947368421054, 'X[0] <= 0.663\\ngini = 0.466\\nsamples = 54\\nvalue = [34, 20]'),\n",
       " Text(119.0661971830986, 177.38526315789474, 'X[0] <= -1.486\\ngini = 0.387\\nsamples = 42\\nvalue = [31, 11]'),\n",
       " Text(116.70845070422536, 165.94105263157894, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(121.42394366197183, 165.94105263157894, 'X[1] <= -1.032\\ngini = 0.369\\nsamples = 41\\nvalue = [31, 10]'),\n",
       " Text(110.81408450704225, 154.49684210526317, 'X[3] <= -1.185\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(108.45633802816901, 143.05263157894737, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(113.17183098591549, 143.05263157894737, 'X[0] <= -1.11\\ngini = 0.48\\nsamples = 5\\nvalue = [2, 3]'),\n",
       " Text(110.81408450704225, 131.60842105263157, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(115.52957746478873, 131.60842105263157, 'X[0] <= -0.838\\ngini = 0.375\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(113.17183098591549, 120.16421052631578, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(117.88732394366197, 120.16421052631578, 'X[0] <= -0.36\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(115.52957746478873, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(120.2450704225352, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(132.0338028169014, 154.49684210526317, 'X[0] <= -0.053\\ngini = 0.32\\nsamples = 35\\nvalue = [28, 7]'),\n",
       " Text(127.31830985915492, 143.05263157894737, 'X[0] <= -1.179\\ngini = 0.124\\nsamples = 15\\nvalue = [14, 1]'),\n",
       " Text(124.96056338028168, 131.60842105263157, 'X[0] <= -1.247\\ngini = 0.32\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(122.60281690140845, 120.16421052631578, 'gini = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(127.31830985915492, 120.16421052631578, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(129.67605633802816, 131.60842105263157, 'gini = 0.0\\nsamples = 10\\nvalue = [10, 0]'),\n",
       " Text(136.7492957746479, 143.05263157894737, 'X[2] <= -0.2\\ngini = 0.42\\nsamples = 20\\nvalue = [14, 6]'),\n",
       " Text(134.39154929577464, 131.60842105263157, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(139.10704225352112, 131.60842105263157, 'X[0] <= 0.39\\ngini = 0.388\\nsamples = 19\\nvalue = [14, 5]'),\n",
       " Text(133.21267605633804, 120.16421052631578, 'X[2] <= 0.438\\ngini = 0.298\\nsamples = 11\\nvalue = [9, 2]'),\n",
       " Text(128.49718309859153, 108.72, 'X[0] <= 0.185\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(126.13943661971831, 97.27578947368421, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(130.8549295774648, 97.27578947368421, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(137.92816901408452, 108.72, 'X[0] <= 0.151\\ngini = 0.198\\nsamples = 9\\nvalue = [8, 1]'),\n",
       " Text(135.57042253521126, 97.27578947368421, 'gini = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(140.28591549295774, 97.27578947368421, 'gini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(145.00140845070422, 120.16421052631578, 'X[0] <= 0.492\\ngini = 0.469\\nsamples = 8\\nvalue = [5, 3]'),\n",
       " Text(142.643661971831, 108.72, 'gini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(147.35915492957747, 108.72, 'X[2] <= 0.438\\ngini = 0.32\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(145.00140845070422, 97.27578947368421, 'gini = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(149.7169014084507, 97.27578947368421, 'gini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(139.10704225352112, 177.38526315789474, 'X[2] <= 0.438\\ngini = 0.375\\nsamples = 12\\nvalue = [3, 9]'),\n",
       " Text(136.7492957746479, 165.94105263157894, 'gini = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(141.46478873239437, 165.94105263157894, 'X[0] <= 1.072\\ngini = 0.18\\nsamples = 10\\nvalue = [1, 9]'),\n",
       " Text(139.10704225352112, 154.49684210526317, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 6]'),\n",
       " Text(143.8225352112676, 154.49684210526317, 'X[0] <= 1.242\\ngini = 0.375\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(141.46478873239437, 143.05263157894737, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(146.18028169014084, 143.05263157894737, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(264.46363336267603, 188.82947368421054, 'X[0] <= 0.936\\ngini = 0.365\\nsamples = 317\\nvalue = [76, 241]'),\n",
       " Text(208.86318221830984, 177.38526315789474, 'X[0] <= -0.156\\ngini = 0.424\\nsamples = 229\\nvalue = [70, 159]'),\n",
       " Text(172.88912852112676, 165.94105263157894, 'X[1] <= 0.578\\ngini = 0.482\\nsamples = 79\\nvalue = [32, 47]'),\n",
       " Text(156.79014084507043, 154.49684210526317, 'X[2] <= -0.2\\ngini = 0.444\\nsamples = 15\\nvalue = [10, 5]'),\n",
       " Text(154.43239436619717, 143.05263157894737, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(159.14788732394365, 143.05263157894737, 'X[2] <= 1.077\\ngini = 0.355\\nsamples = 13\\nvalue = [10, 3]'),\n",
       " Text(156.79014084507043, 131.60842105263157, 'X[0] <= -0.804\\ngini = 0.278\\nsamples = 12\\nvalue = [10, 2]'),\n",
       " Text(154.43239436619717, 120.16421052631578, 'X[2] <= 0.438\\ngini = 0.5\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(152.07464788732395, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(156.79014084507043, 108.72, 'X[3] <= -1.185\\ngini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(154.43239436619717, 97.27578947368421, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(159.14788732394365, 97.27578947368421, 'X[0] <= -1.008\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(156.79014084507043, 85.83157894736843, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(161.5056338028169, 85.83157894736843, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(159.14788732394365, 120.16421052631578, 'gini = 0.0\\nsamples = 8\\nvalue = [8, 0]'),\n",
       " Text(161.5056338028169, 131.60842105263157, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(188.9881161971831, 154.49684210526317, 'X[0] <= -1.383\\ngini = 0.451\\nsamples = 64\\nvalue = [22, 42]'),\n",
       " Text(178.89401408450703, 143.05263157894737, 'X[0] <= -1.656\\ngini = 0.469\\nsamples = 8\\nvalue = [5, 3]'),\n",
       " Text(176.5362676056338, 131.60842105263157, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(181.2517605633803, 131.60842105263157, 'X[2] <= 1.077\\ngini = 0.278\\nsamples = 6\\nvalue = [5, 1]'),\n",
       " Text(178.89401408450703, 120.16421052631578, 'gini = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(183.6095070422535, 120.16421052631578, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(199.08221830985914, 143.05263157894737, 'X[0] <= -0.224\\ngini = 0.423\\nsamples = 56\\nvalue = [17, 39]'),\n",
       " Text(190.68274647887324, 131.60842105263157, 'X[0] <= -1.213\\ngini = 0.398\\nsamples = 51\\nvalue = [14, 37]'),\n",
       " Text(188.325, 120.16421052631578, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(193.04049295774647, 120.16421052631578, 'X[0] <= -0.292\\ngini = 0.413\\nsamples = 48\\nvalue = [14, 34]'),\n",
       " Text(183.31478873239436, 108.72, 'X[0] <= -0.633\\ngini = 0.439\\nsamples = 40\\nvalue = [13, 27]'),\n",
       " Text(170.93661971830986, 97.27578947368421, 'X[0] <= -1.11\\ngini = 0.401\\nsamples = 18\\nvalue = [5, 13]'),\n",
       " Text(166.22112676056338, 85.83157894736843, 'X[2] <= 0.438\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(163.86338028169013, 74.38736842105263, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(168.5788732394366, 74.38736842105263, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(175.65211267605633, 85.83157894736843, 'X[2] <= 0.438\\ngini = 0.375\\nsamples = 16\\nvalue = [4, 12]'),\n",
       " Text(173.2943661971831, 74.38736842105263, 'gini = 0.0\\nsamples = 5\\nvalue = [0, 5]'),\n",
       " Text(178.0098591549296, 74.38736842105263, 'X[2] <= 1.077\\ngini = 0.463\\nsamples = 11\\nvalue = [4, 7]'),\n",
       " Text(173.2943661971831, 62.943157894736856, 'X[0] <= -0.872\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(170.93661971830986, 51.49894736842106, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(175.65211267605633, 51.49894736842106, 'X[0] <= -0.769\\ngini = 0.48\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(170.93661971830986, 40.05473684210526, 'X[3] <= -1.185\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(168.5788732394366, 28.610526315789485, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(173.2943661971831, 28.610526315789485, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(180.3676056338028, 40.05473684210526, 'X[0] <= -0.701\\ngini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(178.0098591549296, 28.610526315789485, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(182.72535211267606, 28.610526315789485, 'X[3] <= -1.185\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(180.3676056338028, 17.166315789473686, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(185.0830985915493, 17.166315789473686, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(182.72535211267606, 62.943157894736856, 'X[0] <= -0.906\\ngini = 0.32\\nsamples = 5\\nvalue = [1, 4]'),\n",
       " Text(180.3676056338028, 51.49894736842106, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(185.0830985915493, 51.49894736842106, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(195.69295774647887, 97.27578947368421, 'X[3] <= -1.185\\ngini = 0.463\\nsamples = 22\\nvalue = [8, 14]'),\n",
       " Text(193.33521126760564, 85.83157894736843, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(198.05070422535212, 85.83157894736843, 'X[2] <= 1.077\\ngini = 0.472\\nsamples = 21\\nvalue = [8, 13]'),\n",
       " Text(189.79859154929576, 74.38736842105263, 'X[0] <= -0.497\\ngini = 0.486\\nsamples = 12\\nvalue = [5, 7]'),\n",
       " Text(187.44084507042254, 62.943157894736856, 'gini = 0.375\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(192.15633802816902, 62.943157894736856, 'X[0] <= -0.428\\ngini = 0.5\\nsamples = 8\\nvalue = [4, 4]'),\n",
       " Text(189.79859154929576, 51.49894736842106, 'gini = 0.5\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(194.51408450704224, 51.49894736842106, 'X[2] <= 0.438\\ngini = 0.5\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(189.79859154929576, 40.05473684210526, 'X[0] <= -0.36\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(187.44084507042254, 28.610526315789485, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(192.15633802816902, 28.610526315789485, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(199.22957746478872, 40.05473684210526, 'X[0] <= -0.36\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(196.8718309859155, 28.610526315789485, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(201.58732394366197, 28.610526315789485, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(206.30281690140845, 74.38736842105263, 'X[0] <= -0.497\\ngini = 0.444\\nsamples = 9\\nvalue = [3, 6]'),\n",
       " Text(201.58732394366197, 62.943157894736856, 'X[0] <= -0.565\\ngini = 0.5\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(199.22957746478872, 51.49894736842106, 'gini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(203.9450704225352, 51.49894736842106, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(211.01830985915493, 62.943157894736856, 'X[0] <= -0.428\\ngini = 0.32\\nsamples = 5\\nvalue = [1, 4]'),\n",
       " Text(208.6605633802817, 51.49894736842106, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(213.37605633802818, 51.49894736842106, 'X[0] <= -0.36\\ngini = 0.375\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(211.01830985915493, 40.05473684210526, 'gini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(215.7338028169014, 40.05473684210526, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(202.7661971830986, 108.72, 'X[2] <= 1.077\\ngini = 0.219\\nsamples = 8\\nvalue = [1, 7]'),\n",
       " Text(200.40845070422534, 97.27578947368421, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 6]'),\n",
       " Text(205.12394366197182, 97.27578947368421, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(207.48169014084507, 131.60842105263157, 'X[2] <= 0.438\\ngini = 0.48\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(205.12394366197182, 120.16421052631578, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(209.8394366197183, 120.16421052631578, 'X[2] <= 1.077\\ngini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(207.48169014084507, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(212.19718309859155, 108.72, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(244.83723591549295, 165.94105263157894, 'X[2] <= 0.438\\ngini = 0.378\\nsamples = 150\\nvalue = [38, 112]'),\n",
       " Text(228.70140845070424, 154.49684210526317, 'X[0] <= 0.731\\ngini = 0.456\\nsamples = 37\\nvalue = [13, 24]'),\n",
       " Text(226.34366197183098, 143.05263157894737, 'X[0] <= -0.087\\ngini = 0.487\\nsamples = 31\\nvalue = [13, 18]'),\n",
       " Text(223.98591549295773, 131.60842105263157, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(228.70140845070424, 131.60842105263157, 'X[0] <= 0.049\\ngini = 0.48\\nsamples = 30\\nvalue = [12, 18]'),\n",
       " Text(226.34366197183098, 120.16421052631578, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(231.05915492957746, 120.16421052631578, 'X[2] <= -0.2\\ngini = 0.497\\nsamples = 26\\nvalue = [12, 14]'),\n",
       " Text(228.70140845070424, 108.72, 'gini = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(233.4169014084507, 108.72, 'X[3] <= -1.185\\ngini = 0.486\\nsamples = 24\\nvalue = [10, 14]'),\n",
       " Text(231.05915492957746, 97.27578947368421, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(235.77464788732394, 97.27578947368421, 'X[0] <= 0.663\\ngini = 0.476\\nsamples = 23\\nvalue = [9, 14]'),\n",
       " Text(229.88028169014083, 85.83157894736843, 'X[0] <= 0.492\\ngini = 0.455\\nsamples = 20\\nvalue = [7, 13]'),\n",
       " Text(225.16478873239436, 74.38736842105263, 'X[0] <= 0.356\\ngini = 0.496\\nsamples = 11\\nvalue = [5, 6]'),\n",
       " Text(222.80704225352113, 62.943157894736856, 'X[1] <= 0.578\\ngini = 0.469\\nsamples = 8\\nvalue = [3, 5]'),\n",
       " Text(220.44929577464788, 51.49894736842106, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(225.16478873239436, 51.49894736842106, 'X[0] <= 0.185\\ngini = 0.49\\nsamples = 7\\nvalue = [3, 4]'),\n",
       " Text(220.44929577464788, 40.05473684210526, 'X[0] <= 0.117\\ngini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(218.09154929577466, 28.610526315789485, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(222.80704225352113, 28.610526315789485, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(229.88028169014083, 40.05473684210526, 'X[0] <= 0.254\\ngini = 0.375\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(227.5225352112676, 28.610526315789485, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(232.2380281690141, 28.610526315789485, 'gini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(227.5225352112676, 62.943157894736856, 'gini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(234.5957746478873, 74.38736842105263, 'X[0] <= 0.595\\ngini = 0.346\\nsamples = 9\\nvalue = [2, 7]'),\n",
       " Text(232.2380281690141, 62.943157894736856, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(236.95352112676056, 62.943157894736856, 'X[1] <= 0.578\\ngini = 0.48\\nsamples = 5\\nvalue = [2, 3]'),\n",
       " Text(234.5957746478873, 51.49894736842106, 'gini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(239.3112676056338, 51.49894736842106, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(241.66901408450704, 85.83157894736843, 'X[1] <= 0.578\\ngini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(239.3112676056338, 74.38736842105263, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(244.02676056338026, 74.38736842105263, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(231.05915492957746, 143.05263157894737, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 6]'),\n",
       " Text(260.9730633802817, 154.49684210526317, 'X[0] <= -0.087\\ngini = 0.345\\nsamples = 113\\nvalue = [25, 88]'),\n",
       " Text(258.61531690140845, 143.05263157894737, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 6]'),\n",
       " Text(263.33080985915495, 143.05263157894737, 'X[0] <= 0.049\\ngini = 0.358\\nsamples = 107\\nvalue = [25, 82]'),\n",
       " Text(250.5105633802817, 131.60842105263157, 'X[0] <= -0.019\\ngini = 0.48\\nsamples = 15\\nvalue = [6, 9]'),\n",
       " Text(245.79507042253522, 120.16421052631578, 'X[3] <= -1.185\\ngini = 0.346\\nsamples = 9\\nvalue = [2, 7]'),\n",
       " Text(243.43732394366197, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(248.15281690140844, 108.72, 'X[2] <= 1.077\\ngini = 0.219\\nsamples = 8\\nvalue = [1, 7]'),\n",
       " Text(245.79507042253522, 97.27578947368421, 'gini = 0.0\\nsamples = 5\\nvalue = [0, 5]'),\n",
       " Text(250.5105633802817, 97.27578947368421, 'gini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(255.22605633802817, 120.16421052631578, 'X[2] <= 1.077\\ngini = 0.444\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(252.86830985915492, 108.72, 'gini = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(257.5838028169014, 108.72, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(276.1510563380282, 131.60842105263157, 'X[0] <= 0.799\\ngini = 0.328\\nsamples = 92\\nvalue = [19, 73]'),\n",
       " Text(264.6570422535211, 120.16421052631578, 'X[1] <= 0.578\\ngini = 0.302\\nsamples = 81\\nvalue = [15, 66]'),\n",
       " Text(262.2992957746479, 108.72, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 6]'),\n",
       " Text(267.0147887323944, 108.72, 'X[0] <= 0.254\\ngini = 0.32\\nsamples = 75\\nvalue = [15, 60]'),\n",
       " Text(255.81549295774647, 97.27578947368421, 'X[3] <= 1.666\\ngini = 0.375\\nsamples = 20\\nvalue = [5, 15]'),\n",
       " Text(253.45774647887325, 85.83157894736843, 'X[2] <= 1.077\\ngini = 0.401\\nsamples = 18\\nvalue = [5, 13]'),\n",
       " Text(248.74225352112677, 74.38736842105263, 'X[0] <= 0.117\\ngini = 0.444\\nsamples = 12\\nvalue = [4, 8]'),\n",
       " Text(246.38450704225352, 62.943157894736856, 'gini = 0.5\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(251.1, 62.943157894736856, 'X[0] <= 0.185\\ngini = 0.375\\nsamples = 8\\nvalue = [2, 6]'),\n",
       " Text(248.74225352112677, 51.49894736842106, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(253.45774647887325, 51.49894736842106, 'X[3] <= -1.185\\ngini = 0.48\\nsamples = 5\\nvalue = [2, 3]'),\n",
       " Text(251.1, 40.05473684210526, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(255.81549295774647, 40.05473684210526, 'gini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(258.1732394366197, 74.38736842105263, 'X[0] <= 0.117\\ngini = 0.278\\nsamples = 6\\nvalue = [1, 5]'),\n",
       " Text(255.81549295774647, 62.943157894736856, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(260.530985915493, 62.943157894736856, 'X[0] <= 0.185\\ngini = 0.32\\nsamples = 5\\nvalue = [1, 4]'),\n",
       " Text(258.1732394366197, 51.49894736842106, 'gini = 0.375\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(262.8887323943662, 51.49894736842106, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(258.1732394366197, 85.83157894736843, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(278.21408450704223, 97.27578947368421, 'X[3] <= 1.666\\ngini = 0.298\\nsamples = 55\\nvalue = [10, 45]'),\n",
       " Text(275.85633802816903, 85.83157894736843, 'X[0] <= 0.39\\ngini = 0.278\\nsamples = 54\\nvalue = [9, 45]'),\n",
       " Text(273.4985915492958, 74.38736842105263, 'gini = 0.0\\nsamples = 5\\nvalue = [0, 5]'),\n",
       " Text(278.21408450704223, 74.38736842105263, 'X[2] <= 1.077\\ngini = 0.3\\nsamples = 49\\nvalue = [9, 40]'),\n",
       " Text(272.3197183098591, 62.943157894736856, 'X[0] <= 0.663\\ngini = 0.238\\nsamples = 29\\nvalue = [4, 25]'),\n",
       " Text(267.6042253521127, 51.49894736842106, 'X[3] <= -1.185\\ngini = 0.305\\nsamples = 16\\nvalue = [3, 13]'),\n",
       " Text(265.2464788732394, 40.05473684210526, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(269.96197183098593, 40.05473684210526, 'X[0] <= 0.595\\ngini = 0.337\\nsamples = 14\\nvalue = [3, 11]'),\n",
       " Text(267.6042253521127, 28.610526315789485, 'X[0] <= 0.526\\ngini = 0.298\\nsamples = 11\\nvalue = [2, 9]'),\n",
       " Text(265.2464788732394, 17.166315789473686, 'X[0] <= 0.458\\ngini = 0.346\\nsamples = 9\\nvalue = [2, 7]'),\n",
       " Text(262.8887323943662, 5.722105263157886, 'gini = 0.375\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(267.6042253521127, 5.722105263157886, 'gini = 0.32\\nsamples = 5\\nvalue = [1, 4]'),\n",
       " Text(269.96197183098593, 17.166315789473686, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(272.3197183098591, 28.610526315789485, 'gini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(277.03521126760563, 51.49894736842106, 'X[0] <= 0.731\\ngini = 0.142\\nsamples = 13\\nvalue = [1, 12]'),\n",
       " Text(274.6774647887324, 40.05473684210526, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 6]'),\n",
       " Text(279.3929577464789, 40.05473684210526, 'X[3] <= -2.611\\ngini = 0.245\\nsamples = 7\\nvalue = [1, 6]'),\n",
       " Text(277.03521126760563, 28.610526315789485, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(281.7507042253521, 28.610526315789485, 'gini = 0.278\\nsamples = 6\\nvalue = [1, 5]'),\n",
       " Text(284.10845070422533, 62.943157894736856, 'X[0] <= 0.526\\ngini = 0.375\\nsamples = 20\\nvalue = [5, 15]'),\n",
       " Text(281.7507042253521, 51.49894736842106, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(286.4661971830986, 51.49894736842106, 'X[0] <= 0.595\\ngini = 0.415\\nsamples = 17\\nvalue = [5, 12]'),\n",
       " Text(284.10845070422533, 40.05473684210526, 'gini = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(288.82394366197184, 40.05473684210526, 'X[0] <= 0.663\\ngini = 0.337\\nsamples = 14\\nvalue = [3, 11]'),\n",
       " Text(286.4661971830986, 28.610526315789485, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(291.1816901408451, 28.610526315789485, 'X[0] <= 0.731\\ngini = 0.375\\nsamples = 12\\nvalue = [3, 9]'),\n",
       " Text(288.82394366197184, 17.166315789473686, 'gini = 0.32\\nsamples = 5\\nvalue = [1, 4]'),\n",
       " Text(293.5394366197183, 17.166315789473686, 'gini = 0.408\\nsamples = 7\\nvalue = [2, 5]'),\n",
       " Text(280.5718309859155, 85.83157894736843, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(287.6450704225352, 120.16421052631578, 'X[2] <= 1.077\\ngini = 0.463\\nsamples = 11\\nvalue = [4, 7]'),\n",
       " Text(285.287323943662, 108.72, 'X[0] <= 0.867\\ngini = 0.48\\nsamples = 10\\nvalue = [4, 6]'),\n",
       " Text(282.92957746478874, 97.27578947368421, 'gini = 0.444\\nsamples = 6\\nvalue = [2, 4]'),\n",
       " Text(287.6450704225352, 97.27578947368421, 'X[1] <= 0.578\\ngini = 0.5\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(285.287323943662, 85.83157894736843, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(290.00281690140844, 85.83157894736843, 'gini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(290.00281690140844, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(320.06408450704225, 177.38526315789474, 'X[0] <= 1.89\\ngini = 0.127\\nsamples = 88\\nvalue = [6, 82]'),\n",
       " Text(310.043661971831, 165.94105263157894, 'X[0] <= 1.413\\ngini = 0.098\\nsamples = 77\\nvalue = [4, 73]'),\n",
       " Text(307.68591549295775, 154.49684210526317, 'X[2] <= 0.438\\ngini = 0.145\\nsamples = 51\\nvalue = [4, 47]'),\n",
       " Text(301.79154929577464, 143.05263157894737, 'X[0] <= 1.208\\ngini = 0.208\\nsamples = 17\\nvalue = [2, 15]'),\n",
       " Text(299.4338028169014, 131.60842105263157, 'X[0] <= 1.14\\ngini = 0.298\\nsamples = 11\\nvalue = [2, 9]'),\n",
       " Text(297.07605633802814, 120.16421052631578, 'X[0] <= 1.072\\ngini = 0.18\\nsamples = 10\\nvalue = [1, 9]'),\n",
       " Text(294.71830985915494, 108.72, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(299.4338028169014, 108.72, 'X[1] <= 0.578\\ngini = 0.278\\nsamples = 6\\nvalue = [1, 5]'),\n",
       " Text(297.07605633802814, 97.27578947368421, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(301.79154929577464, 97.27578947368421, 'gini = 0.375\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(301.79154929577464, 120.16421052631578, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(304.1492957746479, 131.60842105263157, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 6]'),\n",
       " Text(313.58028169014085, 143.05263157894737, 'X[0] <= 1.345\\ngini = 0.111\\nsamples = 34\\nvalue = [2, 32]'),\n",
       " Text(308.86478873239435, 131.60842105263157, 'X[2] <= 1.077\\ngini = 0.071\\nsamples = 27\\nvalue = [1, 26]'),\n",
       " Text(306.50704225352115, 120.16421052631578, 'gini = 0.0\\nsamples = 15\\nvalue = [0, 15]'),\n",
       " Text(311.2225352112676, 120.16421052631578, 'X[0] <= 1.072\\ngini = 0.153\\nsamples = 12\\nvalue = [1, 11]'),\n",
       " Text(308.86478873239435, 108.72, 'X[0] <= 1.004\\ngini = 0.32\\nsamples = 5\\nvalue = [1, 4]'),\n",
       " Text(306.50704225352115, 97.27578947368421, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(311.2225352112676, 97.27578947368421, 'X[3] <= -1.185\\ngini = 0.375\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(308.86478873239435, 85.83157894736843, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(313.58028169014085, 85.83157894736843, 'gini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(313.58028169014085, 108.72, 'gini = 0.0\\nsamples = 7\\nvalue = [0, 7]'),\n",
       " Text(318.2957746478873, 131.60842105263157, 'X[2] <= 1.077\\ngini = 0.245\\nsamples = 7\\nvalue = [1, 6]'),\n",
       " Text(315.9380281690141, 120.16421052631578, 'gini = 0.32\\nsamples = 5\\nvalue = [1, 4]'),\n",
       " Text(320.65352112676055, 120.16421052631578, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(312.4014084507042, 154.49684210526317, 'gini = 0.0\\nsamples = 26\\nvalue = [0, 26]'),\n",
       " Text(330.0845070422535, 165.94105263157894, 'X[0] <= 2.095\\ngini = 0.298\\nsamples = 11\\nvalue = [2, 9]'),\n",
       " Text(327.72676056338025, 154.49684210526317, 'X[2] <= 1.077\\ngini = 0.444\\nsamples = 6\\nvalue = [2, 4]'),\n",
       " Text(325.36901408450706, 143.05263157894737, 'X[0] <= 2.027\\ngini = 0.32\\nsamples = 5\\nvalue = [1, 4]'),\n",
       " Text(323.0112676056338, 131.60842105263157, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(327.72676056338025, 131.60842105263157, 'X[1] <= 0.578\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(325.36901408450706, 120.16421052631578, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(330.0845070422535, 120.16421052631578, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(330.0845070422535, 143.05263157894737, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(332.44225352112676, 154.49684210526317, 'gini = 0.0\\nsamples = 5\\nvalue = [0, 5]')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree.plot_tree(dt_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy of the resulting decision tree model using your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7397260273972602"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = dt_classifier.predict(x_test)\n",
    "# accuracy_score(y_test, predicted)\n",
    "dt_classifier.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of a single train/test split, use K-Fold cross validation to get a better measure of your model's accuracy (K=10). Hint: use model_selection.cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71428571 0.78571429 0.77380952 0.73493976 0.77108434 0.6746988\n",
      " 0.72289157 0.76829268 0.75609756 0.69512195]\n",
      "0.7396936176762801\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = cross_val_score(dt_classifier, scaled_features, classes, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try a RandomForestClassifier instead. Does it perform better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7945205479452054\n",
      "[0.73809524 0.79761905 0.80952381 0.78313253 0.79518072 0.71084337\n",
      " 0.75903614 0.75609756 0.79268293 0.69512195]\n",
      "0.7637333305346823\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=10)\n",
    "rf_classifier = rf_classifier.fit(x_train, y_train)\n",
    "print(rf_classifier.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(rf_classifier, scaled_features, classes, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "Next try using svm.SVC with a linear kernel. How does it compare to the decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8036529680365296\n",
      "[0.71428571 0.77380952 0.86904762 0.80722892 0.84337349 0.69879518\n",
      " 0.80722892 0.80487805 0.90243902 0.74390244]\n",
      "0.7964988875362076\n"
     ]
    }
   ],
   "source": [
    "sv_classifier = SVC(kernel=\"linear\")\n",
    "sv_classifier = sv_classifier.fit(x_train, y_train)\n",
    "print(sv_classifier.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(sv_classifier, scaled_features, classes, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "How about K-Nearest-Neighbors? Hint: use neighbors.KNeighborsClassifier - it's a lot easier than implementing KNN from scratch like we did earlier in the course. Start with a K of 10. K is an example of a hyperparameter - a parameter on the model itself which may need to be tuned for best results on your particular data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7990867579908676\n",
      "[0.77380952 0.76190476 0.83333333 0.74698795 0.87951807 0.72289157\n",
      " 0.81927711 0.79268293 0.81707317 0.70731707]\n",
      "0.7854795488574507\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn = knn.fit(x_train, y_train)\n",
    "print(knn.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(knn, scaled_features, classes, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing K is tricky, so we can't discard KNN until we've tried different values of K. Write a for loop to run KNN with K values ranging from 1 to 50 and see if K makes a substantial difference. Make a note of the best performance you could get out of KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7940595133145824, 7),\n",
       " (0.7927235072694961, 50),\n",
       " (0.7915333809104012, 11),\n",
       " (0.7915039950743742, 14),\n",
       " (0.7914172368918182, 31),\n",
       " (0.7902995256286471, 49),\n",
       " (0.7902271105327232, 28),\n",
       " (0.7890653205155116, 48),\n",
       " (0.7880200243482641, 9),\n",
       " (0.7878891874116676, 39)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_runs = []\n",
    "for i in range(50):\n",
    "    n = i+1\n",
    "    mean_score = cross_val_score(KNeighborsClassifier(n_neighbors=n), scaled_features, classes, cv=10).mean()\n",
    "    knn_runs.append((mean_score, n))\n",
    "knn_runs.sort(key=lambda x: x[0], reverse=True)\n",
    "knn_runs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Now try naive_bayes.MultinomialNB. How does its accuracy stack up? Hint: you'll need to use MinMaxScaler to get the features in the range MultinomialNB requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7990867579908676\n",
      "[0.73809524 0.76190476 0.82142857 0.8313253  0.8313253  0.73493976\n",
      " 0.74698795 0.76829268 0.91463415 0.69512195]\n",
      "0.7844055665169388\n"
     ]
    }
   ],
   "source": [
    "minmax_features = MinMaxScaler().fit_transform(features) # scale between 0 and 1 preserving shape of data\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(minmax_features[mask], classes[mask])\n",
    "print(nb_classifier.score(minmax_features[~mask], classes[~mask]))\n",
    "\n",
    "scores = cross_val_score(nb_classifier, minmax_features, classes, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting SVM\n",
    "\n",
    "svm.SVC may perform differently with different kernels. The choice of kernel is an example of a \"hyperparamter.\" Try the rbf, sigmoid, and poly kernels and see what the best-performing kernel is. Do we have a new winner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8127853881278538\n",
      "[0.75       0.79761905 0.86904762 0.80722892 0.86746988 0.72289157\n",
      " 0.78313253 0.7804878  0.86585366 0.76829268]\n",
      "0.8012023704574396\n"
     ]
    }
   ],
   "source": [
    "svrbf_classifier = SVC(kernel=\"rbf\", gamma=\"auto\")\n",
    "svrbf_classifier = svrbf_classifier.fit(x_train, y_train)\n",
    "print(svrbf_classifier.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(svrbf_classifier, scaled_features, classes, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7397260273972602\n",
      "[0.70238095 0.67857143 0.76190476 0.72289157 0.74698795 0.71084337\n",
      " 0.74698795 0.7804878  0.70731707 0.79268293]\n",
      "0.7351055791108685\n"
     ]
    }
   ],
   "source": [
    "svsig_classifier = SVC(kernel=\"sigmoid\", gamma=\"auto\")\n",
    "svsig_classifier = svsig_classifier.fit(x_train, y_train)\n",
    "print(svsig_classifier.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(svsig_classifier, scaled_features, classes, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8264840182648402\n",
      "[0.76190476 0.77380952 0.88095238 0.79518072 0.84337349 0.69879518\n",
      " 0.77108434 0.79268293 0.86585366 0.74390244]\n",
      "0.792753942599667\n"
     ]
    }
   ],
   "source": [
    "svpoly_classifier = SVC(kernel=\"poly\", gamma=\"auto\")\n",
    "svpoly_classifier = svpoly_classifier.fit(x_train, y_train)\n",
    "print(svpoly_classifier.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(svpoly_classifier, scaled_features, classes, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We've tried all these fancy techniques, but fundamentally this is just a binary classification problem. Try Logisitic Regression, which is a simple way to tackling this sort of thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8127853881278538\n",
      "[0.76190476 0.76190476 0.88095238 0.81927711 0.8313253  0.71084337\n",
      " 0.79518072 0.82926829 0.8902439  0.79268293]\n",
      "0.8073583532737221\n"
     ]
    }
   ],
   "source": [
    "lbfgs_classifier = LogisticRegression(solver=\"lbfgs\")\n",
    "lbfgs_classifier = lbfgs_classifier.fit(x_train, y_train)\n",
    "print(lbfgs_classifier.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(lbfgs_classifier, scaled_features, classes, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8127853881278538\n",
      "[0.76190476 0.76190476 0.88095238 0.81927711 0.8313253  0.71084337\n",
      " 0.79518072 0.82926829 0.8902439  0.79268293]\n",
      "0.8073583532737221\n"
     ]
    }
   ],
   "source": [
    "sag_classifier = LogisticRegression(solver=\"sag\")\n",
    "sag_classifier = sag_classifier.fit(x_train, y_train)\n",
    "print(sag_classifier.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(sag_classifier, scaled_features, classes, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8127853881278538\n",
      "[0.76190476 0.76190476 0.88095238 0.81927711 0.8313253  0.71084337\n",
      " 0.79518072 0.82926829 0.8902439  0.79268293]\n",
      "0.8073583532737221\n"
     ]
    }
   ],
   "source": [
    "ll_classifier = LogisticRegression(solver=\"liblinear\")\n",
    "ll_classifier = ll_classifier.fit(x_train, y_train)\n",
    "print(ll_classifier.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(ll_classifier, scaled_features, classes, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "As a bonus challenge, let's see if an artificial neural network can do even better. You can use Keras to set up a neural network with 1 binary output neuron and see how it performs. Don't be afraid to run a large number of epochs to train the model if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 747 samples\n",
      "Epoch 1/150\n",
      "747/747 - 1s - loss: 0.6470 - accuracy: 0.6158\n",
      "Epoch 2/150\n",
      "747/747 - 0s - loss: 0.6077 - accuracy: 0.7015\n",
      "Epoch 3/150\n",
      "747/747 - 0s - loss: 0.5772 - accuracy: 0.7323\n",
      "Epoch 4/150\n",
      "747/747 - 0s - loss: 0.5478 - accuracy: 0.7537\n",
      "Epoch 5/150\n",
      "747/747 - 0s - loss: 0.4986 - accuracy: 0.7898\n",
      "Epoch 6/150\n",
      "747/747 - 0s - loss: 0.5055 - accuracy: 0.7858\n",
      "Epoch 7/150\n",
      "747/747 - 0s - loss: 0.5031 - accuracy: 0.7724\n",
      "Epoch 8/150\n",
      "747/747 - 0s - loss: 0.4828 - accuracy: 0.7898\n",
      "Epoch 9/150\n",
      "747/747 - 0s - loss: 0.4737 - accuracy: 0.7831\n",
      "Epoch 10/150\n",
      "747/747 - 0s - loss: 0.4764 - accuracy: 0.7831\n",
      "Epoch 11/150\n",
      "747/747 - 0s - loss: 0.4880 - accuracy: 0.7845\n",
      "Epoch 12/150\n",
      "747/747 - 0s - loss: 0.4796 - accuracy: 0.7805\n",
      "Epoch 13/150\n",
      "747/747 - 0s - loss: 0.4741 - accuracy: 0.7965\n",
      "Epoch 14/150\n",
      "747/747 - 0s - loss: 0.4718 - accuracy: 0.7885\n",
      "Epoch 15/150\n",
      "747/747 - 0s - loss: 0.4658 - accuracy: 0.8046\n",
      "Epoch 16/150\n",
      "747/747 - 0s - loss: 0.4731 - accuracy: 0.7965\n",
      "Epoch 17/150\n",
      "747/747 - 0s - loss: 0.4677 - accuracy: 0.7979\n",
      "Epoch 18/150\n",
      "747/747 - 0s - loss: 0.4776 - accuracy: 0.8019\n",
      "Epoch 19/150\n",
      "747/747 - 0s - loss: 0.4603 - accuracy: 0.8019\n",
      "Epoch 20/150\n",
      "747/747 - 0s - loss: 0.4679 - accuracy: 0.7979\n",
      "Epoch 21/150\n",
      "747/747 - 0s - loss: 0.4553 - accuracy: 0.8019\n",
      "Epoch 22/150\n",
      "747/747 - 0s - loss: 0.4500 - accuracy: 0.8032\n",
      "Epoch 23/150\n",
      "747/747 - 0s - loss: 0.4715 - accuracy: 0.7965\n",
      "Epoch 24/150\n",
      "747/747 - 0s - loss: 0.4528 - accuracy: 0.8046\n",
      "Epoch 25/150\n",
      "747/747 - 0s - loss: 0.4692 - accuracy: 0.7965\n",
      "Epoch 26/150\n",
      "747/747 - 0s - loss: 0.4574 - accuracy: 0.7938\n",
      "Epoch 27/150\n",
      "747/747 - 0s - loss: 0.4700 - accuracy: 0.7992\n",
      "Epoch 28/150\n",
      "747/747 - 0s - loss: 0.4611 - accuracy: 0.8046\n",
      "Epoch 29/150\n",
      "747/747 - 0s - loss: 0.4600 - accuracy: 0.7965\n",
      "Epoch 30/150\n",
      "747/747 - 0s - loss: 0.4672 - accuracy: 0.8005\n",
      "Epoch 31/150\n",
      "747/747 - 0s - loss: 0.4728 - accuracy: 0.7858\n",
      "Epoch 32/150\n",
      "747/747 - 0s - loss: 0.4348 - accuracy: 0.8099\n",
      "Epoch 33/150\n",
      "747/747 - 0s - loss: 0.4509 - accuracy: 0.8005\n",
      "Epoch 34/150\n",
      "747/747 - 0s - loss: 0.4490 - accuracy: 0.8059\n",
      "Epoch 35/150\n",
      "747/747 - 0s - loss: 0.4585 - accuracy: 0.8032\n",
      "Epoch 36/150\n",
      "747/747 - 0s - loss: 0.4482 - accuracy: 0.8059\n",
      "Epoch 37/150\n",
      "747/747 - 0s - loss: 0.4481 - accuracy: 0.8019\n",
      "Epoch 38/150\n",
      "747/747 - 0s - loss: 0.4552 - accuracy: 0.7938\n",
      "Epoch 39/150\n",
      "747/747 - 0s - loss: 0.4570 - accuracy: 0.7992\n",
      "Epoch 40/150\n",
      "747/747 - 0s - loss: 0.4575 - accuracy: 0.8046\n",
      "Epoch 41/150\n",
      "747/747 - 0s - loss: 0.4476 - accuracy: 0.7992\n",
      "Epoch 42/150\n",
      "747/747 - 0s - loss: 0.4516 - accuracy: 0.8139\n",
      "Epoch 43/150\n",
      "747/747 - 0s - loss: 0.4490 - accuracy: 0.8193\n",
      "Epoch 44/150\n",
      "747/747 - 0s - loss: 0.4546 - accuracy: 0.8046\n",
      "Epoch 45/150\n",
      "747/747 - 0s - loss: 0.4606 - accuracy: 0.8059\n",
      "Epoch 46/150\n",
      "747/747 - 0s - loss: 0.4432 - accuracy: 0.8166\n",
      "Epoch 47/150\n",
      "747/747 - 0s - loss: 0.4409 - accuracy: 0.8139\n",
      "Epoch 48/150\n",
      "747/747 - 0s - loss: 0.4457 - accuracy: 0.8032\n",
      "Epoch 49/150\n",
      "747/747 - 0s - loss: 0.4587 - accuracy: 0.8072\n",
      "Epoch 50/150\n",
      "747/747 - 0s - loss: 0.4437 - accuracy: 0.8059\n",
      "Epoch 51/150\n",
      "747/747 - 0s - loss: 0.4483 - accuracy: 0.8019\n",
      "Epoch 52/150\n",
      "747/747 - 0s - loss: 0.4526 - accuracy: 0.8046\n",
      "Epoch 53/150\n",
      "747/747 - 0s - loss: 0.4550 - accuracy: 0.8086\n",
      "Epoch 54/150\n",
      "747/747 - 0s - loss: 0.4464 - accuracy: 0.8019\n",
      "Epoch 55/150\n",
      "747/747 - 0s - loss: 0.4459 - accuracy: 0.8046\n",
      "Epoch 56/150\n",
      "747/747 - 0s - loss: 0.4445 - accuracy: 0.8059\n",
      "Epoch 57/150\n",
      "747/747 - 0s - loss: 0.4490 - accuracy: 0.8112\n",
      "Epoch 58/150\n",
      "747/747 - 0s - loss: 0.4584 - accuracy: 0.7992\n",
      "Epoch 59/150\n",
      "747/747 - 0s - loss: 0.4461 - accuracy: 0.8059\n",
      "Epoch 60/150\n",
      "747/747 - 0s - loss: 0.4428 - accuracy: 0.8019\n",
      "Epoch 61/150\n",
      "747/747 - 0s - loss: 0.4548 - accuracy: 0.8059\n",
      "Epoch 62/150\n",
      "747/747 - 0s - loss: 0.4465 - accuracy: 0.8099\n",
      "Epoch 63/150\n",
      "747/747 - 0s - loss: 0.4539 - accuracy: 0.8046\n",
      "Epoch 64/150\n",
      "747/747 - 0s - loss: 0.4555 - accuracy: 0.8046\n",
      "Epoch 65/150\n",
      "747/747 - 0s - loss: 0.4408 - accuracy: 0.8126\n",
      "Epoch 66/150\n",
      "747/747 - 0s - loss: 0.4363 - accuracy: 0.8193\n",
      "Epoch 67/150\n",
      "747/747 - 0s - loss: 0.4462 - accuracy: 0.8099\n",
      "Epoch 68/150\n",
      "747/747 - 0s - loss: 0.4333 - accuracy: 0.8153\n",
      "Epoch 69/150\n",
      "747/747 - 0s - loss: 0.4386 - accuracy: 0.8126\n",
      "Epoch 70/150\n",
      "747/747 - 0s - loss: 0.4402 - accuracy: 0.8193\n",
      "Epoch 71/150\n",
      "747/747 - 0s - loss: 0.4420 - accuracy: 0.8179\n",
      "Epoch 72/150\n",
      "747/747 - 0s - loss: 0.4486 - accuracy: 0.8072\n",
      "Epoch 73/150\n",
      "747/747 - 0s - loss: 0.4456 - accuracy: 0.8139\n",
      "Epoch 74/150\n",
      "747/747 - 0s - loss: 0.4514 - accuracy: 0.8086\n",
      "Epoch 75/150\n",
      "747/747 - 0s - loss: 0.4437 - accuracy: 0.8099\n",
      "Epoch 76/150\n",
      "747/747 - 0s - loss: 0.4453 - accuracy: 0.8112\n",
      "Epoch 77/150\n",
      "747/747 - 0s - loss: 0.4353 - accuracy: 0.8072\n",
      "Epoch 78/150\n",
      "747/747 - 0s - loss: 0.4389 - accuracy: 0.8072\n",
      "Epoch 79/150\n",
      "747/747 - 0s - loss: 0.4448 - accuracy: 0.8086\n",
      "Epoch 80/150\n",
      "747/747 - 0s - loss: 0.4347 - accuracy: 0.8072\n",
      "Epoch 81/150\n",
      "747/747 - 0s - loss: 0.4461 - accuracy: 0.8032\n",
      "Epoch 82/150\n",
      "747/747 - 0s - loss: 0.4367 - accuracy: 0.8032\n",
      "Epoch 83/150\n",
      "747/747 - 0s - loss: 0.4369 - accuracy: 0.8153\n",
      "Epoch 84/150\n",
      "747/747 - 0s - loss: 0.4449 - accuracy: 0.8046\n",
      "Epoch 85/150\n",
      "747/747 - 0s - loss: 0.4477 - accuracy: 0.8046\n",
      "Epoch 86/150\n",
      "747/747 - 0s - loss: 0.4371 - accuracy: 0.8019\n",
      "Epoch 87/150\n",
      "747/747 - 0s - loss: 0.4383 - accuracy: 0.8139\n",
      "Epoch 88/150\n",
      "747/747 - 0s - loss: 0.4435 - accuracy: 0.8072\n",
      "Epoch 89/150\n",
      "747/747 - 0s - loss: 0.4507 - accuracy: 0.8046\n",
      "Epoch 90/150\n",
      "747/747 - 0s - loss: 0.4512 - accuracy: 0.8099\n",
      "Epoch 91/150\n",
      "747/747 - 0s - loss: 0.4322 - accuracy: 0.8086\n",
      "Epoch 92/150\n",
      "747/747 - 0s - loss: 0.4366 - accuracy: 0.8112\n",
      "Epoch 93/150\n",
      "747/747 - 0s - loss: 0.4393 - accuracy: 0.8032\n",
      "Epoch 94/150\n",
      "747/747 - 0s - loss: 0.4394 - accuracy: 0.8019\n",
      "Epoch 95/150\n",
      "747/747 - 0s - loss: 0.4374 - accuracy: 0.8086\n",
      "Epoch 96/150\n",
      "747/747 - 0s - loss: 0.4485 - accuracy: 0.8046\n",
      "Epoch 97/150\n",
      "747/747 - 0s - loss: 0.4415 - accuracy: 0.8005\n",
      "Epoch 98/150\n",
      "747/747 - 0s - loss: 0.4371 - accuracy: 0.8099\n",
      "Epoch 99/150\n",
      "747/747 - 0s - loss: 0.4338 - accuracy: 0.8072\n",
      "Epoch 100/150\n",
      "747/747 - 0s - loss: 0.4388 - accuracy: 0.8112\n",
      "Epoch 101/150\n",
      "747/747 - 0s - loss: 0.4470 - accuracy: 0.8019\n",
      "Epoch 102/150\n",
      "747/747 - 0s - loss: 0.4392 - accuracy: 0.8206\n",
      "Epoch 103/150\n",
      "747/747 - 0s - loss: 0.4433 - accuracy: 0.8086\n",
      "Epoch 104/150\n",
      "747/747 - 0s - loss: 0.4366 - accuracy: 0.8166\n",
      "Epoch 105/150\n",
      "747/747 - 0s - loss: 0.4353 - accuracy: 0.8059\n",
      "Epoch 106/150\n",
      "747/747 - 0s - loss: 0.4337 - accuracy: 0.8046\n",
      "Epoch 107/150\n",
      "747/747 - 0s - loss: 0.4364 - accuracy: 0.8112\n",
      "Epoch 108/150\n",
      "747/747 - 0s - loss: 0.4356 - accuracy: 0.8099\n",
      "Epoch 109/150\n",
      "747/747 - 0s - loss: 0.4385 - accuracy: 0.8005\n",
      "Epoch 110/150\n",
      "747/747 - 0s - loss: 0.4410 - accuracy: 0.8019\n",
      "Epoch 111/150\n",
      "747/747 - 0s - loss: 0.4453 - accuracy: 0.8046\n",
      "Epoch 112/150\n",
      "747/747 - 0s - loss: 0.4323 - accuracy: 0.8046\n",
      "Epoch 113/150\n",
      "747/747 - 0s - loss: 0.4480 - accuracy: 0.8059\n",
      "Epoch 114/150\n",
      "747/747 - 0s - loss: 0.4375 - accuracy: 0.8179\n",
      "Epoch 115/150\n",
      "747/747 - 0s - loss: 0.4329 - accuracy: 0.8046\n",
      "Epoch 116/150\n",
      "747/747 - 0s - loss: 0.4280 - accuracy: 0.8046\n",
      "Epoch 117/150\n",
      "747/747 - 0s - loss: 0.4335 - accuracy: 0.8046\n",
      "Epoch 118/150\n",
      "747/747 - 0s - loss: 0.4369 - accuracy: 0.8086\n",
      "Epoch 119/150\n",
      "747/747 - 0s - loss: 0.4454 - accuracy: 0.8019\n",
      "Epoch 120/150\n",
      "747/747 - 0s - loss: 0.4395 - accuracy: 0.8046\n",
      "Epoch 121/150\n",
      "747/747 - 0s - loss: 0.4433 - accuracy: 0.8059\n",
      "Epoch 122/150\n",
      "747/747 - 0s - loss: 0.4413 - accuracy: 0.8072\n",
      "Epoch 123/150\n",
      "747/747 - 0s - loss: 0.4362 - accuracy: 0.8126\n",
      "Epoch 124/150\n",
      "747/747 - 0s - loss: 0.4482 - accuracy: 0.8059\n",
      "Epoch 125/150\n",
      "747/747 - 0s - loss: 0.4330 - accuracy: 0.8072\n",
      "Epoch 126/150\n",
      "747/747 - 0s - loss: 0.4414 - accuracy: 0.8099\n",
      "Epoch 127/150\n",
      "747/747 - 0s - loss: 0.4327 - accuracy: 0.8072\n",
      "Epoch 128/150\n",
      "747/747 - 0s - loss: 0.4359 - accuracy: 0.8086\n",
      "Epoch 129/150\n",
      "747/747 - 0s - loss: 0.4437 - accuracy: 0.8059\n",
      "Epoch 130/150\n",
      "747/747 - 0s - loss: 0.4371 - accuracy: 0.8179\n",
      "Epoch 131/150\n",
      "747/747 - 0s - loss: 0.4342 - accuracy: 0.8046\n",
      "Epoch 132/150\n",
      "747/747 - 0s - loss: 0.4350 - accuracy: 0.8046\n",
      "Epoch 133/150\n",
      "747/747 - 0s - loss: 0.4338 - accuracy: 0.8126\n",
      "Epoch 134/150\n",
      "747/747 - 0s - loss: 0.4353 - accuracy: 0.8086\n",
      "Epoch 135/150\n",
      "747/747 - 0s - loss: 0.4343 - accuracy: 0.8179\n",
      "Epoch 136/150\n",
      "747/747 - 0s - loss: 0.4354 - accuracy: 0.8072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/150\n",
      "747/747 - 0s - loss: 0.4323 - accuracy: 0.8059\n",
      "Epoch 138/150\n",
      "747/747 - 0s - loss: 0.4292 - accuracy: 0.8179\n",
      "Epoch 139/150\n",
      "747/747 - 0s - loss: 0.4369 - accuracy: 0.8139\n",
      "Epoch 140/150\n",
      "747/747 - 0s - loss: 0.4398 - accuracy: 0.8059\n",
      "Epoch 141/150\n",
      "747/747 - 0s - loss: 0.4328 - accuracy: 0.8086\n",
      "Epoch 142/150\n",
      "747/747 - 0s - loss: 0.4317 - accuracy: 0.8126\n",
      "Epoch 143/150\n",
      "747/747 - 0s - loss: 0.4327 - accuracy: 0.8032\n",
      "Epoch 144/150\n",
      "747/747 - 0s - loss: 0.4370 - accuracy: 0.8153\n",
      "Epoch 145/150\n",
      "747/747 - 0s - loss: 0.4331 - accuracy: 0.8153\n",
      "Epoch 146/150\n",
      "747/747 - 0s - loss: 0.4262 - accuracy: 0.8246\n",
      "Epoch 147/150\n",
      "747/747 - 0s - loss: 0.4403 - accuracy: 0.8032\n",
      "Epoch 148/150\n",
      "747/747 - 0s - loss: 0.4312 - accuracy: 0.8153\n",
      "Epoch 149/150\n",
      "747/747 - 0s - loss: 0.4346 - accuracy: 0.8112\n",
      "Epoch 150/150\n",
      "747/747 - 0s - loss: 0.4369 - accuracy: 0.8086\n",
      "83/1 - 0s - loss: 0.4283 - accuracy: 0.7349\n",
      "Train on 747 samples\n",
      "Epoch 1/150\n",
      "747/747 - 1s - loss: 0.7011 - accuracy: 0.5823\n",
      "Epoch 2/150\n",
      "747/747 - 0s - loss: 0.6228 - accuracy: 0.6667\n",
      "Epoch 3/150\n",
      "747/747 - 0s - loss: 0.5737 - accuracy: 0.7135\n",
      "Epoch 4/150\n",
      "747/747 - 0s - loss: 0.5455 - accuracy: 0.7349\n",
      "Epoch 5/150\n",
      "747/747 - 0s - loss: 0.5202 - accuracy: 0.7631\n",
      "Epoch 6/150\n",
      "747/747 - 0s - loss: 0.4999 - accuracy: 0.7537\n",
      "Epoch 7/150\n",
      "747/747 - 0s - loss: 0.4934 - accuracy: 0.7604\n",
      "Epoch 8/150\n",
      "747/747 - 0s - loss: 0.5202 - accuracy: 0.7684\n",
      "Epoch 9/150\n",
      "747/747 - 0s - loss: 0.5114 - accuracy: 0.7644\n",
      "Epoch 10/150\n",
      "747/747 - 0s - loss: 0.4747 - accuracy: 0.7965\n",
      "Epoch 11/150\n",
      "747/747 - 0s - loss: 0.4972 - accuracy: 0.7805\n",
      "Epoch 12/150\n",
      "747/747 - 0s - loss: 0.4939 - accuracy: 0.7778\n",
      "Epoch 13/150\n",
      "747/747 - 0s - loss: 0.4767 - accuracy: 0.7938\n",
      "Epoch 14/150\n",
      "747/747 - 0s - loss: 0.4870 - accuracy: 0.7871\n",
      "Epoch 15/150\n",
      "747/747 - 0s - loss: 0.4774 - accuracy: 0.7885\n",
      "Epoch 16/150\n",
      "747/747 - 0s - loss: 0.4914 - accuracy: 0.7871\n",
      "Epoch 17/150\n",
      "747/747 - 0s - loss: 0.4813 - accuracy: 0.7845\n",
      "Epoch 18/150\n",
      "747/747 - 0s - loss: 0.4723 - accuracy: 0.7925\n",
      "Epoch 19/150\n",
      "747/747 - 0s - loss: 0.4758 - accuracy: 0.8046\n",
      "Epoch 20/150\n",
      "747/747 - 0s - loss: 0.4787 - accuracy: 0.7965\n",
      "Epoch 21/150\n",
      "747/747 - 0s - loss: 0.4771 - accuracy: 0.7992\n",
      "Epoch 22/150\n",
      "747/747 - 0s - loss: 0.4677 - accuracy: 0.7912\n",
      "Epoch 23/150\n",
      "747/747 - 0s - loss: 0.4574 - accuracy: 0.7871\n",
      "Epoch 24/150\n",
      "747/747 - 0s - loss: 0.4663 - accuracy: 0.7965\n",
      "Epoch 25/150\n",
      "747/747 - 0s - loss: 0.4657 - accuracy: 0.8019\n",
      "Epoch 26/150\n",
      "747/747 - 0s - loss: 0.4650 - accuracy: 0.7965\n",
      "Epoch 27/150\n",
      "747/747 - 0s - loss: 0.4607 - accuracy: 0.8046\n",
      "Epoch 28/150\n",
      "747/747 - 0s - loss: 0.4730 - accuracy: 0.7952\n",
      "Epoch 29/150\n",
      "747/747 - 0s - loss: 0.4591 - accuracy: 0.7992\n",
      "Epoch 30/150\n",
      "747/747 - 0s - loss: 0.4775 - accuracy: 0.7912\n",
      "Epoch 31/150\n",
      "747/747 - 0s - loss: 0.4735 - accuracy: 0.7885\n",
      "Epoch 32/150\n",
      "747/747 - 0s - loss: 0.4762 - accuracy: 0.7912\n",
      "Epoch 33/150\n",
      "747/747 - 0s - loss: 0.4635 - accuracy: 0.7898\n",
      "Epoch 34/150\n",
      "747/747 - 0s - loss: 0.4558 - accuracy: 0.8046\n",
      "Epoch 35/150\n",
      "747/747 - 0s - loss: 0.4581 - accuracy: 0.8019\n",
      "Epoch 36/150\n",
      "747/747 - 0s - loss: 0.4634 - accuracy: 0.8046\n",
      "Epoch 37/150\n",
      "747/747 - 0s - loss: 0.4749 - accuracy: 0.8019\n",
      "Epoch 38/150\n",
      "747/747 - 0s - loss: 0.4669 - accuracy: 0.7979\n",
      "Epoch 39/150\n",
      "747/747 - 0s - loss: 0.4511 - accuracy: 0.7925\n",
      "Epoch 40/150\n",
      "747/747 - 0s - loss: 0.4677 - accuracy: 0.7952\n",
      "Epoch 41/150\n",
      "747/747 - 0s - loss: 0.4608 - accuracy: 0.8046\n",
      "Epoch 42/150\n",
      "747/747 - 0s - loss: 0.4754 - accuracy: 0.8046\n",
      "Epoch 43/150\n",
      "747/747 - 0s - loss: 0.4602 - accuracy: 0.7952\n",
      "Epoch 44/150\n",
      "747/747 - 0s - loss: 0.4617 - accuracy: 0.7992\n",
      "Epoch 45/150\n",
      "747/747 - 0s - loss: 0.4620 - accuracy: 0.7979\n",
      "Epoch 46/150\n",
      "747/747 - 0s - loss: 0.4788 - accuracy: 0.8046\n",
      "Epoch 47/150\n",
      "747/747 - 0s - loss: 0.4614 - accuracy: 0.8019\n",
      "Epoch 48/150\n",
      "747/747 - 0s - loss: 0.4585 - accuracy: 0.7885\n",
      "Epoch 49/150\n",
      "747/747 - 0s - loss: 0.4462 - accuracy: 0.8126\n",
      "Epoch 50/150\n",
      "747/747 - 0s - loss: 0.4639 - accuracy: 0.7979\n",
      "Epoch 51/150\n",
      "747/747 - 0s - loss: 0.4666 - accuracy: 0.7965\n",
      "Epoch 52/150\n",
      "747/747 - 0s - loss: 0.4621 - accuracy: 0.7979\n",
      "Epoch 53/150\n",
      "747/747 - 0s - loss: 0.4582 - accuracy: 0.7979\n",
      "Epoch 54/150\n",
      "747/747 - 0s - loss: 0.4597 - accuracy: 0.7925\n",
      "Epoch 55/150\n",
      "747/747 - 0s - loss: 0.4487 - accuracy: 0.8032\n",
      "Epoch 56/150\n",
      "747/747 - 0s - loss: 0.4642 - accuracy: 0.7912\n",
      "Epoch 57/150\n",
      "747/747 - 0s - loss: 0.4603 - accuracy: 0.8032\n",
      "Epoch 58/150\n",
      "747/747 - 0s - loss: 0.4709 - accuracy: 0.7938\n",
      "Epoch 59/150\n",
      "747/747 - 0s - loss: 0.4474 - accuracy: 0.8126\n",
      "Epoch 60/150\n",
      "747/747 - 0s - loss: 0.4539 - accuracy: 0.8072\n",
      "Epoch 61/150\n",
      "747/747 - 0s - loss: 0.4536 - accuracy: 0.7992\n",
      "Epoch 62/150\n",
      "747/747 - 0s - loss: 0.4523 - accuracy: 0.8126\n",
      "Epoch 63/150\n",
      "747/747 - 0s - loss: 0.4572 - accuracy: 0.8153\n",
      "Epoch 64/150\n",
      "747/747 - 0s - loss: 0.4497 - accuracy: 0.8059\n",
      "Epoch 65/150\n",
      "747/747 - 0s - loss: 0.4569 - accuracy: 0.8032\n",
      "Epoch 66/150\n",
      "747/747 - 0s - loss: 0.4602 - accuracy: 0.8059\n",
      "Epoch 67/150\n",
      "747/747 - 0s - loss: 0.4591 - accuracy: 0.8032\n",
      "Epoch 68/150\n",
      "747/747 - 0s - loss: 0.4586 - accuracy: 0.8072\n",
      "Epoch 69/150\n",
      "747/747 - 0s - loss: 0.4492 - accuracy: 0.8032\n",
      "Epoch 70/150\n",
      "747/747 - 0s - loss: 0.4571 - accuracy: 0.8126\n",
      "Epoch 71/150\n",
      "747/747 - 0s - loss: 0.4551 - accuracy: 0.8046\n",
      "Epoch 72/150\n",
      "747/747 - 0s - loss: 0.4645 - accuracy: 0.8086\n",
      "Epoch 73/150\n",
      "747/747 - 0s - loss: 0.4523 - accuracy: 0.8086\n",
      "Epoch 74/150\n",
      "747/747 - 0s - loss: 0.4562 - accuracy: 0.8019\n",
      "Epoch 75/150\n",
      "747/747 - 0s - loss: 0.4495 - accuracy: 0.8059\n",
      "Epoch 76/150\n",
      "747/747 - 0s - loss: 0.4577 - accuracy: 0.8153\n",
      "Epoch 77/150\n",
      "747/747 - 0s - loss: 0.4575 - accuracy: 0.8046\n",
      "Epoch 78/150\n",
      "747/747 - 0s - loss: 0.4447 - accuracy: 0.8059\n",
      "Epoch 79/150\n",
      "747/747 - 0s - loss: 0.4451 - accuracy: 0.8126\n",
      "Epoch 80/150\n",
      "747/747 - 0s - loss: 0.4574 - accuracy: 0.8179\n",
      "Epoch 81/150\n",
      "747/747 - 0s - loss: 0.4518 - accuracy: 0.8153\n",
      "Epoch 82/150\n",
      "747/747 - 0s - loss: 0.4582 - accuracy: 0.8072\n",
      "Epoch 83/150\n",
      "747/747 - 0s - loss: 0.4474 - accuracy: 0.8112\n",
      "Epoch 84/150\n",
      "747/747 - 0s - loss: 0.4498 - accuracy: 0.8153\n",
      "Epoch 85/150\n",
      "747/747 - 0s - loss: 0.4482 - accuracy: 0.8153\n",
      "Epoch 86/150\n",
      "747/747 - 0s - loss: 0.4486 - accuracy: 0.8126\n",
      "Epoch 87/150\n",
      "747/747 - 0s - loss: 0.4425 - accuracy: 0.8166\n",
      "Epoch 88/150\n",
      "747/747 - 0s - loss: 0.4583 - accuracy: 0.8059\n",
      "Epoch 89/150\n",
      "747/747 - 0s - loss: 0.4573 - accuracy: 0.8086\n",
      "Epoch 90/150\n",
      "747/747 - 0s - loss: 0.4416 - accuracy: 0.8139\n",
      "Epoch 91/150\n",
      "747/747 - 0s - loss: 0.4594 - accuracy: 0.8112\n",
      "Epoch 92/150\n",
      "747/747 - 0s - loss: 0.4609 - accuracy: 0.8019\n",
      "Epoch 93/150\n",
      "747/747 - 0s - loss: 0.4490 - accuracy: 0.8019\n",
      "Epoch 94/150\n",
      "747/747 - 0s - loss: 0.4528 - accuracy: 0.8112\n",
      "Epoch 95/150\n",
      "747/747 - 0s - loss: 0.4547 - accuracy: 0.8032\n",
      "Epoch 96/150\n",
      "747/747 - 0s - loss: 0.4539 - accuracy: 0.8112\n",
      "Epoch 97/150\n",
      "747/747 - 0s - loss: 0.4415 - accuracy: 0.8153\n",
      "Epoch 98/150\n",
      "747/747 - 0s - loss: 0.4508 - accuracy: 0.8032\n",
      "Epoch 99/150\n",
      "747/747 - 0s - loss: 0.4455 - accuracy: 0.8139\n",
      "Epoch 100/150\n",
      "747/747 - 0s - loss: 0.4510 - accuracy: 0.8086\n",
      "Epoch 101/150\n",
      "747/747 - 0s - loss: 0.4412 - accuracy: 0.8099\n",
      "Epoch 102/150\n",
      "747/747 - 0s - loss: 0.4506 - accuracy: 0.8126\n",
      "Epoch 103/150\n",
      "747/747 - 0s - loss: 0.4524 - accuracy: 0.8032\n",
      "Epoch 104/150\n",
      "747/747 - 0s - loss: 0.4447 - accuracy: 0.8059\n",
      "Epoch 105/150\n",
      "747/747 - 0s - loss: 0.4417 - accuracy: 0.8112\n",
      "Epoch 106/150\n",
      "747/747 - 0s - loss: 0.4547 - accuracy: 0.8059\n",
      "Epoch 107/150\n",
      "747/747 - 0s - loss: 0.4471 - accuracy: 0.8072\n",
      "Epoch 108/150\n",
      "747/747 - 0s - loss: 0.4444 - accuracy: 0.8046\n",
      "Epoch 109/150\n",
      "747/747 - 0s - loss: 0.4484 - accuracy: 0.8072\n",
      "Epoch 110/150\n",
      "747/747 - 0s - loss: 0.4519 - accuracy: 0.8032\n",
      "Epoch 111/150\n",
      "747/747 - 0s - loss: 0.4516 - accuracy: 0.8099\n",
      "Epoch 112/150\n",
      "747/747 - 0s - loss: 0.4427 - accuracy: 0.8086\n",
      "Epoch 113/150\n",
      "747/747 - 0s - loss: 0.4444 - accuracy: 0.8086\n",
      "Epoch 114/150\n",
      "747/747 - 0s - loss: 0.4498 - accuracy: 0.8046\n",
      "Epoch 115/150\n",
      "747/747 - 0s - loss: 0.4499 - accuracy: 0.8059\n",
      "Epoch 116/150\n",
      "747/747 - 0s - loss: 0.4522 - accuracy: 0.8126\n",
      "Epoch 117/150\n",
      "747/747 - 0s - loss: 0.4422 - accuracy: 0.8072\n",
      "Epoch 118/150\n",
      "747/747 - 0s - loss: 0.4482 - accuracy: 0.8153\n",
      "Epoch 119/150\n",
      "747/747 - 0s - loss: 0.4541 - accuracy: 0.8059\n",
      "Epoch 120/150\n",
      "747/747 - 0s - loss: 0.4392 - accuracy: 0.8046\n",
      "Epoch 121/150\n",
      "747/747 - 0s - loss: 0.4490 - accuracy: 0.8086\n",
      "Epoch 122/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747/747 - 0s - loss: 0.4484 - accuracy: 0.8086\n",
      "Epoch 123/150\n",
      "747/747 - 0s - loss: 0.4459 - accuracy: 0.8059\n",
      "Epoch 124/150\n",
      "747/747 - 0s - loss: 0.4374 - accuracy: 0.8153\n",
      "Epoch 125/150\n",
      "747/747 - 0s - loss: 0.4407 - accuracy: 0.8059\n",
      "Epoch 126/150\n",
      "747/747 - 0s - loss: 0.4472 - accuracy: 0.8126\n",
      "Epoch 127/150\n",
      "747/747 - 0s - loss: 0.4396 - accuracy: 0.8072\n",
      "Epoch 128/150\n",
      "747/747 - 0s - loss: 0.4430 - accuracy: 0.8086\n",
      "Epoch 129/150\n",
      "747/747 - 0s - loss: 0.4565 - accuracy: 0.8019\n",
      "Epoch 130/150\n",
      "747/747 - 0s - loss: 0.4402 - accuracy: 0.8086\n",
      "Epoch 131/150\n",
      "747/747 - 0s - loss: 0.4425 - accuracy: 0.8166\n",
      "Epoch 132/150\n",
      "747/747 - 0s - loss: 0.4496 - accuracy: 0.8166\n",
      "Epoch 133/150\n",
      "747/747 - 0s - loss: 0.4499 - accuracy: 0.8032\n",
      "Epoch 134/150\n",
      "747/747 - 0s - loss: 0.4446 - accuracy: 0.8139\n",
      "Epoch 135/150\n",
      "747/747 - 0s - loss: 0.4438 - accuracy: 0.8086\n",
      "Epoch 136/150\n",
      "747/747 - 0s - loss: 0.4469 - accuracy: 0.8193\n",
      "Epoch 137/150\n",
      "747/747 - 0s - loss: 0.4516 - accuracy: 0.8059\n",
      "Epoch 138/150\n",
      "747/747 - 0s - loss: 0.4411 - accuracy: 0.8126\n",
      "Epoch 139/150\n",
      "747/747 - 0s - loss: 0.4413 - accuracy: 0.8179\n",
      "Epoch 140/150\n",
      "747/747 - 0s - loss: 0.4429 - accuracy: 0.8112\n",
      "Epoch 141/150\n",
      "747/747 - 0s - loss: 0.4441 - accuracy: 0.8005\n",
      "Epoch 142/150\n",
      "747/747 - 0s - loss: 0.4440 - accuracy: 0.8099\n",
      "Epoch 143/150\n",
      "747/747 - 0s - loss: 0.4449 - accuracy: 0.8099\n",
      "Epoch 144/150\n",
      "747/747 - 0s - loss: 0.4352 - accuracy: 0.8166\n",
      "Epoch 145/150\n",
      "747/747 - 0s - loss: 0.4367 - accuracy: 0.8166\n",
      "Epoch 146/150\n",
      "747/747 - 0s - loss: 0.4471 - accuracy: 0.8072\n",
      "Epoch 147/150\n",
      "747/747 - 0s - loss: 0.4369 - accuracy: 0.8166\n",
      "Epoch 148/150\n",
      "747/747 - 0s - loss: 0.4466 - accuracy: 0.8086\n",
      "Epoch 149/150\n",
      "747/747 - 0s - loss: 0.4489 - accuracy: 0.8112\n",
      "Epoch 150/150\n",
      "747/747 - 0s - loss: 0.4515 - accuracy: 0.8086\n",
      "83/1 - 0s - loss: 0.5336 - accuracy: 0.7831\n",
      "Train on 747 samples\n",
      "Epoch 1/150\n",
      "747/747 - 1s - loss: 0.6237 - accuracy: 0.6814\n",
      "Epoch 2/150\n",
      "747/747 - 0s - loss: 0.5725 - accuracy: 0.7242\n",
      "Epoch 3/150\n",
      "747/747 - 0s - loss: 0.5386 - accuracy: 0.7510\n",
      "Epoch 4/150\n",
      "747/747 - 0s - loss: 0.5385 - accuracy: 0.7550\n",
      "Epoch 5/150\n",
      "747/747 - 0s - loss: 0.5154 - accuracy: 0.7724\n",
      "Epoch 6/150\n",
      "747/747 - 0s - loss: 0.4986 - accuracy: 0.7831\n",
      "Epoch 7/150\n",
      "747/747 - 0s - loss: 0.4921 - accuracy: 0.7871\n",
      "Epoch 8/150\n",
      "747/747 - 0s - loss: 0.5021 - accuracy: 0.7805\n",
      "Epoch 9/150\n",
      "747/747 - 0s - loss: 0.4876 - accuracy: 0.7938\n",
      "Epoch 10/150\n",
      "747/747 - 0s - loss: 0.4915 - accuracy: 0.7845\n",
      "Epoch 11/150\n",
      "747/747 - 0s - loss: 0.4805 - accuracy: 0.7898\n",
      "Epoch 12/150\n",
      "747/747 - 0s - loss: 0.4914 - accuracy: 0.7805\n",
      "Epoch 13/150\n",
      "747/747 - 0s - loss: 0.4895 - accuracy: 0.7871\n",
      "Epoch 14/150\n",
      "747/747 - 0s - loss: 0.4885 - accuracy: 0.7885\n",
      "Epoch 15/150\n",
      "747/747 - 0s - loss: 0.4730 - accuracy: 0.7912\n",
      "Epoch 16/150\n",
      "747/747 - 0s - loss: 0.4964 - accuracy: 0.7764\n",
      "Epoch 17/150\n",
      "747/747 - 0s - loss: 0.4724 - accuracy: 0.7871\n",
      "Epoch 18/150\n",
      "747/747 - 0s - loss: 0.4810 - accuracy: 0.7938\n",
      "Epoch 19/150\n",
      "747/747 - 0s - loss: 0.4765 - accuracy: 0.7912\n",
      "Epoch 20/150\n",
      "747/747 - 0s - loss: 0.4850 - accuracy: 0.7898\n",
      "Epoch 21/150\n",
      "747/747 - 0s - loss: 0.4789 - accuracy: 0.7858\n",
      "Epoch 22/150\n",
      "747/747 - 0s - loss: 0.4858 - accuracy: 0.7805\n",
      "Epoch 23/150\n",
      "747/747 - 0s - loss: 0.4847 - accuracy: 0.7965\n",
      "Epoch 24/150\n",
      "747/747 - 0s - loss: 0.4738 - accuracy: 0.7952\n",
      "Epoch 25/150\n",
      "747/747 - 0s - loss: 0.4867 - accuracy: 0.7778\n",
      "Epoch 26/150\n",
      "747/747 - 0s - loss: 0.4687 - accuracy: 0.7938\n",
      "Epoch 27/150\n",
      "747/747 - 0s - loss: 0.4867 - accuracy: 0.7965\n",
      "Epoch 28/150\n",
      "747/747 - 0s - loss: 0.4874 - accuracy: 0.7952\n",
      "Epoch 29/150\n",
      "747/747 - 0s - loss: 0.4925 - accuracy: 0.7912\n",
      "Epoch 30/150\n",
      "747/747 - 0s - loss: 0.4782 - accuracy: 0.7912\n",
      "Epoch 31/150\n",
      "747/747 - 0s - loss: 0.4861 - accuracy: 0.7858\n",
      "Epoch 32/150\n",
      "747/747 - 0s - loss: 0.4804 - accuracy: 0.7831\n",
      "Epoch 33/150\n",
      "747/747 - 0s - loss: 0.4810 - accuracy: 0.7858\n",
      "Epoch 34/150\n",
      "747/747 - 0s - loss: 0.4810 - accuracy: 0.7965\n",
      "Epoch 35/150\n",
      "747/747 - 0s - loss: 0.4742 - accuracy: 0.7938\n",
      "Epoch 36/150\n",
      "747/747 - 0s - loss: 0.4648 - accuracy: 0.8059\n",
      "Epoch 37/150\n",
      "747/747 - 0s - loss: 0.4844 - accuracy: 0.7965\n",
      "Epoch 38/150\n",
      "747/747 - 0s - loss: 0.4737 - accuracy: 0.7925\n",
      "Epoch 39/150\n",
      "747/747 - 0s - loss: 0.4603 - accuracy: 0.7885\n",
      "Epoch 40/150\n",
      "747/747 - 0s - loss: 0.4747 - accuracy: 0.7952\n",
      "Epoch 41/150\n",
      "747/747 - 0s - loss: 0.4768 - accuracy: 0.7898\n",
      "Epoch 42/150\n",
      "747/747 - 0s - loss: 0.4711 - accuracy: 0.7925\n",
      "Epoch 43/150\n",
      "747/747 - 0s - loss: 0.4837 - accuracy: 0.7871\n",
      "Epoch 44/150\n",
      "747/747 - 0s - loss: 0.4754 - accuracy: 0.7952\n",
      "Epoch 45/150\n",
      "747/747 - 0s - loss: 0.4689 - accuracy: 0.7871\n",
      "Epoch 46/150\n",
      "747/747 - 0s - loss: 0.4714 - accuracy: 0.7979\n",
      "Epoch 47/150\n",
      "747/747 - 0s - loss: 0.4898 - accuracy: 0.7925\n",
      "Epoch 48/150\n",
      "747/747 - 0s - loss: 0.4739 - accuracy: 0.7992\n",
      "Epoch 49/150\n",
      "747/747 - 0s - loss: 0.4664 - accuracy: 0.7979\n",
      "Epoch 50/150\n",
      "747/747 - 0s - loss: 0.4704 - accuracy: 0.7925\n",
      "Epoch 51/150\n",
      "747/747 - 0s - loss: 0.4615 - accuracy: 0.7885\n",
      "Epoch 52/150\n",
      "747/747 - 0s - loss: 0.4752 - accuracy: 0.7925\n",
      "Epoch 53/150\n",
      "747/747 - 0s - loss: 0.4704 - accuracy: 0.7925\n",
      "Epoch 54/150\n",
      "747/747 - 0s - loss: 0.4632 - accuracy: 0.7952\n",
      "Epoch 55/150\n",
      "747/747 - 0s - loss: 0.4736 - accuracy: 0.8046\n",
      "Epoch 56/150\n",
      "747/747 - 0s - loss: 0.4833 - accuracy: 0.7898\n",
      "Epoch 57/150\n",
      "747/747 - 0s - loss: 0.4797 - accuracy: 0.7912\n",
      "Epoch 58/150\n",
      "747/747 - 0s - loss: 0.4699 - accuracy: 0.7992\n",
      "Epoch 59/150\n",
      "747/747 - 0s - loss: 0.4695 - accuracy: 0.8046\n",
      "Epoch 60/150\n",
      "747/747 - 0s - loss: 0.4643 - accuracy: 0.7992\n",
      "Epoch 61/150\n",
      "747/747 - 0s - loss: 0.4466 - accuracy: 0.7979\n",
      "Epoch 62/150\n",
      "747/747 - 0s - loss: 0.4707 - accuracy: 0.8005\n",
      "Epoch 63/150\n",
      "747/747 - 0s - loss: 0.4684 - accuracy: 0.8019\n",
      "Epoch 64/150\n",
      "747/747 - 0s - loss: 0.4578 - accuracy: 0.7979\n",
      "Epoch 65/150\n",
      "747/747 - 0s - loss: 0.4723 - accuracy: 0.7885\n",
      "Epoch 66/150\n",
      "747/747 - 0s - loss: 0.4803 - accuracy: 0.8032\n",
      "Epoch 67/150\n",
      "747/747 - 0s - loss: 0.4646 - accuracy: 0.7952\n",
      "Epoch 68/150\n",
      "747/747 - 0s - loss: 0.4641 - accuracy: 0.8046\n",
      "Epoch 69/150\n",
      "747/747 - 0s - loss: 0.4760 - accuracy: 0.7979\n",
      "Epoch 70/150\n",
      "747/747 - 0s - loss: 0.4629 - accuracy: 0.7979\n",
      "Epoch 71/150\n",
      "747/747 - 0s - loss: 0.4725 - accuracy: 0.7898\n",
      "Epoch 72/150\n",
      "747/747 - 0s - loss: 0.4627 - accuracy: 0.7965\n",
      "Epoch 73/150\n",
      "747/747 - 0s - loss: 0.4594 - accuracy: 0.8019\n",
      "Epoch 74/150\n",
      "747/747 - 0s - loss: 0.4636 - accuracy: 0.7925\n",
      "Epoch 75/150\n",
      "747/747 - 0s - loss: 0.4618 - accuracy: 0.7992\n",
      "Epoch 76/150\n",
      "747/747 - 0s - loss: 0.4718 - accuracy: 0.7805\n",
      "Epoch 77/150\n",
      "747/747 - 0s - loss: 0.4665 - accuracy: 0.8005\n",
      "Epoch 78/150\n",
      "747/747 - 0s - loss: 0.4722 - accuracy: 0.7938\n",
      "Epoch 79/150\n",
      "747/747 - 0s - loss: 0.4681 - accuracy: 0.8112\n",
      "Epoch 80/150\n",
      "747/747 - 0s - loss: 0.4642 - accuracy: 0.7979\n",
      "Epoch 81/150\n",
      "747/747 - 0s - loss: 0.4678 - accuracy: 0.8019\n",
      "Epoch 82/150\n",
      "747/747 - 0s - loss: 0.4729 - accuracy: 0.7952\n",
      "Epoch 83/150\n",
      "747/747 - 0s - loss: 0.4531 - accuracy: 0.8126\n",
      "Epoch 84/150\n",
      "747/747 - 0s - loss: 0.4675 - accuracy: 0.7952\n",
      "Epoch 85/150\n",
      "747/747 - 0s - loss: 0.4632 - accuracy: 0.8059\n",
      "Epoch 86/150\n",
      "747/747 - 0s - loss: 0.4573 - accuracy: 0.7952\n",
      "Epoch 87/150\n",
      "747/747 - 0s - loss: 0.4636 - accuracy: 0.7938\n",
      "Epoch 88/150\n",
      "747/747 - 0s - loss: 0.4589 - accuracy: 0.7925\n",
      "Epoch 89/150\n",
      "747/747 - 0s - loss: 0.4593 - accuracy: 0.7992\n",
      "Epoch 90/150\n",
      "747/747 - 0s - loss: 0.4695 - accuracy: 0.7952\n",
      "Epoch 91/150\n",
      "747/747 - 0s - loss: 0.4673 - accuracy: 0.8005\n",
      "Epoch 92/150\n",
      "747/747 - 0s - loss: 0.4649 - accuracy: 0.7979\n",
      "Epoch 93/150\n",
      "747/747 - 0s - loss: 0.4582 - accuracy: 0.8005\n",
      "Epoch 94/150\n",
      "747/747 - 0s - loss: 0.4669 - accuracy: 0.7965\n",
      "Epoch 95/150\n",
      "747/747 - 0s - loss: 0.4561 - accuracy: 0.8099\n",
      "Epoch 96/150\n",
      "747/747 - 0s - loss: 0.4621 - accuracy: 0.8032\n",
      "Epoch 97/150\n",
      "747/747 - 0s - loss: 0.4672 - accuracy: 0.8019\n",
      "Epoch 98/150\n",
      "747/747 - 0s - loss: 0.4701 - accuracy: 0.8032\n",
      "Epoch 99/150\n",
      "747/747 - 0s - loss: 0.4584 - accuracy: 0.7952\n",
      "Epoch 100/150\n",
      "747/747 - 0s - loss: 0.4695 - accuracy: 0.8005\n",
      "Epoch 101/150\n",
      "747/747 - 0s - loss: 0.4672 - accuracy: 0.8019\n",
      "Epoch 102/150\n",
      "747/747 - 0s - loss: 0.4685 - accuracy: 0.8005\n",
      "Epoch 103/150\n",
      "747/747 - 0s - loss: 0.4674 - accuracy: 0.8032\n",
      "Epoch 104/150\n",
      "747/747 - 0s - loss: 0.4569 - accuracy: 0.8059\n",
      "Epoch 105/150\n",
      "747/747 - 0s - loss: 0.4632 - accuracy: 0.8019\n",
      "Epoch 106/150\n",
      "747/747 - 0s - loss: 0.4544 - accuracy: 0.8019\n",
      "Epoch 107/150\n",
      "747/747 - 0s - loss: 0.4615 - accuracy: 0.8032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/150\n",
      "747/747 - 0s - loss: 0.4656 - accuracy: 0.7952\n",
      "Epoch 109/150\n",
      "747/747 - 0s - loss: 0.4697 - accuracy: 0.8086\n",
      "Epoch 110/150\n",
      "747/747 - 0s - loss: 0.4680 - accuracy: 0.8005\n",
      "Epoch 111/150\n",
      "747/747 - 0s - loss: 0.4702 - accuracy: 0.8019\n",
      "Epoch 112/150\n",
      "747/747 - 0s - loss: 0.4605 - accuracy: 0.8086\n",
      "Epoch 113/150\n",
      "747/747 - 0s - loss: 0.4635 - accuracy: 0.7992\n",
      "Epoch 114/150\n",
      "747/747 - 0s - loss: 0.4640 - accuracy: 0.8072\n",
      "Epoch 115/150\n",
      "747/747 - 0s - loss: 0.4523 - accuracy: 0.8032\n",
      "Epoch 116/150\n",
      "747/747 - 0s - loss: 0.4527 - accuracy: 0.8112\n",
      "Epoch 117/150\n",
      "747/747 - 0s - loss: 0.4613 - accuracy: 0.8046\n",
      "Epoch 118/150\n",
      "747/747 - 0s - loss: 0.4613 - accuracy: 0.7992\n",
      "Epoch 119/150\n",
      "747/747 - 0s - loss: 0.4577 - accuracy: 0.7938\n",
      "Epoch 120/150\n",
      "747/747 - 0s - loss: 0.4573 - accuracy: 0.8032\n",
      "Epoch 121/150\n",
      "747/747 - 0s - loss: 0.4537 - accuracy: 0.8153\n",
      "Epoch 122/150\n",
      "747/747 - 0s - loss: 0.4528 - accuracy: 0.8086\n",
      "Epoch 123/150\n",
      "747/747 - 0s - loss: 0.4632 - accuracy: 0.8099\n",
      "Epoch 124/150\n",
      "747/747 - 0s - loss: 0.4609 - accuracy: 0.8046\n",
      "Epoch 125/150\n",
      "747/747 - 0s - loss: 0.4612 - accuracy: 0.8032\n",
      "Epoch 126/150\n",
      "747/747 - 0s - loss: 0.4625 - accuracy: 0.8046\n",
      "Epoch 127/150\n",
      "747/747 - 0s - loss: 0.4525 - accuracy: 0.8072\n",
      "Epoch 128/150\n",
      "747/747 - 0s - loss: 0.4671 - accuracy: 0.8032\n",
      "Epoch 129/150\n",
      "747/747 - 0s - loss: 0.4568 - accuracy: 0.8112\n",
      "Epoch 130/150\n",
      "747/747 - 0s - loss: 0.4554 - accuracy: 0.8059\n",
      "Epoch 131/150\n",
      "747/747 - 0s - loss: 0.4603 - accuracy: 0.8099\n",
      "Epoch 132/150\n",
      "747/747 - 0s - loss: 0.4717 - accuracy: 0.7965\n",
      "Epoch 133/150\n",
      "747/747 - 0s - loss: 0.4495 - accuracy: 0.8139\n",
      "Epoch 134/150\n",
      "747/747 - 0s - loss: 0.4628 - accuracy: 0.8086\n",
      "Epoch 135/150\n",
      "747/747 - 0s - loss: 0.4649 - accuracy: 0.8059\n",
      "Epoch 136/150\n",
      "747/747 - 0s - loss: 0.4622 - accuracy: 0.8046\n",
      "Epoch 137/150\n",
      "747/747 - 0s - loss: 0.4522 - accuracy: 0.8072\n",
      "Epoch 138/150\n",
      "747/747 - 0s - loss: 0.4627 - accuracy: 0.7979\n",
      "Epoch 139/150\n",
      "747/747 - 0s - loss: 0.4544 - accuracy: 0.8032\n",
      "Epoch 140/150\n",
      "747/747 - 0s - loss: 0.4616 - accuracy: 0.8032\n",
      "Epoch 141/150\n",
      "747/747 - 0s - loss: 0.4536 - accuracy: 0.8059\n",
      "Epoch 142/150\n",
      "747/747 - 0s - loss: 0.4601 - accuracy: 0.7925\n",
      "Epoch 143/150\n",
      "747/747 - 0s - loss: 0.4619 - accuracy: 0.8005\n",
      "Epoch 144/150\n",
      "747/747 - 0s - loss: 0.4586 - accuracy: 0.8072\n",
      "Epoch 145/150\n",
      "747/747 - 0s - loss: 0.4604 - accuracy: 0.8005\n",
      "Epoch 146/150\n",
      "747/747 - 0s - loss: 0.4645 - accuracy: 0.8059\n",
      "Epoch 147/150\n",
      "747/747 - 0s - loss: 0.4710 - accuracy: 0.7912\n",
      "Epoch 148/150\n",
      "747/747 - 0s - loss: 0.4622 - accuracy: 0.8112\n",
      "Epoch 149/150\n",
      "747/747 - 0s - loss: 0.4748 - accuracy: 0.7845\n",
      "Epoch 150/150\n",
      "747/747 - 0s - loss: 0.4523 - accuracy: 0.8046\n",
      "83/1 - 1s - loss: 0.3480 - accuracy: 0.8554\n",
      "Train on 747 samples\n",
      "Epoch 1/150\n",
      "747/747 - 1s - loss: 0.6980 - accuracy: 0.4900\n",
      "Epoch 2/150\n",
      "747/747 - 0s - loss: 0.6437 - accuracy: 0.6037\n",
      "Epoch 3/150\n",
      "747/747 - 0s - loss: 0.6007 - accuracy: 0.7028\n",
      "Epoch 4/150\n",
      "747/747 - 0s - loss: 0.5769 - accuracy: 0.7550\n",
      "Epoch 5/150\n",
      "747/747 - 0s - loss: 0.5492 - accuracy: 0.7738\n",
      "Epoch 6/150\n",
      "747/747 - 0s - loss: 0.5318 - accuracy: 0.7898\n",
      "Epoch 7/150\n",
      "747/747 - 0s - loss: 0.5107 - accuracy: 0.7871\n",
      "Epoch 8/150\n",
      "747/747 - 0s - loss: 0.5158 - accuracy: 0.7805\n",
      "Epoch 9/150\n",
      "747/747 - 0s - loss: 0.4823 - accuracy: 0.7845\n",
      "Epoch 10/150\n",
      "747/747 - 0s - loss: 0.4885 - accuracy: 0.7898\n",
      "Epoch 11/150\n",
      "747/747 - 0s - loss: 0.4813 - accuracy: 0.7952\n",
      "Epoch 12/150\n",
      "747/747 - 0s - loss: 0.4774 - accuracy: 0.7791\n",
      "Epoch 13/150\n",
      "747/747 - 0s - loss: 0.4800 - accuracy: 0.7898\n",
      "Epoch 14/150\n",
      "747/747 - 0s - loss: 0.4836 - accuracy: 0.7845\n",
      "Epoch 15/150\n",
      "747/747 - 0s - loss: 0.4900 - accuracy: 0.7885\n",
      "Epoch 16/150\n",
      "747/747 - 0s - loss: 0.4831 - accuracy: 0.7912\n",
      "Epoch 17/150\n",
      "747/747 - 0s - loss: 0.4728 - accuracy: 0.7979\n",
      "Epoch 18/150\n",
      "747/747 - 0s - loss: 0.4721 - accuracy: 0.7898\n",
      "Epoch 19/150\n",
      "747/747 - 0s - loss: 0.4869 - accuracy: 0.7952\n",
      "Epoch 20/150\n",
      "747/747 - 0s - loss: 0.4743 - accuracy: 0.7912\n",
      "Epoch 21/150\n",
      "747/747 - 0s - loss: 0.4818 - accuracy: 0.7965\n",
      "Epoch 22/150\n",
      "747/747 - 0s - loss: 0.4787 - accuracy: 0.7925\n",
      "Epoch 23/150\n",
      "747/747 - 0s - loss: 0.4697 - accuracy: 0.7858\n",
      "Epoch 24/150\n",
      "747/747 - 0s - loss: 0.4897 - accuracy: 0.7845\n",
      "Epoch 25/150\n",
      "747/747 - 0s - loss: 0.4729 - accuracy: 0.7965\n",
      "Epoch 26/150\n",
      "747/747 - 0s - loss: 0.4631 - accuracy: 0.8005\n",
      "Epoch 27/150\n",
      "747/747 - 0s - loss: 0.4643 - accuracy: 0.7979\n",
      "Epoch 28/150\n",
      "747/747 - 0s - loss: 0.4752 - accuracy: 0.7938\n",
      "Epoch 29/150\n",
      "747/747 - 0s - loss: 0.4733 - accuracy: 0.8005\n",
      "Epoch 30/150\n",
      "747/747 - 0s - loss: 0.4689 - accuracy: 0.7992\n",
      "Epoch 31/150\n",
      "747/747 - 0s - loss: 0.4665 - accuracy: 0.7952\n",
      "Epoch 32/150\n",
      "747/747 - 0s - loss: 0.4828 - accuracy: 0.7898\n",
      "Epoch 33/150\n",
      "747/747 - 0s - loss: 0.4768 - accuracy: 0.7925\n",
      "Epoch 34/150\n",
      "747/747 - 0s - loss: 0.4607 - accuracy: 0.7898\n",
      "Epoch 35/150\n",
      "747/747 - 0s - loss: 0.4649 - accuracy: 0.7885\n",
      "Epoch 36/150\n",
      "747/747 - 0s - loss: 0.4746 - accuracy: 0.7925\n",
      "Epoch 37/150\n",
      "747/747 - 0s - loss: 0.4620 - accuracy: 0.7952\n",
      "Epoch 38/150\n",
      "747/747 - 0s - loss: 0.4720 - accuracy: 0.7938\n",
      "Epoch 39/150\n",
      "747/747 - 0s - loss: 0.4702 - accuracy: 0.7992\n",
      "Epoch 40/150\n",
      "747/747 - 0s - loss: 0.4742 - accuracy: 0.7925\n",
      "Epoch 41/150\n",
      "747/747 - 0s - loss: 0.4797 - accuracy: 0.7952\n",
      "Epoch 42/150\n",
      "747/747 - 0s - loss: 0.4672 - accuracy: 0.8005\n",
      "Epoch 43/150\n",
      "747/747 - 0s - loss: 0.4554 - accuracy: 0.8019\n",
      "Epoch 44/150\n",
      "747/747 - 0s - loss: 0.4704 - accuracy: 0.8019\n",
      "Epoch 45/150\n",
      "747/747 - 0s - loss: 0.4555 - accuracy: 0.8032\n",
      "Epoch 46/150\n",
      "747/747 - 0s - loss: 0.4626 - accuracy: 0.8059\n",
      "Epoch 47/150\n",
      "747/747 - 0s - loss: 0.4582 - accuracy: 0.7992\n",
      "Epoch 48/150\n",
      "747/747 - 0s - loss: 0.4644 - accuracy: 0.8059\n",
      "Epoch 49/150\n",
      "747/747 - 0s - loss: 0.4611 - accuracy: 0.8019\n",
      "Epoch 50/150\n",
      "747/747 - 0s - loss: 0.4735 - accuracy: 0.8005\n",
      "Epoch 51/150\n",
      "747/747 - 0s - loss: 0.4667 - accuracy: 0.7898\n",
      "Epoch 52/150\n",
      "747/747 - 0s - loss: 0.4807 - accuracy: 0.7979\n",
      "Epoch 53/150\n",
      "747/747 - 0s - loss: 0.4645 - accuracy: 0.7925\n",
      "Epoch 54/150\n",
      "747/747 - 0s - loss: 0.4553 - accuracy: 0.8072\n",
      "Epoch 55/150\n",
      "747/747 - 0s - loss: 0.4597 - accuracy: 0.8032\n",
      "Epoch 56/150\n",
      "747/747 - 0s - loss: 0.4639 - accuracy: 0.8059\n",
      "Epoch 57/150\n",
      "747/747 - 0s - loss: 0.4715 - accuracy: 0.7871\n",
      "Epoch 58/150\n",
      "747/747 - 0s - loss: 0.4745 - accuracy: 0.8019\n",
      "Epoch 59/150\n",
      "747/747 - 0s - loss: 0.4571 - accuracy: 0.8005\n",
      "Epoch 60/150\n",
      "747/747 - 0s - loss: 0.4684 - accuracy: 0.7992\n",
      "Epoch 61/150\n",
      "747/747 - 0s - loss: 0.4635 - accuracy: 0.7979\n",
      "Epoch 62/150\n",
      "747/747 - 0s - loss: 0.4649 - accuracy: 0.7979\n",
      "Epoch 63/150\n",
      "747/747 - 0s - loss: 0.4634 - accuracy: 0.8059\n",
      "Epoch 64/150\n",
      "747/747 - 0s - loss: 0.4661 - accuracy: 0.8032\n",
      "Epoch 65/150\n",
      "747/747 - 0s - loss: 0.4572 - accuracy: 0.8019\n",
      "Epoch 66/150\n",
      "747/747 - 0s - loss: 0.4693 - accuracy: 0.8072\n",
      "Epoch 67/150\n",
      "747/747 - 0s - loss: 0.4521 - accuracy: 0.8086\n",
      "Epoch 68/150\n",
      "747/747 - 0s - loss: 0.4669 - accuracy: 0.7952\n",
      "Epoch 69/150\n",
      "747/747 - 0s - loss: 0.4586 - accuracy: 0.8019\n",
      "Epoch 70/150\n",
      "747/747 - 0s - loss: 0.4694 - accuracy: 0.7992\n",
      "Epoch 71/150\n",
      "747/747 - 0s - loss: 0.4584 - accuracy: 0.8086\n",
      "Epoch 72/150\n",
      "747/747 - 0s - loss: 0.4600 - accuracy: 0.8032\n",
      "Epoch 73/150\n",
      "747/747 - 0s - loss: 0.4594 - accuracy: 0.8046\n",
      "Epoch 74/150\n",
      "747/747 - 0s - loss: 0.4560 - accuracy: 0.8059\n",
      "Epoch 75/150\n",
      "747/747 - 0s - loss: 0.4664 - accuracy: 0.8046\n",
      "Epoch 76/150\n",
      "747/747 - 0s - loss: 0.4579 - accuracy: 0.8005\n",
      "Epoch 77/150\n",
      "747/747 - 0s - loss: 0.4547 - accuracy: 0.8059\n",
      "Epoch 78/150\n",
      "747/747 - 0s - loss: 0.4660 - accuracy: 0.8019\n",
      "Epoch 79/150\n",
      "747/747 - 0s - loss: 0.4476 - accuracy: 0.8139\n",
      "Epoch 80/150\n",
      "747/747 - 0s - loss: 0.4551 - accuracy: 0.8153\n",
      "Epoch 81/150\n",
      "747/747 - 0s - loss: 0.4574 - accuracy: 0.8005\n",
      "Epoch 82/150\n",
      "747/747 - 0s - loss: 0.4623 - accuracy: 0.8059\n",
      "Epoch 83/150\n",
      "747/747 - 0s - loss: 0.4602 - accuracy: 0.8059\n",
      "Epoch 84/150\n",
      "747/747 - 0s - loss: 0.4564 - accuracy: 0.8032\n",
      "Epoch 85/150\n",
      "747/747 - 0s - loss: 0.4600 - accuracy: 0.8059\n",
      "Epoch 86/150\n",
      "747/747 - 0s - loss: 0.4665 - accuracy: 0.8032\n",
      "Epoch 87/150\n",
      "747/747 - 0s - loss: 0.4577 - accuracy: 0.8059\n",
      "Epoch 88/150\n",
      "747/747 - 0s - loss: 0.4534 - accuracy: 0.8019\n",
      "Epoch 89/150\n",
      "747/747 - 0s - loss: 0.4631 - accuracy: 0.8046\n",
      "Epoch 90/150\n",
      "747/747 - 0s - loss: 0.4588 - accuracy: 0.8099\n",
      "Epoch 91/150\n",
      "747/747 - 0s - loss: 0.4520 - accuracy: 0.8059\n",
      "Epoch 92/150\n",
      "747/747 - 0s - loss: 0.4599 - accuracy: 0.8179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/150\n",
      "747/747 - 0s - loss: 0.4558 - accuracy: 0.7965\n",
      "Epoch 94/150\n",
      "747/747 - 0s - loss: 0.4460 - accuracy: 0.8032\n",
      "Epoch 95/150\n",
      "747/747 - 0s - loss: 0.4504 - accuracy: 0.8032\n",
      "Epoch 96/150\n",
      "747/747 - 0s - loss: 0.4524 - accuracy: 0.7938\n",
      "Epoch 97/150\n",
      "747/747 - 0s - loss: 0.4538 - accuracy: 0.8046\n",
      "Epoch 98/150\n",
      "747/747 - 0s - loss: 0.4588 - accuracy: 0.7952\n",
      "Epoch 99/150\n",
      "747/747 - 0s - loss: 0.4620 - accuracy: 0.8005\n",
      "Epoch 100/150\n",
      "747/747 - 0s - loss: 0.4548 - accuracy: 0.8005\n",
      "Epoch 101/150\n",
      "747/747 - 0s - loss: 0.4640 - accuracy: 0.8032\n",
      "Epoch 102/150\n",
      "747/747 - 0s - loss: 0.4671 - accuracy: 0.8032\n",
      "Epoch 103/150\n",
      "747/747 - 0s - loss: 0.4522 - accuracy: 0.7965\n",
      "Epoch 104/150\n",
      "747/747 - 0s - loss: 0.4540 - accuracy: 0.8086\n",
      "Epoch 105/150\n",
      "747/747 - 0s - loss: 0.4578 - accuracy: 0.8099\n",
      "Epoch 106/150\n",
      "747/747 - 0s - loss: 0.4590 - accuracy: 0.8112\n",
      "Epoch 107/150\n",
      "747/747 - 0s - loss: 0.4574 - accuracy: 0.8112\n",
      "Epoch 108/150\n",
      "747/747 - 0s - loss: 0.4602 - accuracy: 0.8046\n",
      "Epoch 109/150\n",
      "747/747 - 0s - loss: 0.4633 - accuracy: 0.8032\n",
      "Epoch 110/150\n",
      "747/747 - 0s - loss: 0.4592 - accuracy: 0.7979\n",
      "Epoch 111/150\n",
      "747/747 - 0s - loss: 0.4565 - accuracy: 0.8019\n",
      "Epoch 112/150\n",
      "747/747 - 0s - loss: 0.4557 - accuracy: 0.8059\n",
      "Epoch 113/150\n",
      "747/747 - 0s - loss: 0.4570 - accuracy: 0.8059\n",
      "Epoch 114/150\n",
      "747/747 - 0s - loss: 0.4599 - accuracy: 0.8032\n",
      "Epoch 115/150\n",
      "747/747 - 0s - loss: 0.4459 - accuracy: 0.8019\n",
      "Epoch 116/150\n",
      "747/747 - 0s - loss: 0.4507 - accuracy: 0.8032\n",
      "Epoch 117/150\n",
      "747/747 - 0s - loss: 0.4563 - accuracy: 0.8046\n",
      "Epoch 118/150\n",
      "747/747 - 0s - loss: 0.4440 - accuracy: 0.8005\n",
      "Epoch 119/150\n",
      "747/747 - 0s - loss: 0.4525 - accuracy: 0.8086\n",
      "Epoch 120/150\n",
      "747/747 - 0s - loss: 0.4511 - accuracy: 0.8139\n",
      "Epoch 121/150\n",
      "747/747 - 0s - loss: 0.4528 - accuracy: 0.7979\n",
      "Epoch 122/150\n",
      "747/747 - 0s - loss: 0.4452 - accuracy: 0.8153\n",
      "Epoch 123/150\n",
      "747/747 - 0s - loss: 0.4517 - accuracy: 0.8059\n",
      "Epoch 124/150\n",
      "747/747 - 0s - loss: 0.4535 - accuracy: 0.7979\n",
      "Epoch 125/150\n",
      "747/747 - 0s - loss: 0.4474 - accuracy: 0.8005\n",
      "Epoch 126/150\n",
      "747/747 - 0s - loss: 0.4494 - accuracy: 0.8099\n",
      "Epoch 127/150\n",
      "747/747 - 0s - loss: 0.4489 - accuracy: 0.8059\n",
      "Epoch 128/150\n",
      "747/747 - 0s - loss: 0.4510 - accuracy: 0.8059\n",
      "Epoch 129/150\n",
      "747/747 - 0s - loss: 0.4573 - accuracy: 0.8032\n",
      "Epoch 130/150\n",
      "747/747 - 0s - loss: 0.4505 - accuracy: 0.8005\n",
      "Epoch 131/150\n",
      "747/747 - 0s - loss: 0.4532 - accuracy: 0.8032\n",
      "Epoch 132/150\n",
      "747/747 - 0s - loss: 0.4589 - accuracy: 0.8046\n",
      "Epoch 133/150\n",
      "747/747 - 0s - loss: 0.4596 - accuracy: 0.7992\n",
      "Epoch 134/150\n",
      "747/747 - 0s - loss: 0.4526 - accuracy: 0.7979\n",
      "Epoch 135/150\n",
      "747/747 - 0s - loss: 0.4473 - accuracy: 0.8126\n",
      "Epoch 136/150\n",
      "747/747 - 0s - loss: 0.4695 - accuracy: 0.8019\n",
      "Epoch 137/150\n",
      "747/747 - 0s - loss: 0.4520 - accuracy: 0.8099\n",
      "Epoch 138/150\n",
      "747/747 - 0s - loss: 0.4462 - accuracy: 0.8112\n",
      "Epoch 139/150\n",
      "747/747 - 0s - loss: 0.4614 - accuracy: 0.8153\n",
      "Epoch 140/150\n",
      "747/747 - 0s - loss: 0.4528 - accuracy: 0.8019\n",
      "Epoch 141/150\n",
      "747/747 - 0s - loss: 0.4553 - accuracy: 0.7979\n",
      "Epoch 142/150\n",
      "747/747 - 0s - loss: 0.4619 - accuracy: 0.8072\n",
      "Epoch 143/150\n",
      "747/747 - 0s - loss: 0.4448 - accuracy: 0.8112\n",
      "Epoch 144/150\n",
      "747/747 - 0s - loss: 0.4518 - accuracy: 0.8086\n",
      "Epoch 145/150\n",
      "747/747 - 0s - loss: 0.4581 - accuracy: 0.8086\n",
      "Epoch 146/150\n",
      "747/747 - 0s - loss: 0.4525 - accuracy: 0.8046\n",
      "Epoch 147/150\n",
      "747/747 - 0s - loss: 0.4467 - accuracy: 0.8099\n",
      "Epoch 148/150\n",
      "747/747 - 0s - loss: 0.4490 - accuracy: 0.8153\n",
      "Epoch 149/150\n",
      "747/747 - 0s - loss: 0.4529 - accuracy: 0.8005\n",
      "Epoch 150/150\n",
      "747/747 - 0s - loss: 0.4507 - accuracy: 0.8072\n",
      "83/1 - 0s - loss: 0.3762 - accuracy: 0.8434\n",
      "Train on 747 samples\n",
      "Epoch 1/150\n",
      "747/747 - 1s - loss: 0.6652 - accuracy: 0.5810\n",
      "Epoch 2/150\n",
      "747/747 - 0s - loss: 0.5925 - accuracy: 0.7470\n",
      "Epoch 3/150\n",
      "747/747 - 0s - loss: 0.5652 - accuracy: 0.7510\n",
      "Epoch 4/150\n",
      "747/747 - 0s - loss: 0.5260 - accuracy: 0.7791\n",
      "Epoch 5/150\n",
      "747/747 - 0s - loss: 0.5117 - accuracy: 0.7738\n",
      "Epoch 6/150\n",
      "747/747 - 0s - loss: 0.5196 - accuracy: 0.7657\n",
      "Epoch 7/150\n",
      "747/747 - 0s - loss: 0.4857 - accuracy: 0.7818\n",
      "Epoch 8/150\n",
      "747/747 - 0s - loss: 0.5108 - accuracy: 0.7724\n",
      "Epoch 9/150\n",
      "747/747 - 0s - loss: 0.4931 - accuracy: 0.7845\n",
      "Epoch 10/150\n",
      "747/747 - 0s - loss: 0.5047 - accuracy: 0.7805\n",
      "Epoch 11/150\n",
      "747/747 - 0s - loss: 0.5008 - accuracy: 0.7711\n",
      "Epoch 12/150\n",
      "747/747 - 0s - loss: 0.5030 - accuracy: 0.7885\n",
      "Epoch 13/150\n",
      "747/747 - 0s - loss: 0.4998 - accuracy: 0.7764\n",
      "Epoch 14/150\n",
      "747/747 - 0s - loss: 0.4873 - accuracy: 0.7912\n",
      "Epoch 15/150\n",
      "747/747 - 0s - loss: 0.4824 - accuracy: 0.7831\n",
      "Epoch 16/150\n",
      "747/747 - 0s - loss: 0.4861 - accuracy: 0.7898\n",
      "Epoch 17/150\n",
      "747/747 - 0s - loss: 0.4862 - accuracy: 0.7898\n",
      "Epoch 18/150\n",
      "747/747 - 0s - loss: 0.4916 - accuracy: 0.7858\n",
      "Epoch 19/150\n",
      "747/747 - 0s - loss: 0.4884 - accuracy: 0.8019\n",
      "Epoch 20/150\n",
      "747/747 - 0s - loss: 0.4836 - accuracy: 0.7711\n",
      "Epoch 21/150\n",
      "747/747 - 0s - loss: 0.4938 - accuracy: 0.7805\n",
      "Epoch 22/150\n",
      "747/747 - 0s - loss: 0.4969 - accuracy: 0.7898\n",
      "Epoch 23/150\n",
      "747/747 - 0s - loss: 0.4766 - accuracy: 0.7845\n",
      "Epoch 24/150\n",
      "747/747 - 0s - loss: 0.4930 - accuracy: 0.7858\n",
      "Epoch 25/150\n",
      "747/747 - 0s - loss: 0.4825 - accuracy: 0.7938\n",
      "Epoch 26/150\n",
      "747/747 - 0s - loss: 0.4814 - accuracy: 0.7885\n",
      "Epoch 27/150\n",
      "747/747 - 0s - loss: 0.4790 - accuracy: 0.7858\n",
      "Epoch 28/150\n",
      "747/747 - 0s - loss: 0.4815 - accuracy: 0.7845\n",
      "Epoch 29/150\n",
      "747/747 - 0s - loss: 0.4973 - accuracy: 0.7858\n",
      "Epoch 30/150\n",
      "747/747 - 0s - loss: 0.4930 - accuracy: 0.7845\n",
      "Epoch 31/150\n",
      "747/747 - 0s - loss: 0.4857 - accuracy: 0.7858\n",
      "Epoch 32/150\n",
      "747/747 - 0s - loss: 0.4847 - accuracy: 0.7912\n",
      "Epoch 33/150\n",
      "747/747 - 0s - loss: 0.4722 - accuracy: 0.7952\n",
      "Epoch 34/150\n",
      "747/747 - 0s - loss: 0.4810 - accuracy: 0.7965\n",
      "Epoch 35/150\n",
      "747/747 - 0s - loss: 0.4653 - accuracy: 0.7952\n",
      "Epoch 36/150\n",
      "747/747 - 0s - loss: 0.4763 - accuracy: 0.7805\n",
      "Epoch 37/150\n",
      "747/747 - 0s - loss: 0.4716 - accuracy: 0.7818\n",
      "Epoch 38/150\n",
      "747/747 - 0s - loss: 0.4836 - accuracy: 0.7898\n",
      "Epoch 39/150\n",
      "747/747 - 0s - loss: 0.4803 - accuracy: 0.7912\n",
      "Epoch 40/150\n",
      "747/747 - 0s - loss: 0.4807 - accuracy: 0.7912\n",
      "Epoch 41/150\n",
      "747/747 - 0s - loss: 0.4741 - accuracy: 0.7992\n",
      "Epoch 42/150\n",
      "747/747 - 0s - loss: 0.4802 - accuracy: 0.7871\n",
      "Epoch 43/150\n",
      "747/747 - 0s - loss: 0.4719 - accuracy: 0.7992\n",
      "Epoch 44/150\n",
      "747/747 - 0s - loss: 0.4711 - accuracy: 0.7925\n",
      "Epoch 45/150\n",
      "747/747 - 0s - loss: 0.4767 - accuracy: 0.7697\n",
      "Epoch 46/150\n",
      "747/747 - 0s - loss: 0.4684 - accuracy: 0.7831\n",
      "Epoch 47/150\n",
      "747/747 - 0s - loss: 0.4799 - accuracy: 0.7858\n",
      "Epoch 48/150\n",
      "747/747 - 0s - loss: 0.4721 - accuracy: 0.7912\n",
      "Epoch 49/150\n",
      "747/747 - 0s - loss: 0.4795 - accuracy: 0.7912\n",
      "Epoch 50/150\n",
      "747/747 - 0s - loss: 0.4810 - accuracy: 0.7858\n",
      "Epoch 51/150\n",
      "747/747 - 0s - loss: 0.4804 - accuracy: 0.7938\n",
      "Epoch 52/150\n",
      "747/747 - 0s - loss: 0.4657 - accuracy: 0.7912\n",
      "Epoch 53/150\n",
      "747/747 - 0s - loss: 0.4739 - accuracy: 0.8032\n",
      "Epoch 54/150\n",
      "747/747 - 0s - loss: 0.4717 - accuracy: 0.8005\n",
      "Epoch 55/150\n",
      "747/747 - 0s - loss: 0.4706 - accuracy: 0.7912\n",
      "Epoch 56/150\n",
      "747/747 - 0s - loss: 0.4720 - accuracy: 0.7871\n",
      "Epoch 57/150\n",
      "747/747 - 0s - loss: 0.4723 - accuracy: 0.7885\n",
      "Epoch 58/150\n",
      "747/747 - 0s - loss: 0.4720 - accuracy: 0.7965\n",
      "Epoch 59/150\n",
      "747/747 - 0s - loss: 0.4764 - accuracy: 0.7992\n",
      "Epoch 60/150\n",
      "747/747 - 0s - loss: 0.4714 - accuracy: 0.7925\n",
      "Epoch 61/150\n",
      "747/747 - 0s - loss: 0.4578 - accuracy: 0.8086\n",
      "Epoch 62/150\n",
      "747/747 - 0s - loss: 0.4683 - accuracy: 0.7979\n",
      "Epoch 63/150\n",
      "747/747 - 0s - loss: 0.4644 - accuracy: 0.8072\n",
      "Epoch 64/150\n",
      "747/747 - 0s - loss: 0.4697 - accuracy: 0.8032\n",
      "Epoch 65/150\n",
      "747/747 - 0s - loss: 0.4790 - accuracy: 0.7885\n",
      "Epoch 66/150\n",
      "747/747 - 0s - loss: 0.4757 - accuracy: 0.7925\n",
      "Epoch 67/150\n",
      "747/747 - 0s - loss: 0.4722 - accuracy: 0.7992\n",
      "Epoch 68/150\n",
      "747/747 - 0s - loss: 0.4719 - accuracy: 0.7898\n",
      "Epoch 69/150\n",
      "747/747 - 0s - loss: 0.4749 - accuracy: 0.8046\n",
      "Epoch 70/150\n",
      "747/747 - 0s - loss: 0.4655 - accuracy: 0.8005\n",
      "Epoch 71/150\n",
      "747/747 - 0s - loss: 0.4687 - accuracy: 0.8086\n",
      "Epoch 72/150\n",
      "747/747 - 0s - loss: 0.4644 - accuracy: 0.8032\n",
      "Epoch 73/150\n",
      "747/747 - 0s - loss: 0.4834 - accuracy: 0.7898\n",
      "Epoch 74/150\n",
      "747/747 - 0s - loss: 0.4689 - accuracy: 0.8072\n",
      "Epoch 75/150\n",
      "747/747 - 0s - loss: 0.4722 - accuracy: 0.8032\n",
      "Epoch 76/150\n",
      "747/747 - 0s - loss: 0.4669 - accuracy: 0.7938\n",
      "Epoch 77/150\n",
      "747/747 - 0s - loss: 0.4665 - accuracy: 0.7898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/150\n",
      "747/747 - 0s - loss: 0.4627 - accuracy: 0.8032\n",
      "Epoch 79/150\n",
      "747/747 - 0s - loss: 0.4756 - accuracy: 0.7871\n",
      "Epoch 80/150\n",
      "747/747 - 0s - loss: 0.4686 - accuracy: 0.7965\n",
      "Epoch 81/150\n",
      "747/747 - 0s - loss: 0.4554 - accuracy: 0.8166\n",
      "Epoch 82/150\n",
      "747/747 - 0s - loss: 0.4638 - accuracy: 0.7938\n",
      "Epoch 83/150\n",
      "747/747 - 0s - loss: 0.4560 - accuracy: 0.8005\n",
      "Epoch 84/150\n",
      "747/747 - 0s - loss: 0.4634 - accuracy: 0.8072\n",
      "Epoch 85/150\n",
      "747/747 - 0s - loss: 0.4647 - accuracy: 0.8032\n",
      "Epoch 86/150\n",
      "747/747 - 0s - loss: 0.4598 - accuracy: 0.7979\n",
      "Epoch 87/150\n",
      "747/747 - 0s - loss: 0.4729 - accuracy: 0.7898\n",
      "Epoch 88/150\n",
      "747/747 - 0s - loss: 0.4615 - accuracy: 0.7925\n",
      "Epoch 89/150\n",
      "747/747 - 0s - loss: 0.4620 - accuracy: 0.7952\n",
      "Epoch 90/150\n",
      "747/747 - 0s - loss: 0.4722 - accuracy: 0.7965\n",
      "Epoch 91/150\n",
      "747/747 - 0s - loss: 0.4556 - accuracy: 0.8086\n",
      "Epoch 92/150\n",
      "747/747 - 0s - loss: 0.4630 - accuracy: 0.8032\n",
      "Epoch 93/150\n",
      "747/747 - 0s - loss: 0.4680 - accuracy: 0.7965\n",
      "Epoch 94/150\n",
      "747/747 - 0s - loss: 0.4646 - accuracy: 0.8072\n",
      "Epoch 95/150\n",
      "747/747 - 0s - loss: 0.4738 - accuracy: 0.8005\n",
      "Epoch 96/150\n",
      "747/747 - 0s - loss: 0.4690 - accuracy: 0.7979\n",
      "Epoch 97/150\n",
      "747/747 - 0s - loss: 0.4624 - accuracy: 0.8005\n",
      "Epoch 98/150\n",
      "747/747 - 0s - loss: 0.4690 - accuracy: 0.8099\n",
      "Epoch 99/150\n",
      "747/747 - 0s - loss: 0.4590 - accuracy: 0.8072\n",
      "Epoch 100/150\n",
      "747/747 - 0s - loss: 0.4601 - accuracy: 0.8046\n",
      "Epoch 101/150\n",
      "747/747 - 0s - loss: 0.4661 - accuracy: 0.7979\n",
      "Epoch 102/150\n",
      "747/747 - 0s - loss: 0.4646 - accuracy: 0.7952\n",
      "Epoch 103/150\n",
      "747/747 - 0s - loss: 0.4557 - accuracy: 0.8220\n",
      "Epoch 104/150\n",
      "747/747 - 0s - loss: 0.4636 - accuracy: 0.8059\n",
      "Epoch 105/150\n",
      "747/747 - 0s - loss: 0.4663 - accuracy: 0.8019\n",
      "Epoch 106/150\n",
      "747/747 - 0s - loss: 0.4667 - accuracy: 0.7979\n",
      "Epoch 107/150\n",
      "747/747 - 0s - loss: 0.4715 - accuracy: 0.7898\n",
      "Epoch 108/150\n",
      "747/747 - 0s - loss: 0.4583 - accuracy: 0.8086\n",
      "Epoch 109/150\n",
      "747/747 - 0s - loss: 0.4620 - accuracy: 0.8086\n",
      "Epoch 110/150\n",
      "747/747 - 0s - loss: 0.4557 - accuracy: 0.8059\n",
      "Epoch 111/150\n",
      "747/747 - 0s - loss: 0.4580 - accuracy: 0.8032\n",
      "Epoch 112/150\n",
      "747/747 - 0s - loss: 0.4547 - accuracy: 0.8059\n",
      "Epoch 113/150\n",
      "747/747 - 0s - loss: 0.4572 - accuracy: 0.8059\n",
      "Epoch 114/150\n",
      "747/747 - 0s - loss: 0.4611 - accuracy: 0.7885\n",
      "Epoch 115/150\n",
      "747/747 - 0s - loss: 0.4536 - accuracy: 0.8139\n",
      "Epoch 116/150\n",
      "747/747 - 0s - loss: 0.4548 - accuracy: 0.8019\n",
      "Epoch 117/150\n",
      "747/747 - 0s - loss: 0.4721 - accuracy: 0.7952\n",
      "Epoch 118/150\n",
      "747/747 - 0s - loss: 0.4574 - accuracy: 0.7992\n",
      "Epoch 119/150\n",
      "747/747 - 0s - loss: 0.4646 - accuracy: 0.8072\n",
      "Epoch 120/150\n",
      "747/747 - 0s - loss: 0.4701 - accuracy: 0.7979\n",
      "Epoch 121/150\n",
      "747/747 - 0s - loss: 0.4648 - accuracy: 0.8046\n",
      "Epoch 122/150\n",
      "747/747 - 0s - loss: 0.4583 - accuracy: 0.8086\n",
      "Epoch 123/150\n",
      "747/747 - 0s - loss: 0.4616 - accuracy: 0.7979\n",
      "Epoch 124/150\n",
      "747/747 - 0s - loss: 0.4480 - accuracy: 0.8112\n",
      "Epoch 125/150\n",
      "747/747 - 0s - loss: 0.4524 - accuracy: 0.8086\n",
      "Epoch 126/150\n",
      "747/747 - 0s - loss: 0.4622 - accuracy: 0.7858\n",
      "Epoch 127/150\n",
      "747/747 - 0s - loss: 0.4696 - accuracy: 0.8032\n",
      "Epoch 128/150\n",
      "747/747 - 0s - loss: 0.4647 - accuracy: 0.8059\n",
      "Epoch 129/150\n",
      "747/747 - 0s - loss: 0.4593 - accuracy: 0.8086\n",
      "Epoch 130/150\n",
      "747/747 - 0s - loss: 0.4601 - accuracy: 0.8072\n",
      "Epoch 131/150\n",
      "747/747 - 0s - loss: 0.4579 - accuracy: 0.8005\n",
      "Epoch 132/150\n",
      "747/747 - 0s - loss: 0.4657 - accuracy: 0.8032\n",
      "Epoch 133/150\n",
      "747/747 - 0s - loss: 0.4544 - accuracy: 0.8153\n",
      "Epoch 134/150\n",
      "747/747 - 0s - loss: 0.4618 - accuracy: 0.8099\n",
      "Epoch 135/150\n",
      "747/747 - 0s - loss: 0.4737 - accuracy: 0.7965\n",
      "Epoch 136/150\n",
      "747/747 - 0s - loss: 0.4609 - accuracy: 0.8059\n",
      "Epoch 137/150\n",
      "747/747 - 0s - loss: 0.4582 - accuracy: 0.8072\n",
      "Epoch 138/150\n",
      "747/747 - 0s - loss: 0.4542 - accuracy: 0.8046\n",
      "Epoch 139/150\n",
      "747/747 - 0s - loss: 0.4586 - accuracy: 0.8086\n",
      "Epoch 140/150\n",
      "747/747 - 0s - loss: 0.4508 - accuracy: 0.8179\n",
      "Epoch 141/150\n",
      "747/747 - 0s - loss: 0.4551 - accuracy: 0.8099\n",
      "Epoch 142/150\n",
      "747/747 - 0s - loss: 0.4703 - accuracy: 0.7912\n",
      "Epoch 143/150\n",
      "747/747 - 0s - loss: 0.4652 - accuracy: 0.8032\n",
      "Epoch 144/150\n",
      "747/747 - 0s - loss: 0.4585 - accuracy: 0.8059\n",
      "Epoch 145/150\n",
      "747/747 - 0s - loss: 0.4510 - accuracy: 0.8112\n",
      "Epoch 146/150\n",
      "747/747 - 0s - loss: 0.4561 - accuracy: 0.8112\n",
      "Epoch 147/150\n",
      "747/747 - 0s - loss: 0.4530 - accuracy: 0.8086\n",
      "Epoch 148/150\n",
      "747/747 - 0s - loss: 0.4604 - accuracy: 0.7979\n",
      "Epoch 149/150\n",
      "747/747 - 0s - loss: 0.4446 - accuracy: 0.8086\n",
      "Epoch 150/150\n",
      "747/747 - 0s - loss: 0.4565 - accuracy: 0.8126\n",
      "83/1 - 0s - loss: 0.2566 - accuracy: 0.8675\n",
      "Train on 747 samples\n",
      "Epoch 1/150\n",
      "747/747 - 2s - loss: 0.7404 - accuracy: 0.4859\n",
      "Epoch 2/150\n",
      "747/747 - 0s - loss: 0.6542 - accuracy: 0.5475\n",
      "Epoch 3/150\n",
      "747/747 - 0s - loss: 0.5943 - accuracy: 0.6747\n",
      "Epoch 4/150\n",
      "747/747 - 0s - loss: 0.5657 - accuracy: 0.7323\n",
      "Epoch 5/150\n",
      "747/747 - 0s - loss: 0.5281 - accuracy: 0.7631\n",
      "Epoch 6/150\n",
      "747/747 - 0s - loss: 0.4977 - accuracy: 0.7764\n",
      "Epoch 7/150\n",
      "747/747 - 0s - loss: 0.4922 - accuracy: 0.7805\n",
      "Epoch 8/150\n",
      "747/747 - 0s - loss: 0.5045 - accuracy: 0.8019\n",
      "Epoch 9/150\n",
      "747/747 - 0s - loss: 0.4944 - accuracy: 0.7979\n",
      "Epoch 10/150\n",
      "747/747 - 0s - loss: 0.4809 - accuracy: 0.8019\n",
      "Epoch 11/150\n",
      "747/747 - 0s - loss: 0.4688 - accuracy: 0.8032\n",
      "Epoch 12/150\n",
      "747/747 - 0s - loss: 0.4672 - accuracy: 0.8072\n",
      "Epoch 13/150\n",
      "747/747 - 0s - loss: 0.4788 - accuracy: 0.8059\n",
      "Epoch 14/150\n",
      "747/747 - 0s - loss: 0.4750 - accuracy: 0.8099\n",
      "Epoch 15/150\n",
      "747/747 - 0s - loss: 0.4777 - accuracy: 0.8086\n",
      "Epoch 16/150\n",
      "747/747 - 0s - loss: 0.4759 - accuracy: 0.8099\n",
      "Epoch 17/150\n",
      "747/747 - 0s - loss: 0.4736 - accuracy: 0.8032\n",
      "Epoch 18/150\n",
      "747/747 - 0s - loss: 0.4771 - accuracy: 0.8019\n",
      "Epoch 19/150\n",
      "747/747 - 0s - loss: 0.4823 - accuracy: 0.7979\n",
      "Epoch 20/150\n",
      "747/747 - 0s - loss: 0.4620 - accuracy: 0.8086\n",
      "Epoch 21/150\n",
      "747/747 - 0s - loss: 0.4706 - accuracy: 0.8005\n",
      "Epoch 22/150\n",
      "747/747 - 0s - loss: 0.4676 - accuracy: 0.8072\n",
      "Epoch 23/150\n",
      "747/747 - 0s - loss: 0.4540 - accuracy: 0.8112\n",
      "Epoch 24/150\n",
      "747/747 - 0s - loss: 0.4673 - accuracy: 0.8059\n",
      "Epoch 25/150\n",
      "747/747 - 0s - loss: 0.4645 - accuracy: 0.8059\n",
      "Epoch 26/150\n",
      "747/747 - 0s - loss: 0.4737 - accuracy: 0.8086\n",
      "Epoch 27/150\n",
      "747/747 - 0s - loss: 0.4680 - accuracy: 0.8059\n",
      "Epoch 28/150\n",
      "747/747 - 0s - loss: 0.4623 - accuracy: 0.8032\n",
      "Epoch 29/150\n",
      "747/747 - 0s - loss: 0.4646 - accuracy: 0.7992\n",
      "Epoch 30/150\n",
      "747/747 - 0s - loss: 0.4686 - accuracy: 0.7925\n",
      "Epoch 31/150\n",
      "747/747 - 0s - loss: 0.4645 - accuracy: 0.8166\n",
      "Epoch 32/150\n",
      "747/747 - 0s - loss: 0.4620 - accuracy: 0.8086\n",
      "Epoch 33/150\n",
      "747/747 - 0s - loss: 0.4697 - accuracy: 0.8086\n",
      "Epoch 34/150\n",
      "747/747 - 0s - loss: 0.4564 - accuracy: 0.8005\n",
      "Epoch 35/150\n",
      "747/747 - 0s - loss: 0.4589 - accuracy: 0.8059\n",
      "Epoch 36/150\n",
      "747/747 - 0s - loss: 0.4635 - accuracy: 0.8032\n",
      "Epoch 37/150\n",
      "747/747 - 0s - loss: 0.4567 - accuracy: 0.8059\n",
      "Epoch 38/150\n",
      "747/747 - 0s - loss: 0.4463 - accuracy: 0.8086\n",
      "Epoch 39/150\n",
      "747/747 - 0s - loss: 0.4641 - accuracy: 0.8112\n",
      "Epoch 40/150\n",
      "747/747 - 0s - loss: 0.4488 - accuracy: 0.8032\n",
      "Epoch 41/150\n",
      "747/747 - 0s - loss: 0.4531 - accuracy: 0.8086\n",
      "Epoch 42/150\n",
      "747/747 - 0s - loss: 0.4608 - accuracy: 0.8099\n",
      "Epoch 43/150\n",
      "747/747 - 0s - loss: 0.4591 - accuracy: 0.8072\n",
      "Epoch 44/150\n",
      "747/747 - 0s - loss: 0.4600 - accuracy: 0.8112\n",
      "Epoch 45/150\n",
      "747/747 - 0s - loss: 0.4620 - accuracy: 0.8153\n",
      "Epoch 46/150\n",
      "747/747 - 0s - loss: 0.4668 - accuracy: 0.8126\n",
      "Epoch 47/150\n",
      "747/747 - 0s - loss: 0.4647 - accuracy: 0.8059\n",
      "Epoch 48/150\n",
      "747/747 - 0s - loss: 0.4540 - accuracy: 0.8032\n",
      "Epoch 49/150\n",
      "747/747 - 0s - loss: 0.4601 - accuracy: 0.8046\n",
      "Epoch 50/150\n",
      "747/747 - 0s - loss: 0.4473 - accuracy: 0.8166\n",
      "Epoch 51/150\n",
      "747/747 - 0s - loss: 0.4521 - accuracy: 0.8166\n",
      "Epoch 52/150\n",
      "747/747 - 0s - loss: 0.4594 - accuracy: 0.8059\n",
      "Epoch 53/150\n",
      "747/747 - 0s - loss: 0.4534 - accuracy: 0.8099\n",
      "Epoch 54/150\n",
      "747/747 - 0s - loss: 0.4371 - accuracy: 0.8126\n",
      "Epoch 55/150\n",
      "747/747 - 0s - loss: 0.4625 - accuracy: 0.8032\n",
      "Epoch 56/150\n",
      "747/747 - 0s - loss: 0.4475 - accuracy: 0.8099\n",
      "Epoch 57/150\n",
      "747/747 - 0s - loss: 0.4659 - accuracy: 0.8099\n",
      "Epoch 58/150\n",
      "747/747 - 0s - loss: 0.4506 - accuracy: 0.8072\n",
      "Epoch 59/150\n",
      "747/747 - 0s - loss: 0.4586 - accuracy: 0.7979\n",
      "Epoch 60/150\n",
      "747/747 - 0s - loss: 0.4502 - accuracy: 0.8179\n",
      "Epoch 61/150\n",
      "747/747 - 0s - loss: 0.4480 - accuracy: 0.8112\n",
      "Epoch 62/150\n",
      "747/747 - 0s - loss: 0.4537 - accuracy: 0.8193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/150\n",
      "747/747 - 0s - loss: 0.4438 - accuracy: 0.8166\n",
      "Epoch 64/150\n",
      "747/747 - 0s - loss: 0.4566 - accuracy: 0.8139\n",
      "Epoch 65/150\n",
      "747/747 - 0s - loss: 0.4579 - accuracy: 0.8099\n",
      "Epoch 66/150\n",
      "747/747 - 0s - loss: 0.4456 - accuracy: 0.8072\n",
      "Epoch 67/150\n",
      "747/747 - 0s - loss: 0.4528 - accuracy: 0.8086\n",
      "Epoch 68/150\n",
      "747/747 - 0s - loss: 0.4453 - accuracy: 0.8072\n",
      "Epoch 69/150\n",
      "747/747 - 0s - loss: 0.4462 - accuracy: 0.8233\n",
      "Epoch 70/150\n",
      "747/747 - 0s - loss: 0.4487 - accuracy: 0.8166\n",
      "Epoch 71/150\n",
      "747/747 - 0s - loss: 0.4480 - accuracy: 0.8112\n",
      "Epoch 72/150\n",
      "747/747 - 0s - loss: 0.4419 - accuracy: 0.8126\n",
      "Epoch 73/150\n",
      "747/747 - 0s - loss: 0.4487 - accuracy: 0.8059\n",
      "Epoch 74/150\n",
      "747/747 - 0s - loss: 0.4442 - accuracy: 0.8166\n",
      "Epoch 75/150\n",
      "747/747 - 0s - loss: 0.4473 - accuracy: 0.8112\n",
      "Epoch 76/150\n",
      "747/747 - 0s - loss: 0.4525 - accuracy: 0.8086\n",
      "Epoch 77/150\n",
      "747/747 - 0s - loss: 0.4503 - accuracy: 0.8126\n",
      "Epoch 78/150\n",
      "747/747 - 0s - loss: 0.4578 - accuracy: 0.8032\n",
      "Epoch 79/150\n",
      "747/747 - 0s - loss: 0.4433 - accuracy: 0.8179\n",
      "Epoch 80/150\n",
      "747/747 - 0s - loss: 0.4486 - accuracy: 0.8139\n",
      "Epoch 81/150\n",
      "747/747 - 0s - loss: 0.4548 - accuracy: 0.8059\n",
      "Epoch 82/150\n",
      "747/747 - 0s - loss: 0.4488 - accuracy: 0.8099\n",
      "Epoch 83/150\n",
      "747/747 - 0s - loss: 0.4442 - accuracy: 0.8220\n",
      "Epoch 84/150\n",
      "747/747 - 0s - loss: 0.4390 - accuracy: 0.8193\n",
      "Epoch 85/150\n",
      "747/747 - 0s - loss: 0.4420 - accuracy: 0.8193\n",
      "Epoch 86/150\n",
      "747/747 - 0s - loss: 0.4615 - accuracy: 0.8099\n",
      "Epoch 87/150\n",
      "747/747 - 0s - loss: 0.4375 - accuracy: 0.8193\n",
      "Epoch 88/150\n",
      "747/747 - 0s - loss: 0.4523 - accuracy: 0.8139\n",
      "Epoch 89/150\n",
      "747/747 - 0s - loss: 0.4527 - accuracy: 0.8112\n",
      "Epoch 90/150\n",
      "747/747 - 0s - loss: 0.4477 - accuracy: 0.8099\n",
      "Epoch 91/150\n",
      "747/747 - 0s - loss: 0.4470 - accuracy: 0.8139\n",
      "Epoch 92/150\n",
      "747/747 - 0s - loss: 0.4493 - accuracy: 0.8099\n",
      "Epoch 93/150\n",
      "747/747 - 0s - loss: 0.4393 - accuracy: 0.8153\n",
      "Epoch 94/150\n",
      "747/747 - 0s - loss: 0.4468 - accuracy: 0.8139\n",
      "Epoch 95/150\n",
      "747/747 - 0s - loss: 0.4475 - accuracy: 0.8220\n",
      "Epoch 96/150\n",
      "747/747 - 0s - loss: 0.4475 - accuracy: 0.8099\n",
      "Epoch 97/150\n",
      "747/747 - 0s - loss: 0.4422 - accuracy: 0.8139\n",
      "Epoch 98/150\n",
      "747/747 - 0s - loss: 0.4496 - accuracy: 0.8153\n",
      "Epoch 99/150\n",
      "747/747 - 0s - loss: 0.4445 - accuracy: 0.8193\n",
      "Epoch 100/150\n",
      "747/747 - 0s - loss: 0.4553 - accuracy: 0.8099\n",
      "Epoch 101/150\n",
      "747/747 - 0s - loss: 0.4468 - accuracy: 0.8206\n",
      "Epoch 102/150\n",
      "747/747 - 0s - loss: 0.4460 - accuracy: 0.8179\n",
      "Epoch 103/150\n",
      "747/747 - 0s - loss: 0.4327 - accuracy: 0.8099\n",
      "Epoch 104/150\n",
      "747/747 - 0s - loss: 0.4459 - accuracy: 0.8193\n",
      "Epoch 105/150\n",
      "747/747 - 0s - loss: 0.4535 - accuracy: 0.8112\n",
      "Epoch 106/150\n",
      "747/747 - 0s - loss: 0.4407 - accuracy: 0.8166\n",
      "Epoch 107/150\n",
      "747/747 - 0s - loss: 0.4428 - accuracy: 0.8179\n",
      "Epoch 108/150\n",
      "747/747 - 0s - loss: 0.4412 - accuracy: 0.8153\n",
      "Epoch 109/150\n",
      "747/747 - 0s - loss: 0.4355 - accuracy: 0.8086\n",
      "Epoch 110/150\n",
      "747/747 - 0s - loss: 0.4364 - accuracy: 0.8139\n",
      "Epoch 111/150\n",
      "747/747 - 0s - loss: 0.4501 - accuracy: 0.8166\n",
      "Epoch 112/150\n",
      "747/747 - 0s - loss: 0.4342 - accuracy: 0.8206\n",
      "Epoch 113/150\n",
      "747/747 - 0s - loss: 0.4419 - accuracy: 0.8139\n",
      "Epoch 114/150\n",
      "747/747 - 0s - loss: 0.4463 - accuracy: 0.8099\n",
      "Epoch 115/150\n",
      "747/747 - 0s - loss: 0.4449 - accuracy: 0.8112\n",
      "Epoch 116/150\n",
      "747/747 - 0s - loss: 0.4347 - accuracy: 0.8139\n",
      "Epoch 117/150\n",
      "747/747 - 0s - loss: 0.4521 - accuracy: 0.8046\n",
      "Epoch 118/150\n",
      "747/747 - 0s - loss: 0.4452 - accuracy: 0.8126\n",
      "Epoch 119/150\n",
      "747/747 - 0s - loss: 0.4421 - accuracy: 0.8126\n",
      "Epoch 120/150\n",
      "747/747 - 0s - loss: 0.4442 - accuracy: 0.8046\n",
      "Epoch 121/150\n",
      "747/747 - 0s - loss: 0.4439 - accuracy: 0.8126\n",
      "Epoch 122/150\n",
      "747/747 - 0s - loss: 0.4442 - accuracy: 0.8166\n",
      "Epoch 123/150\n",
      "747/747 - 0s - loss: 0.4449 - accuracy: 0.8179\n",
      "Epoch 124/150\n",
      "747/747 - 0s - loss: 0.4410 - accuracy: 0.8126\n",
      "Epoch 125/150\n",
      "747/747 - 0s - loss: 0.4504 - accuracy: 0.8126\n",
      "Epoch 126/150\n",
      "747/747 - 0s - loss: 0.4370 - accuracy: 0.8139\n",
      "Epoch 127/150\n",
      "747/747 - 0s - loss: 0.4421 - accuracy: 0.8206\n",
      "Epoch 128/150\n",
      "747/747 - 0s - loss: 0.4421 - accuracy: 0.8059\n",
      "Epoch 129/150\n",
      "747/747 - 0s - loss: 0.4439 - accuracy: 0.8139\n",
      "Epoch 130/150\n",
      "747/747 - 0s - loss: 0.4410 - accuracy: 0.8153\n",
      "Epoch 131/150\n",
      "747/747 - 0s - loss: 0.4419 - accuracy: 0.8233\n",
      "Epoch 132/150\n",
      "747/747 - 0s - loss: 0.4433 - accuracy: 0.8220\n",
      "Epoch 133/150\n",
      "747/747 - 0s - loss: 0.4370 - accuracy: 0.8206\n",
      "Epoch 134/150\n",
      "747/747 - 0s - loss: 0.4429 - accuracy: 0.8153\n",
      "Epoch 135/150\n",
      "747/747 - 0s - loss: 0.4502 - accuracy: 0.8139\n",
      "Epoch 136/150\n",
      "747/747 - 0s - loss: 0.4408 - accuracy: 0.8233\n",
      "Epoch 137/150\n",
      "747/747 - 0s - loss: 0.4384 - accuracy: 0.8153\n",
      "Epoch 138/150\n",
      "747/747 - 0s - loss: 0.4462 - accuracy: 0.8220\n",
      "Epoch 139/150\n",
      "747/747 - 0s - loss: 0.4505 - accuracy: 0.8166\n",
      "Epoch 140/150\n",
      "747/747 - 0s - loss: 0.4437 - accuracy: 0.8139\n",
      "Epoch 141/150\n",
      "747/747 - 0s - loss: 0.4426 - accuracy: 0.8193\n",
      "Epoch 142/150\n",
      "747/747 - 0s - loss: 0.4450 - accuracy: 0.8153\n",
      "Epoch 143/150\n",
      "747/747 - 0s - loss: 0.4437 - accuracy: 0.8059\n",
      "Epoch 144/150\n",
      "747/747 - 0s - loss: 0.4401 - accuracy: 0.8112\n",
      "Epoch 145/150\n",
      "747/747 - 0s - loss: 0.4492 - accuracy: 0.8179\n",
      "Epoch 146/150\n",
      "747/747 - 0s - loss: 0.4433 - accuracy: 0.8153\n",
      "Epoch 147/150\n",
      "747/747 - 0s - loss: 0.4470 - accuracy: 0.8260\n",
      "Epoch 148/150\n",
      "747/747 - 0s - loss: 0.4375 - accuracy: 0.8179\n",
      "Epoch 149/150\n",
      "747/747 - 0s - loss: 0.4377 - accuracy: 0.8233\n",
      "Epoch 150/150\n",
      "747/747 - 0s - loss: 0.4384 - accuracy: 0.8166\n",
      "83/1 - 0s - loss: 0.4005 - accuracy: 0.7349\n",
      "Train on 747 samples\n",
      "Epoch 1/150\n",
      "747/747 - 2s - loss: 0.7011 - accuracy: 0.4886\n",
      "Epoch 2/150\n",
      "747/747 - 0s - loss: 0.6110 - accuracy: 0.6787\n",
      "Epoch 3/150\n",
      "747/747 - 0s - loss: 0.5714 - accuracy: 0.7082\n",
      "Epoch 4/150\n",
      "747/747 - 0s - loss: 0.5398 - accuracy: 0.7470\n",
      "Epoch 5/150\n",
      "747/747 - 0s - loss: 0.5122 - accuracy: 0.7778\n",
      "Epoch 6/150\n",
      "747/747 - 0s - loss: 0.4753 - accuracy: 0.7764\n",
      "Epoch 7/150\n",
      "747/747 - 0s - loss: 0.4899 - accuracy: 0.7778\n",
      "Epoch 8/150\n",
      "747/747 - 0s - loss: 0.5004 - accuracy: 0.7751\n",
      "Epoch 9/150\n",
      "747/747 - 0s - loss: 0.4814 - accuracy: 0.8005\n",
      "Epoch 10/150\n",
      "747/747 - 0s - loss: 0.4861 - accuracy: 0.7778\n",
      "Epoch 11/150\n",
      "747/747 - 0s - loss: 0.4899 - accuracy: 0.7791\n",
      "Epoch 12/150\n",
      "747/747 - 0s - loss: 0.4712 - accuracy: 0.8005\n",
      "Epoch 13/150\n",
      "747/747 - 0s - loss: 0.4917 - accuracy: 0.7885\n",
      "Epoch 14/150\n",
      "747/747 - 0s - loss: 0.4661 - accuracy: 0.7858\n",
      "Epoch 15/150\n",
      "747/747 - 0s - loss: 0.4759 - accuracy: 0.7979\n",
      "Epoch 16/150\n",
      "747/747 - 0s - loss: 0.4929 - accuracy: 0.7845\n",
      "Epoch 17/150\n",
      "747/747 - 0s - loss: 0.4636 - accuracy: 0.7845\n",
      "Epoch 18/150\n",
      "747/747 - 0s - loss: 0.4897 - accuracy: 0.7912\n",
      "Epoch 19/150\n",
      "747/747 - 0s - loss: 0.4804 - accuracy: 0.8032\n",
      "Epoch 20/150\n",
      "747/747 - 0s - loss: 0.4715 - accuracy: 0.7938\n",
      "Epoch 21/150\n",
      "747/747 - 0s - loss: 0.4806 - accuracy: 0.7992\n",
      "Epoch 22/150\n",
      "747/747 - 0s - loss: 0.4691 - accuracy: 0.7912\n",
      "Epoch 23/150\n",
      "747/747 - 0s - loss: 0.4656 - accuracy: 0.7979\n",
      "Epoch 24/150\n",
      "747/747 - 0s - loss: 0.4641 - accuracy: 0.7938\n",
      "Epoch 25/150\n",
      "747/747 - 0s - loss: 0.4746 - accuracy: 0.8005\n",
      "Epoch 26/150\n",
      "747/747 - 0s - loss: 0.4621 - accuracy: 0.7912\n",
      "Epoch 27/150\n",
      "747/747 - 0s - loss: 0.4640 - accuracy: 0.7965\n",
      "Epoch 28/150\n",
      "747/747 - 0s - loss: 0.4783 - accuracy: 0.8046\n",
      "Epoch 29/150\n",
      "747/747 - 0s - loss: 0.4728 - accuracy: 0.8019\n",
      "Epoch 30/150\n",
      "747/747 - 0s - loss: 0.4721 - accuracy: 0.8019\n",
      "Epoch 31/150\n",
      "747/747 - 0s - loss: 0.4698 - accuracy: 0.7938\n",
      "Epoch 32/150\n",
      "747/747 - 0s - loss: 0.4706 - accuracy: 0.7992\n",
      "Epoch 33/150\n",
      "747/747 - 0s - loss: 0.4816 - accuracy: 0.7965\n",
      "Epoch 34/150\n",
      "747/747 - 0s - loss: 0.4554 - accuracy: 0.7938\n",
      "Epoch 35/150\n",
      "747/747 - 0s - loss: 0.4683 - accuracy: 0.8046\n",
      "Epoch 36/150\n",
      "747/747 - 0s - loss: 0.4761 - accuracy: 0.7965\n",
      "Epoch 37/150\n",
      "747/747 - 0s - loss: 0.4660 - accuracy: 0.8005\n",
      "Epoch 38/150\n",
      "747/747 - 0s - loss: 0.4653 - accuracy: 0.7979\n",
      "Epoch 39/150\n",
      "747/747 - 0s - loss: 0.4543 - accuracy: 0.8032\n",
      "Epoch 40/150\n",
      "747/747 - 0s - loss: 0.4733 - accuracy: 0.8046\n",
      "Epoch 41/150\n",
      "747/747 - 0s - loss: 0.4598 - accuracy: 0.8032\n",
      "Epoch 42/150\n",
      "747/747 - 0s - loss: 0.4499 - accuracy: 0.8072\n",
      "Epoch 43/150\n",
      "747/747 - 0s - loss: 0.4503 - accuracy: 0.7952\n",
      "Epoch 44/150\n",
      "747/747 - 0s - loss: 0.4650 - accuracy: 0.7898\n",
      "Epoch 45/150\n",
      "747/747 - 0s - loss: 0.4483 - accuracy: 0.8005\n",
      "Epoch 46/150\n",
      "747/747 - 0s - loss: 0.4541 - accuracy: 0.8072\n",
      "Epoch 47/150\n",
      "747/747 - 0s - loss: 0.4494 - accuracy: 0.8112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/150\n",
      "747/747 - 0s - loss: 0.4591 - accuracy: 0.8099\n",
      "Epoch 49/150\n",
      "747/747 - 0s - loss: 0.4369 - accuracy: 0.8099\n",
      "Epoch 50/150\n",
      "747/747 - 0s - loss: 0.4630 - accuracy: 0.8112\n",
      "Epoch 51/150\n",
      "747/747 - 0s - loss: 0.4523 - accuracy: 0.7992\n",
      "Epoch 52/150\n",
      "747/747 - 0s - loss: 0.4475 - accuracy: 0.8179\n",
      "Epoch 53/150\n",
      "747/747 - 0s - loss: 0.4624 - accuracy: 0.8072\n",
      "Epoch 54/150\n",
      "747/747 - 0s - loss: 0.4529 - accuracy: 0.8019\n",
      "Epoch 55/150\n",
      "747/747 - 0s - loss: 0.4607 - accuracy: 0.8072\n",
      "Epoch 56/150\n",
      "747/747 - 0s - loss: 0.4576 - accuracy: 0.8019\n",
      "Epoch 57/150\n",
      "747/747 - 0s - loss: 0.4444 - accuracy: 0.8153\n",
      "Epoch 58/150\n",
      "747/747 - 0s - loss: 0.4490 - accuracy: 0.8072\n",
      "Epoch 59/150\n",
      "747/747 - 0s - loss: 0.4548 - accuracy: 0.8019\n",
      "Epoch 60/150\n",
      "747/747 - 0s - loss: 0.4440 - accuracy: 0.8139\n",
      "Epoch 61/150\n",
      "747/747 - 0s - loss: 0.4570 - accuracy: 0.8019\n",
      "Epoch 62/150\n",
      "747/747 - 0s - loss: 0.4592 - accuracy: 0.8059\n",
      "Epoch 63/150\n",
      "747/747 - 0s - loss: 0.4548 - accuracy: 0.7992\n",
      "Epoch 64/150\n",
      "747/747 - 0s - loss: 0.4434 - accuracy: 0.8032\n",
      "Epoch 65/150\n",
      "747/747 - 0s - loss: 0.4525 - accuracy: 0.8099\n",
      "Epoch 66/150\n",
      "747/747 - 0s - loss: 0.4510 - accuracy: 0.8099\n",
      "Epoch 67/150\n",
      "747/747 - 0s - loss: 0.4436 - accuracy: 0.7992\n",
      "Epoch 68/150\n",
      "747/747 - 0s - loss: 0.4489 - accuracy: 0.8086\n",
      "Epoch 69/150\n",
      "747/747 - 0s - loss: 0.4493 - accuracy: 0.8099\n",
      "Epoch 70/150\n",
      "747/747 - 0s - loss: 0.4512 - accuracy: 0.8179\n",
      "Epoch 71/150\n",
      "747/747 - 0s - loss: 0.4483 - accuracy: 0.8072\n",
      "Epoch 72/150\n",
      "747/747 - 0s - loss: 0.4573 - accuracy: 0.8019\n",
      "Epoch 73/150\n",
      "747/747 - 0s - loss: 0.4477 - accuracy: 0.8179\n",
      "Epoch 74/150\n",
      "747/747 - 0s - loss: 0.4488 - accuracy: 0.8005\n",
      "Epoch 75/150\n",
      "747/747 - 0s - loss: 0.4517 - accuracy: 0.8086\n",
      "Epoch 76/150\n",
      "747/747 - 0s - loss: 0.4442 - accuracy: 0.8139\n",
      "Epoch 77/150\n",
      "747/747 - 0s - loss: 0.4570 - accuracy: 0.8072\n",
      "Epoch 78/150\n",
      "747/747 - 0s - loss: 0.4527 - accuracy: 0.7992\n",
      "Epoch 79/150\n",
      "747/747 - 0s - loss: 0.4567 - accuracy: 0.8072\n",
      "Epoch 80/150\n",
      "747/747 - 0s - loss: 0.4433 - accuracy: 0.8166\n",
      "Epoch 81/150\n",
      "747/747 - 0s - loss: 0.4544 - accuracy: 0.8059\n",
      "Epoch 82/150\n",
      "747/747 - 0s - loss: 0.4455 - accuracy: 0.8086\n",
      "Epoch 83/150\n",
      "747/747 - 0s - loss: 0.4456 - accuracy: 0.8059\n",
      "Epoch 84/150\n",
      "747/747 - 0s - loss: 0.4517 - accuracy: 0.7965\n",
      "Epoch 85/150\n",
      "747/747 - 0s - loss: 0.4593 - accuracy: 0.8193\n",
      "Epoch 86/150\n",
      "747/747 - 0s - loss: 0.4443 - accuracy: 0.8046\n",
      "Epoch 87/150\n",
      "747/747 - 0s - loss: 0.4440 - accuracy: 0.8046\n",
      "Epoch 88/150\n",
      "747/747 - 0s - loss: 0.4453 - accuracy: 0.8193\n",
      "Epoch 89/150\n",
      "747/747 - 0s - loss: 0.4412 - accuracy: 0.8099\n",
      "Epoch 90/150\n",
      "747/747 - 0s - loss: 0.4490 - accuracy: 0.8193\n",
      "Epoch 91/150\n",
      "747/747 - 0s - loss: 0.4429 - accuracy: 0.8032\n",
      "Epoch 92/150\n",
      "747/747 - 0s - loss: 0.4435 - accuracy: 0.8112\n",
      "Epoch 93/150\n",
      "747/747 - 0s - loss: 0.4493 - accuracy: 0.8086\n",
      "Epoch 94/150\n",
      "747/747 - 0s - loss: 0.4569 - accuracy: 0.8099\n",
      "Epoch 95/150\n",
      "747/747 - 0s - loss: 0.4505 - accuracy: 0.8099\n",
      "Epoch 96/150\n",
      "747/747 - 0s - loss: 0.4510 - accuracy: 0.8059\n",
      "Epoch 97/150\n",
      "747/747 - 0s - loss: 0.4504 - accuracy: 0.8046\n",
      "Epoch 98/150\n",
      "747/747 - 0s - loss: 0.4509 - accuracy: 0.8072\n",
      "Epoch 99/150\n",
      "747/747 - 0s - loss: 0.4367 - accuracy: 0.8153\n",
      "Epoch 100/150\n",
      "747/747 - 0s - loss: 0.4397 - accuracy: 0.8072\n",
      "Epoch 101/150\n",
      "747/747 - 0s - loss: 0.4434 - accuracy: 0.8112\n",
      "Epoch 102/150\n",
      "747/747 - 0s - loss: 0.4364 - accuracy: 0.8086\n",
      "Epoch 103/150\n",
      "747/747 - 0s - loss: 0.4466 - accuracy: 0.8153\n",
      "Epoch 104/150\n",
      "747/747 - 0s - loss: 0.4458 - accuracy: 0.8046\n",
      "Epoch 105/150\n",
      "747/747 - 0s - loss: 0.4504 - accuracy: 0.8193\n",
      "Epoch 106/150\n",
      "747/747 - 0s - loss: 0.4541 - accuracy: 0.8086\n",
      "Epoch 107/150\n",
      "747/747 - 0s - loss: 0.4419 - accuracy: 0.8153\n",
      "Epoch 108/150\n",
      "747/747 - 0s - loss: 0.4480 - accuracy: 0.8032\n",
      "Epoch 109/150\n",
      "747/747 - 0s - loss: 0.4470 - accuracy: 0.8059\n",
      "Epoch 110/150\n",
      "747/747 - 0s - loss: 0.4488 - accuracy: 0.8086\n",
      "Epoch 111/150\n",
      "747/747 - 0s - loss: 0.4403 - accuracy: 0.8139\n",
      "Epoch 112/150\n",
      "747/747 - 0s - loss: 0.4372 - accuracy: 0.8112\n",
      "Epoch 113/150\n",
      "747/747 - 0s - loss: 0.4344 - accuracy: 0.8206\n",
      "Epoch 114/150\n",
      "747/747 - 0s - loss: 0.4478 - accuracy: 0.8126\n",
      "Epoch 115/150\n",
      "747/747 - 0s - loss: 0.4345 - accuracy: 0.8046\n",
      "Epoch 116/150\n",
      "747/747 - 0s - loss: 0.4352 - accuracy: 0.8059\n",
      "Epoch 117/150\n",
      "747/747 - 0s - loss: 0.4400 - accuracy: 0.8139\n",
      "Epoch 118/150\n",
      "747/747 - 0s - loss: 0.4510 - accuracy: 0.8032\n",
      "Epoch 119/150\n",
      "747/747 - 0s - loss: 0.4475 - accuracy: 0.8139\n",
      "Epoch 120/150\n",
      "747/747 - 0s - loss: 0.4522 - accuracy: 0.7938\n",
      "Epoch 121/150\n",
      "747/747 - 0s - loss: 0.4340 - accuracy: 0.8166\n",
      "Epoch 122/150\n",
      "747/747 - 0s - loss: 0.4448 - accuracy: 0.8112\n",
      "Epoch 123/150\n",
      "747/747 - 0s - loss: 0.4423 - accuracy: 0.8046\n",
      "Epoch 124/150\n",
      "747/747 - 0s - loss: 0.4434 - accuracy: 0.8193\n",
      "Epoch 125/150\n",
      "747/747 - 0s - loss: 0.4389 - accuracy: 0.8153\n",
      "Epoch 126/150\n",
      "747/747 - 0s - loss: 0.4464 - accuracy: 0.8126\n",
      "Epoch 127/150\n",
      "747/747 - 0s - loss: 0.4335 - accuracy: 0.8206\n",
      "Epoch 128/150\n",
      "747/747 - 0s - loss: 0.4394 - accuracy: 0.8193\n",
      "Epoch 129/150\n",
      "747/747 - 0s - loss: 0.4433 - accuracy: 0.8019\n",
      "Epoch 130/150\n",
      "747/747 - 0s - loss: 0.4448 - accuracy: 0.8126\n",
      "Epoch 131/150\n",
      "747/747 - 0s - loss: 0.4449 - accuracy: 0.8166\n",
      "Epoch 132/150\n",
      "747/747 - 0s - loss: 0.4392 - accuracy: 0.8206\n",
      "Epoch 133/150\n",
      "747/747 - 0s - loss: 0.4381 - accuracy: 0.8179\n",
      "Epoch 134/150\n",
      "747/747 - 0s - loss: 0.4369 - accuracy: 0.8046\n",
      "Epoch 135/150\n",
      "747/747 - 0s - loss: 0.4307 - accuracy: 0.8179\n",
      "Epoch 136/150\n",
      "747/747 - 0s - loss: 0.4442 - accuracy: 0.8166\n",
      "Epoch 137/150\n",
      "747/747 - 0s - loss: 0.4468 - accuracy: 0.8112\n",
      "Epoch 138/150\n",
      "747/747 - 0s - loss: 0.4428 - accuracy: 0.8153\n",
      "Epoch 139/150\n",
      "747/747 - 0s - loss: 0.4394 - accuracy: 0.8086\n",
      "Epoch 140/150\n",
      "747/747 - 0s - loss: 0.4423 - accuracy: 0.8046\n",
      "Epoch 141/150\n",
      "747/747 - 0s - loss: 0.4358 - accuracy: 0.8112\n",
      "Epoch 142/150\n",
      "747/747 - 0s - loss: 0.4386 - accuracy: 0.8206\n",
      "Epoch 143/150\n",
      "747/747 - 0s - loss: 0.4386 - accuracy: 0.8153\n",
      "Epoch 144/150\n",
      "747/747 - 0s - loss: 0.4340 - accuracy: 0.8126\n",
      "Epoch 145/150\n",
      "747/747 - 0s - loss: 0.4455 - accuracy: 0.8153\n",
      "Epoch 146/150\n",
      "747/747 - 0s - loss: 0.4315 - accuracy: 0.8246\n",
      "Epoch 147/150\n",
      "747/747 - 0s - loss: 0.4389 - accuracy: 0.8112\n",
      "Epoch 148/150\n",
      "747/747 - 0s - loss: 0.4442 - accuracy: 0.8099\n",
      "Epoch 149/150\n",
      "747/747 - 0s - loss: 0.4399 - accuracy: 0.8046\n",
      "Epoch 150/150\n",
      "747/747 - 0s - loss: 0.4386 - accuracy: 0.8153\n",
      "83/1 - 0s - loss: 0.6129 - accuracy: 0.7349\n",
      "Train on 747 samples\n",
      "Epoch 1/150\n",
      "747/747 - 1s - loss: 0.6752 - accuracy: 0.5810\n",
      "Epoch 2/150\n",
      "747/747 - 0s - loss: 0.6230 - accuracy: 0.6975\n",
      "Epoch 3/150\n",
      "747/747 - 0s - loss: 0.5758 - accuracy: 0.7497\n",
      "Epoch 4/150\n",
      "747/747 - 0s - loss: 0.5373 - accuracy: 0.7858\n",
      "Epoch 5/150\n",
      "747/747 - 0s - loss: 0.5213 - accuracy: 0.7644\n",
      "Epoch 6/150\n",
      "747/747 - 0s - loss: 0.5094 - accuracy: 0.7657\n",
      "Epoch 7/150\n",
      "747/747 - 0s - loss: 0.5053 - accuracy: 0.7858\n",
      "Epoch 8/150\n",
      "747/747 - 0s - loss: 0.5114 - accuracy: 0.7898\n",
      "Epoch 9/150\n",
      "747/747 - 0s - loss: 0.4934 - accuracy: 0.7898\n",
      "Epoch 10/150\n",
      "747/747 - 0s - loss: 0.4871 - accuracy: 0.7898\n",
      "Epoch 11/150\n",
      "747/747 - 0s - loss: 0.5070 - accuracy: 0.7818\n",
      "Epoch 12/150\n",
      "747/747 - 0s - loss: 0.4905 - accuracy: 0.7871\n",
      "Epoch 13/150\n",
      "747/747 - 0s - loss: 0.4905 - accuracy: 0.7778\n",
      "Epoch 14/150\n",
      "747/747 - 0s - loss: 0.5006 - accuracy: 0.7898\n",
      "Epoch 15/150\n",
      "747/747 - 0s - loss: 0.5014 - accuracy: 0.7738\n",
      "Epoch 16/150\n",
      "747/747 - 0s - loss: 0.5044 - accuracy: 0.7858\n",
      "Epoch 17/150\n",
      "747/747 - 0s - loss: 0.4844 - accuracy: 0.7805\n",
      "Epoch 18/150\n",
      "747/747 - 0s - loss: 0.4852 - accuracy: 0.7912\n",
      "Epoch 19/150\n",
      "747/747 - 0s - loss: 0.4940 - accuracy: 0.7764\n",
      "Epoch 20/150\n",
      "747/747 - 0s - loss: 0.4891 - accuracy: 0.7831\n",
      "Epoch 21/150\n",
      "747/747 - 0s - loss: 0.4830 - accuracy: 0.7845\n",
      "Epoch 22/150\n",
      "747/747 - 0s - loss: 0.4956 - accuracy: 0.7778\n",
      "Epoch 23/150\n",
      "747/747 - 0s - loss: 0.4825 - accuracy: 0.7805\n",
      "Epoch 24/150\n",
      "747/747 - 0s - loss: 0.4906 - accuracy: 0.7845\n",
      "Epoch 25/150\n",
      "747/747 - 0s - loss: 0.4824 - accuracy: 0.7764\n",
      "Epoch 26/150\n",
      "747/747 - 0s - loss: 0.4864 - accuracy: 0.7938\n",
      "Epoch 27/150\n",
      "747/747 - 0s - loss: 0.4849 - accuracy: 0.7778\n",
      "Epoch 28/150\n",
      "747/747 - 0s - loss: 0.4873 - accuracy: 0.7858\n",
      "Epoch 29/150\n",
      "747/747 - 0s - loss: 0.4924 - accuracy: 0.7845\n",
      "Epoch 30/150\n",
      "747/747 - 0s - loss: 0.4751 - accuracy: 0.7778\n",
      "Epoch 31/150\n",
      "747/747 - 0s - loss: 0.4775 - accuracy: 0.7871\n",
      "Epoch 32/150\n",
      "747/747 - 0s - loss: 0.4711 - accuracy: 0.7858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/150\n",
      "747/747 - 0s - loss: 0.4638 - accuracy: 0.7871\n",
      "Epoch 34/150\n",
      "747/747 - 0s - loss: 0.4821 - accuracy: 0.7751\n",
      "Epoch 35/150\n",
      "747/747 - 0s - loss: 0.4780 - accuracy: 0.7845\n",
      "Epoch 36/150\n",
      "747/747 - 0s - loss: 0.4787 - accuracy: 0.7845\n",
      "Epoch 37/150\n",
      "747/747 - 0s - loss: 0.4778 - accuracy: 0.7952\n",
      "Epoch 38/150\n",
      "747/747 - 0s - loss: 0.4729 - accuracy: 0.7871\n",
      "Epoch 39/150\n",
      "747/747 - 0s - loss: 0.4658 - accuracy: 0.7871\n",
      "Epoch 40/150\n",
      "747/747 - 0s - loss: 0.4760 - accuracy: 0.7898\n",
      "Epoch 41/150\n",
      "747/747 - 0s - loss: 0.4651 - accuracy: 0.7925\n",
      "Epoch 42/150\n",
      "747/747 - 0s - loss: 0.4795 - accuracy: 0.8005\n",
      "Epoch 43/150\n",
      "747/747 - 0s - loss: 0.4758 - accuracy: 0.7938\n",
      "Epoch 44/150\n",
      "747/747 - 0s - loss: 0.4778 - accuracy: 0.7898\n",
      "Epoch 45/150\n",
      "747/747 - 0s - loss: 0.4721 - accuracy: 0.7912\n",
      "Epoch 46/150\n",
      "747/747 - 0s - loss: 0.4722 - accuracy: 0.8032\n",
      "Epoch 47/150\n",
      "747/747 - 0s - loss: 0.4704 - accuracy: 0.8059\n",
      "Epoch 48/150\n",
      "747/747 - 0s - loss: 0.4799 - accuracy: 0.8046\n",
      "Epoch 49/150\n",
      "747/747 - 0s - loss: 0.4695 - accuracy: 0.7885\n",
      "Epoch 50/150\n",
      "747/747 - 0s - loss: 0.4710 - accuracy: 0.8019\n",
      "Epoch 51/150\n",
      "747/747 - 0s - loss: 0.4718 - accuracy: 0.8019\n",
      "Epoch 52/150\n",
      "747/747 - 0s - loss: 0.4775 - accuracy: 0.7925\n",
      "Epoch 53/150\n",
      "747/747 - 0s - loss: 0.4699 - accuracy: 0.7952\n",
      "Epoch 54/150\n",
      "747/747 - 0s - loss: 0.4708 - accuracy: 0.7912\n",
      "Epoch 55/150\n",
      "747/747 - 0s - loss: 0.4733 - accuracy: 0.7885\n",
      "Epoch 56/150\n",
      "747/747 - 0s - loss: 0.4712 - accuracy: 0.7871\n",
      "Epoch 57/150\n",
      "747/747 - 0s - loss: 0.4707 - accuracy: 0.7925\n",
      "Epoch 58/150\n",
      "747/747 - 0s - loss: 0.4755 - accuracy: 0.7845\n",
      "Epoch 59/150\n",
      "747/747 - 0s - loss: 0.4651 - accuracy: 0.7925\n",
      "Epoch 60/150\n",
      "747/747 - 0s - loss: 0.4587 - accuracy: 0.7912\n",
      "Epoch 61/150\n",
      "747/747 - 0s - loss: 0.4716 - accuracy: 0.7912\n",
      "Epoch 62/150\n",
      "747/747 - 0s - loss: 0.4713 - accuracy: 0.7885\n",
      "Epoch 63/150\n",
      "747/747 - 0s - loss: 0.4580 - accuracy: 0.7912\n",
      "Epoch 64/150\n",
      "747/747 - 0s - loss: 0.4725 - accuracy: 0.7925\n",
      "Epoch 65/150\n",
      "747/747 - 0s - loss: 0.4734 - accuracy: 0.8059\n",
      "Epoch 66/150\n",
      "747/747 - 0s - loss: 0.4603 - accuracy: 0.7831\n",
      "Epoch 67/150\n",
      "747/747 - 0s - loss: 0.4635 - accuracy: 0.7912\n",
      "Epoch 68/150\n",
      "747/747 - 0s - loss: 0.4686 - accuracy: 0.7979\n",
      "Epoch 69/150\n",
      "747/747 - 0s - loss: 0.4628 - accuracy: 0.7979\n",
      "Epoch 70/150\n",
      "747/747 - 0s - loss: 0.4697 - accuracy: 0.7965\n",
      "Epoch 71/150\n",
      "747/747 - 0s - loss: 0.4720 - accuracy: 0.7885\n",
      "Epoch 72/150\n",
      "747/747 - 0s - loss: 0.4621 - accuracy: 0.7992\n",
      "Epoch 73/150\n",
      "747/747 - 0s - loss: 0.4578 - accuracy: 0.7965\n",
      "Epoch 74/150\n",
      "747/747 - 0s - loss: 0.4645 - accuracy: 0.7898\n",
      "Epoch 75/150\n",
      "747/747 - 0s - loss: 0.4580 - accuracy: 0.7858\n",
      "Epoch 76/150\n",
      "747/747 - 0s - loss: 0.4607 - accuracy: 0.7938\n",
      "Epoch 77/150\n",
      "747/747 - 0s - loss: 0.4540 - accuracy: 0.7979\n",
      "Epoch 78/150\n",
      "747/747 - 0s - loss: 0.4704 - accuracy: 0.8032\n",
      "Epoch 79/150\n",
      "747/747 - 0s - loss: 0.4732 - accuracy: 0.7952\n",
      "Epoch 80/150\n",
      "747/747 - 0s - loss: 0.4700 - accuracy: 0.7845\n",
      "Epoch 81/150\n",
      "747/747 - 0s - loss: 0.4639 - accuracy: 0.8059\n",
      "Epoch 82/150\n",
      "747/747 - 0s - loss: 0.4568 - accuracy: 0.8019\n",
      "Epoch 83/150\n",
      "747/747 - 0s - loss: 0.4677 - accuracy: 0.8005\n",
      "Epoch 84/150\n",
      "747/747 - 0s - loss: 0.4650 - accuracy: 0.7938\n",
      "Epoch 85/150\n",
      "747/747 - 0s - loss: 0.4561 - accuracy: 0.8086\n",
      "Epoch 86/150\n",
      "747/747 - 0s - loss: 0.4624 - accuracy: 0.8099\n",
      "Epoch 87/150\n",
      "747/747 - 0s - loss: 0.4598 - accuracy: 0.7938\n",
      "Epoch 88/150\n",
      "747/747 - 0s - loss: 0.4588 - accuracy: 0.7979\n",
      "Epoch 89/150\n",
      "747/747 - 0s - loss: 0.4659 - accuracy: 0.8019\n",
      "Epoch 90/150\n",
      "747/747 - 0s - loss: 0.4632 - accuracy: 0.8046\n",
      "Epoch 91/150\n",
      "747/747 - 0s - loss: 0.4678 - accuracy: 0.7938\n",
      "Epoch 92/150\n",
      "747/747 - 0s - loss: 0.4649 - accuracy: 0.8032\n",
      "Epoch 93/150\n",
      "747/747 - 0s - loss: 0.4645 - accuracy: 0.8046\n",
      "Epoch 94/150\n",
      "747/747 - 0s - loss: 0.4710 - accuracy: 0.7938\n",
      "Epoch 95/150\n",
      "747/747 - 0s - loss: 0.4709 - accuracy: 0.7925\n",
      "Epoch 96/150\n",
      "747/747 - 0s - loss: 0.4748 - accuracy: 0.8019\n",
      "Epoch 97/150\n",
      "747/747 - 0s - loss: 0.4612 - accuracy: 0.8005\n",
      "Epoch 98/150\n",
      "747/747 - 0s - loss: 0.4598 - accuracy: 0.8005\n",
      "Epoch 99/150\n",
      "747/747 - 0s - loss: 0.4580 - accuracy: 0.8019\n",
      "Epoch 100/150\n",
      "747/747 - 0s - loss: 0.4612 - accuracy: 0.7965\n",
      "Epoch 101/150\n",
      "747/747 - 0s - loss: 0.4576 - accuracy: 0.8059\n",
      "Epoch 102/150\n",
      "747/747 - 0s - loss: 0.4660 - accuracy: 0.7965\n",
      "Epoch 103/150\n",
      "747/747 - 0s - loss: 0.4614 - accuracy: 0.8086\n",
      "Epoch 104/150\n",
      "747/747 - 0s - loss: 0.4521 - accuracy: 0.8046\n",
      "Epoch 105/150\n",
      "747/747 - 0s - loss: 0.4734 - accuracy: 0.7952\n",
      "Epoch 106/150\n",
      "747/747 - 0s - loss: 0.4546 - accuracy: 0.8059\n",
      "Epoch 107/150\n",
      "747/747 - 0s - loss: 0.4601 - accuracy: 0.7992\n",
      "Epoch 108/150\n",
      "747/747 - 0s - loss: 0.4597 - accuracy: 0.7925\n",
      "Epoch 109/150\n",
      "747/747 - 0s - loss: 0.4674 - accuracy: 0.8032\n",
      "Epoch 110/150\n",
      "747/747 - 0s - loss: 0.4670 - accuracy: 0.8019\n",
      "Epoch 111/150\n",
      "747/747 - 0s - loss: 0.4675 - accuracy: 0.7952\n",
      "Epoch 112/150\n",
      "747/747 - 0s - loss: 0.4698 - accuracy: 0.8019\n",
      "Epoch 113/150\n",
      "747/747 - 0s - loss: 0.4574 - accuracy: 0.8046\n",
      "Epoch 114/150\n",
      "747/747 - 0s - loss: 0.4618 - accuracy: 0.7992\n",
      "Epoch 115/150\n",
      "747/747 - 0s - loss: 0.4584 - accuracy: 0.8005\n",
      "Epoch 116/150\n",
      "747/747 - 0s - loss: 0.4545 - accuracy: 0.8059\n",
      "Epoch 117/150\n",
      "747/747 - 0s - loss: 0.4581 - accuracy: 0.8005\n",
      "Epoch 118/150\n",
      "747/747 - 0s - loss: 0.4572 - accuracy: 0.8046\n",
      "Epoch 119/150\n",
      "747/747 - 0s - loss: 0.4625 - accuracy: 0.7979\n",
      "Epoch 120/150\n",
      "747/747 - 0s - loss: 0.4609 - accuracy: 0.7885\n",
      "Epoch 121/150\n",
      "747/747 - 0s - loss: 0.4518 - accuracy: 0.8019\n",
      "Epoch 122/150\n",
      "747/747 - 0s - loss: 0.4563 - accuracy: 0.7938\n",
      "Epoch 123/150\n",
      "747/747 - 0s - loss: 0.4566 - accuracy: 0.8046\n",
      "Epoch 124/150\n",
      "747/747 - 0s - loss: 0.4651 - accuracy: 0.8019\n",
      "Epoch 125/150\n",
      "747/747 - 0s - loss: 0.4565 - accuracy: 0.8139\n",
      "Epoch 126/150\n",
      "747/747 - 0s - loss: 0.4628 - accuracy: 0.7952\n",
      "Epoch 127/150\n",
      "747/747 - 0s - loss: 0.4538 - accuracy: 0.8072\n",
      "Epoch 128/150\n",
      "747/747 - 0s - loss: 0.4503 - accuracy: 0.8046\n",
      "Epoch 129/150\n",
      "747/747 - 0s - loss: 0.4528 - accuracy: 0.8179\n",
      "Epoch 130/150\n",
      "747/747 - 0s - loss: 0.4581 - accuracy: 0.8032\n",
      "Epoch 131/150\n",
      "747/747 - 0s - loss: 0.4621 - accuracy: 0.7965\n",
      "Epoch 132/150\n",
      "747/747 - 0s - loss: 0.4623 - accuracy: 0.8032\n",
      "Epoch 133/150\n",
      "747/747 - 0s - loss: 0.4624 - accuracy: 0.8005\n",
      "Epoch 134/150\n",
      "747/747 - 0s - loss: 0.4487 - accuracy: 0.8086\n",
      "Epoch 135/150\n",
      "747/747 - 0s - loss: 0.4622 - accuracy: 0.7952\n",
      "Epoch 136/150\n",
      "747/747 - 0s - loss: 0.4568 - accuracy: 0.8032\n",
      "Epoch 137/150\n",
      "747/747 - 0s - loss: 0.4608 - accuracy: 0.7952\n",
      "Epoch 138/150\n",
      "747/747 - 0s - loss: 0.4685 - accuracy: 0.7992\n",
      "Epoch 139/150\n",
      "747/747 - 0s - loss: 0.4568 - accuracy: 0.8099\n",
      "Epoch 140/150\n",
      "747/747 - 0s - loss: 0.4543 - accuracy: 0.8126\n",
      "Epoch 141/150\n",
      "747/747 - 0s - loss: 0.4621 - accuracy: 0.8046\n",
      "Epoch 142/150\n",
      "747/747 - 0s - loss: 0.4533 - accuracy: 0.7938\n",
      "Epoch 143/150\n",
      "747/747 - 0s - loss: 0.4573 - accuracy: 0.8046\n",
      "Epoch 144/150\n",
      "747/747 - 0s - loss: 0.4620 - accuracy: 0.8032\n",
      "Epoch 145/150\n",
      "747/747 - 0s - loss: 0.4571 - accuracy: 0.8086\n",
      "Epoch 146/150\n",
      "747/747 - 0s - loss: 0.4640 - accuracy: 0.8059\n",
      "Epoch 147/150\n",
      "747/747 - 0s - loss: 0.4504 - accuracy: 0.8005\n",
      "Epoch 148/150\n",
      "747/747 - 0s - loss: 0.4533 - accuracy: 0.8046\n",
      "Epoch 149/150\n",
      "747/747 - 0s - loss: 0.4619 - accuracy: 0.8086\n",
      "Epoch 150/150\n",
      "747/747 - 0s - loss: 0.4573 - accuracy: 0.8005\n",
      "83/1 - 0s - loss: 0.4247 - accuracy: 0.8554\n",
      "Train on 747 samples\n",
      "Epoch 1/150\n",
      "747/747 - 1s - loss: 0.7156 - accuracy: 0.4900\n",
      "Epoch 2/150\n",
      "747/747 - 0s - loss: 0.6357 - accuracy: 0.6238\n",
      "Epoch 3/150\n",
      "747/747 - 0s - loss: 0.5918 - accuracy: 0.7149\n",
      "Epoch 4/150\n",
      "747/747 - 0s - loss: 0.5541 - accuracy: 0.7376\n",
      "Epoch 5/150\n",
      "747/747 - 0s - loss: 0.5284 - accuracy: 0.7470\n",
      "Epoch 6/150\n",
      "747/747 - 0s - loss: 0.5292 - accuracy: 0.7550\n",
      "Epoch 7/150\n",
      "747/747 - 0s - loss: 0.5249 - accuracy: 0.7751\n",
      "Epoch 8/150\n",
      "747/747 - 0s - loss: 0.5144 - accuracy: 0.7711\n",
      "Epoch 9/150\n",
      "747/747 - 0s - loss: 0.5133 - accuracy: 0.7764\n",
      "Epoch 10/150\n",
      "747/747 - 0s - loss: 0.5022 - accuracy: 0.7751\n",
      "Epoch 11/150\n",
      "747/747 - 0s - loss: 0.5062 - accuracy: 0.7805\n",
      "Epoch 12/150\n",
      "747/747 - 0s - loss: 0.5036 - accuracy: 0.7738\n",
      "Epoch 13/150\n",
      "747/747 - 0s - loss: 0.5145 - accuracy: 0.7751\n",
      "Epoch 14/150\n",
      "747/747 - 0s - loss: 0.5079 - accuracy: 0.7764\n",
      "Epoch 15/150\n",
      "747/747 - 0s - loss: 0.4922 - accuracy: 0.7845\n",
      "Epoch 16/150\n",
      "747/747 - 0s - loss: 0.4878 - accuracy: 0.7764\n",
      "Epoch 17/150\n",
      "747/747 - 0s - loss: 0.4911 - accuracy: 0.7738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/150\n",
      "747/747 - 0s - loss: 0.4972 - accuracy: 0.7818\n",
      "Epoch 19/150\n",
      "747/747 - 0s - loss: 0.5030 - accuracy: 0.7831\n",
      "Epoch 20/150\n",
      "747/747 - 0s - loss: 0.5133 - accuracy: 0.7805\n",
      "Epoch 21/150\n",
      "747/747 - 0s - loss: 0.4920 - accuracy: 0.7791\n",
      "Epoch 22/150\n",
      "747/747 - 0s - loss: 0.4770 - accuracy: 0.7938\n",
      "Epoch 23/150\n",
      "747/747 - 0s - loss: 0.5028 - accuracy: 0.7858\n",
      "Epoch 24/150\n",
      "747/747 - 0s - loss: 0.4823 - accuracy: 0.7858\n",
      "Epoch 25/150\n",
      "747/747 - 0s - loss: 0.4924 - accuracy: 0.7751\n",
      "Epoch 26/150\n",
      "747/747 - 0s - loss: 0.4916 - accuracy: 0.7818\n",
      "Epoch 27/150\n",
      "747/747 - 0s - loss: 0.4900 - accuracy: 0.7845\n",
      "Epoch 28/150\n",
      "747/747 - 0s - loss: 0.4778 - accuracy: 0.7912\n",
      "Epoch 29/150\n",
      "747/747 - 0s - loss: 0.4844 - accuracy: 0.7845\n",
      "Epoch 30/150\n",
      "747/747 - 0s - loss: 0.4867 - accuracy: 0.7738\n",
      "Epoch 31/150\n",
      "747/747 - 0s - loss: 0.4670 - accuracy: 0.7871\n",
      "Epoch 32/150\n",
      "747/747 - 0s - loss: 0.4855 - accuracy: 0.7871\n",
      "Epoch 33/150\n",
      "747/747 - 0s - loss: 0.4770 - accuracy: 0.7818\n",
      "Epoch 34/150\n",
      "747/747 - 0s - loss: 0.4831 - accuracy: 0.7885\n",
      "Epoch 35/150\n",
      "747/747 - 0s - loss: 0.4791 - accuracy: 0.7925\n",
      "Epoch 36/150\n",
      "747/747 - 0s - loss: 0.4772 - accuracy: 0.7845\n",
      "Epoch 37/150\n",
      "747/747 - 0s - loss: 0.4898 - accuracy: 0.7845\n",
      "Epoch 38/150\n",
      "747/747 - 0s - loss: 0.4768 - accuracy: 0.7818\n",
      "Epoch 39/150\n",
      "747/747 - 0s - loss: 0.4798 - accuracy: 0.7858\n",
      "Epoch 40/150\n",
      "747/747 - 0s - loss: 0.4665 - accuracy: 0.7938\n",
      "Epoch 41/150\n",
      "747/747 - 0s - loss: 0.4843 - accuracy: 0.7898\n",
      "Epoch 42/150\n",
      "747/747 - 0s - loss: 0.4747 - accuracy: 0.7845\n",
      "Epoch 43/150\n",
      "747/747 - 0s - loss: 0.4745 - accuracy: 0.7871\n",
      "Epoch 44/150\n",
      "747/747 - 0s - loss: 0.4756 - accuracy: 0.7885\n",
      "Epoch 45/150\n",
      "747/747 - 0s - loss: 0.4823 - accuracy: 0.7938\n",
      "Epoch 46/150\n",
      "747/747 - 0s - loss: 0.4731 - accuracy: 0.7912\n",
      "Epoch 47/150\n",
      "747/747 - 0s - loss: 0.4755 - accuracy: 0.7818\n",
      "Epoch 48/150\n",
      "747/747 - 0s - loss: 0.4784 - accuracy: 0.7885\n",
      "Epoch 49/150\n",
      "747/747 - 0s - loss: 0.4735 - accuracy: 0.7898\n",
      "Epoch 50/150\n",
      "747/747 - 0s - loss: 0.4605 - accuracy: 0.7938\n",
      "Epoch 51/150\n",
      "747/747 - 0s - loss: 0.4856 - accuracy: 0.7885\n",
      "Epoch 52/150\n",
      "747/747 - 0s - loss: 0.4751 - accuracy: 0.7885\n",
      "Epoch 53/150\n",
      "747/747 - 0s - loss: 0.4779 - accuracy: 0.7871\n",
      "Epoch 54/150\n",
      "747/747 - 0s - loss: 0.4726 - accuracy: 0.7845\n",
      "Epoch 55/150\n",
      "747/747 - 0s - loss: 0.4853 - accuracy: 0.7831\n",
      "Epoch 56/150\n",
      "747/747 - 0s - loss: 0.4733 - accuracy: 0.7845\n",
      "Epoch 57/150\n",
      "747/747 - 0s - loss: 0.4805 - accuracy: 0.7898\n",
      "Epoch 58/150\n",
      "747/747 - 0s - loss: 0.4771 - accuracy: 0.7818\n",
      "Epoch 59/150\n",
      "747/747 - 0s - loss: 0.4740 - accuracy: 0.7912\n",
      "Epoch 60/150\n",
      "747/747 - 0s - loss: 0.4677 - accuracy: 0.8019\n",
      "Epoch 61/150\n",
      "747/747 - 0s - loss: 0.4813 - accuracy: 0.7764\n",
      "Epoch 62/150\n",
      "747/747 - 0s - loss: 0.4651 - accuracy: 0.7938\n",
      "Epoch 63/150\n",
      "747/747 - 0s - loss: 0.4763 - accuracy: 0.7925\n",
      "Epoch 64/150\n",
      "747/747 - 0s - loss: 0.4730 - accuracy: 0.7912\n",
      "Epoch 65/150\n",
      "747/747 - 0s - loss: 0.4761 - accuracy: 0.7885\n",
      "Epoch 66/150\n",
      "747/747 - 0s - loss: 0.4691 - accuracy: 0.7979\n",
      "Epoch 67/150\n",
      "747/747 - 0s - loss: 0.4792 - accuracy: 0.7845\n",
      "Epoch 68/150\n",
      "747/747 - 0s - loss: 0.4744 - accuracy: 0.7925\n",
      "Epoch 69/150\n",
      "747/747 - 0s - loss: 0.4665 - accuracy: 0.7912\n",
      "Epoch 70/150\n",
      "747/747 - 0s - loss: 0.4689 - accuracy: 0.7885\n",
      "Epoch 71/150\n",
      "747/747 - 0s - loss: 0.4659 - accuracy: 0.8005\n",
      "Epoch 72/150\n",
      "747/747 - 0s - loss: 0.4674 - accuracy: 0.7965\n",
      "Epoch 73/150\n",
      "747/747 - 0s - loss: 0.4614 - accuracy: 0.7938\n",
      "Epoch 74/150\n",
      "747/747 - 0s - loss: 0.4564 - accuracy: 0.7912\n",
      "Epoch 75/150\n",
      "747/747 - 0s - loss: 0.4598 - accuracy: 0.7938\n",
      "Epoch 76/150\n",
      "747/747 - 0s - loss: 0.4540 - accuracy: 0.7885\n",
      "Epoch 77/150\n",
      "747/747 - 0s - loss: 0.4684 - accuracy: 0.7952\n",
      "Epoch 78/150\n",
      "747/747 - 0s - loss: 0.4548 - accuracy: 0.8032\n",
      "Epoch 79/150\n",
      "747/747 - 0s - loss: 0.4709 - accuracy: 0.7898\n",
      "Epoch 80/150\n",
      "747/747 - 0s - loss: 0.4679 - accuracy: 0.7992\n",
      "Epoch 81/150\n",
      "747/747 - 0s - loss: 0.4732 - accuracy: 0.7925\n",
      "Epoch 82/150\n",
      "747/747 - 0s - loss: 0.4667 - accuracy: 0.7979\n",
      "Epoch 83/150\n",
      "747/747 - 0s - loss: 0.4535 - accuracy: 0.8046\n",
      "Epoch 84/150\n",
      "747/747 - 0s - loss: 0.4540 - accuracy: 0.7979\n",
      "Epoch 85/150\n",
      "747/747 - 0s - loss: 0.4707 - accuracy: 0.7952\n",
      "Epoch 86/150\n",
      "747/747 - 0s - loss: 0.4567 - accuracy: 0.8019\n",
      "Epoch 87/150\n",
      "747/747 - 0s - loss: 0.4695 - accuracy: 0.7992\n",
      "Epoch 88/150\n",
      "747/747 - 0s - loss: 0.4597 - accuracy: 0.7898\n",
      "Epoch 89/150\n",
      "747/747 - 0s - loss: 0.4770 - accuracy: 0.7885\n",
      "Epoch 90/150\n",
      "747/747 - 0s - loss: 0.4618 - accuracy: 0.7979\n",
      "Epoch 91/150\n",
      "747/747 - 0s - loss: 0.4764 - accuracy: 0.7925\n",
      "Epoch 92/150\n",
      "747/747 - 0s - loss: 0.4713 - accuracy: 0.7898\n",
      "Epoch 93/150\n",
      "747/747 - 0s - loss: 0.4669 - accuracy: 0.7912\n",
      "Epoch 94/150\n",
      "747/747 - 0s - loss: 0.4719 - accuracy: 0.7979\n",
      "Epoch 95/150\n",
      "747/747 - 0s - loss: 0.4711 - accuracy: 0.7992\n",
      "Epoch 96/150\n",
      "747/747 - 0s - loss: 0.4675 - accuracy: 0.7965\n",
      "Epoch 97/150\n",
      "747/747 - 0s - loss: 0.4613 - accuracy: 0.7979\n",
      "Epoch 98/150\n",
      "747/747 - 0s - loss: 0.4686 - accuracy: 0.7938\n",
      "Epoch 99/150\n",
      "747/747 - 0s - loss: 0.4663 - accuracy: 0.7952\n",
      "Epoch 100/150\n",
      "747/747 - 0s - loss: 0.4545 - accuracy: 0.7952\n",
      "Epoch 101/150\n",
      "747/747 - 0s - loss: 0.4665 - accuracy: 0.7952\n",
      "Epoch 102/150\n",
      "747/747 - 0s - loss: 0.4578 - accuracy: 0.7952\n",
      "Epoch 103/150\n",
      "747/747 - 0s - loss: 0.4686 - accuracy: 0.7938\n",
      "Epoch 104/150\n",
      "747/747 - 0s - loss: 0.4619 - accuracy: 0.8059\n",
      "Epoch 105/150\n",
      "747/747 - 0s - loss: 0.4697 - accuracy: 0.7952\n",
      "Epoch 106/150\n",
      "747/747 - 0s - loss: 0.4498 - accuracy: 0.8005\n",
      "Epoch 107/150\n",
      "747/747 - 0s - loss: 0.4590 - accuracy: 0.7925\n",
      "Epoch 108/150\n",
      "747/747 - 0s - loss: 0.4491 - accuracy: 0.8032\n",
      "Epoch 109/150\n",
      "747/747 - 0s - loss: 0.4695 - accuracy: 0.7925\n",
      "Epoch 110/150\n",
      "747/747 - 0s - loss: 0.4714 - accuracy: 0.8019\n",
      "Epoch 111/150\n",
      "747/747 - 0s - loss: 0.4674 - accuracy: 0.7938\n",
      "Epoch 112/150\n",
      "747/747 - 0s - loss: 0.4591 - accuracy: 0.7965\n",
      "Epoch 113/150\n",
      "747/747 - 0s - loss: 0.4734 - accuracy: 0.7979\n",
      "Epoch 114/150\n",
      "747/747 - 0s - loss: 0.4709 - accuracy: 0.7938\n",
      "Epoch 115/150\n",
      "747/747 - 0s - loss: 0.4558 - accuracy: 0.7965\n",
      "Epoch 116/150\n",
      "747/747 - 0s - loss: 0.4656 - accuracy: 0.7912\n",
      "Epoch 117/150\n",
      "747/747 - 0s - loss: 0.4648 - accuracy: 0.7992\n",
      "Epoch 118/150\n",
      "747/747 - 0s - loss: 0.4616 - accuracy: 0.7952\n",
      "Epoch 119/150\n",
      "747/747 - 0s - loss: 0.4501 - accuracy: 0.7885\n",
      "Epoch 120/150\n",
      "747/747 - 0s - loss: 0.4685 - accuracy: 0.7965\n",
      "Epoch 121/150\n",
      "747/747 - 0s - loss: 0.4636 - accuracy: 0.8019\n",
      "Epoch 122/150\n",
      "747/747 - 0s - loss: 0.4675 - accuracy: 0.8032\n",
      "Epoch 123/150\n",
      "747/747 - 0s - loss: 0.4625 - accuracy: 0.7952\n",
      "Epoch 124/150\n",
      "747/747 - 0s - loss: 0.4700 - accuracy: 0.7992\n",
      "Epoch 125/150\n",
      "747/747 - 0s - loss: 0.4639 - accuracy: 0.8032\n",
      "Epoch 126/150\n",
      "747/747 - 0s - loss: 0.4616 - accuracy: 0.7925\n",
      "Epoch 127/150\n",
      "747/747 - 0s - loss: 0.4572 - accuracy: 0.8072\n",
      "Epoch 128/150\n",
      "747/747 - 0s - loss: 0.4545 - accuracy: 0.7952\n",
      "Epoch 129/150\n",
      "747/747 - 0s - loss: 0.4591 - accuracy: 0.7898\n",
      "Epoch 130/150\n",
      "747/747 - 0s - loss: 0.4680 - accuracy: 0.8059\n",
      "Epoch 131/150\n",
      "747/747 - 0s - loss: 0.4679 - accuracy: 0.8005\n",
      "Epoch 132/150\n",
      "747/747 - 0s - loss: 0.4636 - accuracy: 0.8005\n",
      "Epoch 133/150\n",
      "747/747 - 0s - loss: 0.4645 - accuracy: 0.8019\n",
      "Epoch 134/150\n",
      "747/747 - 0s - loss: 0.4590 - accuracy: 0.8099\n",
      "Epoch 135/150\n",
      "747/747 - 0s - loss: 0.4727 - accuracy: 0.8032\n",
      "Epoch 136/150\n",
      "747/747 - 0s - loss: 0.4635 - accuracy: 0.8005\n",
      "Epoch 137/150\n",
      "747/747 - 0s - loss: 0.4569 - accuracy: 0.7912\n",
      "Epoch 138/150\n",
      "747/747 - 0s - loss: 0.4607 - accuracy: 0.7992\n",
      "Epoch 139/150\n",
      "747/747 - 0s - loss: 0.4505 - accuracy: 0.8059\n",
      "Epoch 140/150\n",
      "747/747 - 0s - loss: 0.4531 - accuracy: 0.7979\n",
      "Epoch 141/150\n",
      "747/747 - 0s - loss: 0.4701 - accuracy: 0.7952\n",
      "Epoch 142/150\n",
      "747/747 - 0s - loss: 0.4641 - accuracy: 0.8046\n",
      "Epoch 143/150\n",
      "747/747 - 0s - loss: 0.4581 - accuracy: 0.7965\n",
      "Epoch 144/150\n",
      "747/747 - 0s - loss: 0.4569 - accuracy: 0.8059\n",
      "Epoch 145/150\n",
      "747/747 - 0s - loss: 0.4626 - accuracy: 0.7992\n",
      "Epoch 146/150\n",
      "747/747 - 0s - loss: 0.4559 - accuracy: 0.8046\n",
      "Epoch 147/150\n",
      "747/747 - 0s - loss: 0.4586 - accuracy: 0.7965\n",
      "Epoch 148/150\n",
      "747/747 - 0s - loss: 0.4678 - accuracy: 0.7992\n",
      "Epoch 149/150\n",
      "747/747 - 0s - loss: 0.4630 - accuracy: 0.8059\n",
      "Epoch 150/150\n",
      "747/747 - 0s - loss: 0.4639 - accuracy: 0.7898\n",
      "83/1 - 0s - loss: 0.5018 - accuracy: 0.8795\n",
      "Train on 747 samples\n",
      "Epoch 1/150\n",
      "747/747 - 1s - loss: 0.5810 - accuracy: 0.6894\n",
      "Epoch 2/150\n",
      "747/747 - 0s - loss: 0.5503 - accuracy: 0.7470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/150\n",
      "747/747 - 0s - loss: 0.5225 - accuracy: 0.7564\n",
      "Epoch 4/150\n",
      "747/747 - 0s - loss: 0.5114 - accuracy: 0.7805\n",
      "Epoch 5/150\n",
      "747/747 - 0s - loss: 0.4897 - accuracy: 0.7992\n",
      "Epoch 6/150\n",
      "747/747 - 0s - loss: 0.4920 - accuracy: 0.7845\n",
      "Epoch 7/150\n",
      "747/747 - 0s - loss: 0.5051 - accuracy: 0.7912\n",
      "Epoch 8/150\n",
      "747/747 - 0s - loss: 0.4805 - accuracy: 0.7871\n",
      "Epoch 9/150\n",
      "747/747 - 0s - loss: 0.4939 - accuracy: 0.7912\n",
      "Epoch 10/150\n",
      "747/747 - 0s - loss: 0.4956 - accuracy: 0.7845\n",
      "Epoch 11/150\n",
      "747/747 - 0s - loss: 0.4815 - accuracy: 0.7845\n",
      "Epoch 12/150\n",
      "747/747 - 0s - loss: 0.5021 - accuracy: 0.7818\n",
      "Epoch 13/150\n",
      "747/747 - 0s - loss: 0.4863 - accuracy: 0.7885\n",
      "Epoch 14/150\n",
      "747/747 - 0s - loss: 0.4773 - accuracy: 0.8072\n",
      "Epoch 15/150\n",
      "747/747 - 0s - loss: 0.4928 - accuracy: 0.7818\n",
      "Epoch 16/150\n",
      "747/747 - 0s - loss: 0.4689 - accuracy: 0.7858\n",
      "Epoch 17/150\n",
      "747/747 - 0s - loss: 0.4811 - accuracy: 0.8059\n",
      "Epoch 18/150\n",
      "747/747 - 0s - loss: 0.4650 - accuracy: 0.7965\n",
      "Epoch 19/150\n",
      "747/747 - 0s - loss: 0.4762 - accuracy: 0.7885\n",
      "Epoch 20/150\n",
      "747/747 - 0s - loss: 0.4733 - accuracy: 0.8046\n",
      "Epoch 21/150\n",
      "747/747 - 0s - loss: 0.4688 - accuracy: 0.7979\n",
      "Epoch 22/150\n",
      "747/747 - 0s - loss: 0.4728 - accuracy: 0.7952\n",
      "Epoch 23/150\n",
      "747/747 - 0s - loss: 0.4692 - accuracy: 0.8005\n",
      "Epoch 24/150\n",
      "747/747 - 0s - loss: 0.4674 - accuracy: 0.8032\n",
      "Epoch 25/150\n",
      "747/747 - 0s - loss: 0.4676 - accuracy: 0.8005\n",
      "Epoch 26/150\n",
      "747/747 - 0s - loss: 0.4808 - accuracy: 0.7965\n",
      "Epoch 27/150\n",
      "747/747 - 0s - loss: 0.4730 - accuracy: 0.8032\n",
      "Epoch 28/150\n",
      "747/747 - 0s - loss: 0.4647 - accuracy: 0.7938\n",
      "Epoch 29/150\n",
      "747/747 - 0s - loss: 0.4693 - accuracy: 0.7885\n",
      "Epoch 30/150\n",
      "747/747 - 0s - loss: 0.4735 - accuracy: 0.7858\n",
      "Epoch 31/150\n",
      "747/747 - 0s - loss: 0.4635 - accuracy: 0.8059\n",
      "Epoch 32/150\n",
      "747/747 - 0s - loss: 0.4674 - accuracy: 0.7965\n",
      "Epoch 33/150\n",
      "747/747 - 0s - loss: 0.4662 - accuracy: 0.7871\n",
      "Epoch 34/150\n",
      "747/747 - 0s - loss: 0.4638 - accuracy: 0.8019\n",
      "Epoch 35/150\n",
      "747/747 - 0s - loss: 0.4563 - accuracy: 0.7979\n",
      "Epoch 36/150\n",
      "747/747 - 0s - loss: 0.4639 - accuracy: 0.7965\n",
      "Epoch 37/150\n",
      "747/747 - 0s - loss: 0.4563 - accuracy: 0.8019\n",
      "Epoch 38/150\n",
      "747/747 - 0s - loss: 0.4557 - accuracy: 0.7992\n",
      "Epoch 39/150\n",
      "747/747 - 0s - loss: 0.4558 - accuracy: 0.8032\n",
      "Epoch 40/150\n",
      "747/747 - 0s - loss: 0.4561 - accuracy: 0.8019\n",
      "Epoch 41/150\n",
      "747/747 - 0s - loss: 0.4547 - accuracy: 0.8086\n",
      "Epoch 42/150\n",
      "747/747 - 0s - loss: 0.4569 - accuracy: 0.8005\n",
      "Epoch 43/150\n",
      "747/747 - 0s - loss: 0.4503 - accuracy: 0.8046\n",
      "Epoch 44/150\n",
      "747/747 - 0s - loss: 0.4709 - accuracy: 0.8126\n",
      "Epoch 45/150\n",
      "747/747 - 0s - loss: 0.4549 - accuracy: 0.8072\n",
      "Epoch 46/150\n",
      "747/747 - 0s - loss: 0.4647 - accuracy: 0.8032\n",
      "Epoch 47/150\n",
      "747/747 - 0s - loss: 0.4552 - accuracy: 0.7938\n",
      "Epoch 48/150\n",
      "747/747 - 0s - loss: 0.4608 - accuracy: 0.7912\n",
      "Epoch 49/150\n",
      "747/747 - 0s - loss: 0.4541 - accuracy: 0.8112\n",
      "Epoch 50/150\n",
      "747/747 - 0s - loss: 0.4500 - accuracy: 0.8112\n",
      "Epoch 51/150\n",
      "747/747 - 0s - loss: 0.4592 - accuracy: 0.7952\n",
      "Epoch 52/150\n",
      "747/747 - 0s - loss: 0.4557 - accuracy: 0.8005\n",
      "Epoch 53/150\n",
      "747/747 - 0s - loss: 0.4530 - accuracy: 0.8112\n",
      "Epoch 54/150\n",
      "747/747 - 0s - loss: 0.4584 - accuracy: 0.8112\n",
      "Epoch 55/150\n",
      "747/747 - 0s - loss: 0.4624 - accuracy: 0.8032\n",
      "Epoch 56/150\n",
      "747/747 - 0s - loss: 0.4621 - accuracy: 0.7898\n",
      "Epoch 57/150\n",
      "747/747 - 0s - loss: 0.4605 - accuracy: 0.7965\n",
      "Epoch 58/150\n",
      "747/747 - 0s - loss: 0.4536 - accuracy: 0.7992\n",
      "Epoch 59/150\n",
      "747/747 - 0s - loss: 0.4617 - accuracy: 0.8032\n",
      "Epoch 60/150\n",
      "747/747 - 0s - loss: 0.4602 - accuracy: 0.7979\n",
      "Epoch 61/150\n",
      "747/747 - 0s - loss: 0.4609 - accuracy: 0.7992\n",
      "Epoch 62/150\n",
      "747/747 - 0s - loss: 0.4619 - accuracy: 0.8019\n",
      "Epoch 63/150\n",
      "747/747 - 0s - loss: 0.4532 - accuracy: 0.8086\n",
      "Epoch 64/150\n",
      "747/747 - 0s - loss: 0.4608 - accuracy: 0.8005\n",
      "Epoch 65/150\n",
      "747/747 - 0s - loss: 0.4625 - accuracy: 0.8032\n",
      "Epoch 66/150\n",
      "747/747 - 0s - loss: 0.4471 - accuracy: 0.8059\n",
      "Epoch 67/150\n",
      "747/747 - 0s - loss: 0.4571 - accuracy: 0.8005\n",
      "Epoch 68/150\n",
      "747/747 - 0s - loss: 0.4494 - accuracy: 0.8019\n",
      "Epoch 69/150\n",
      "747/747 - 0s - loss: 0.4502 - accuracy: 0.8112\n",
      "Epoch 70/150\n",
      "747/747 - 0s - loss: 0.4559 - accuracy: 0.8019\n",
      "Epoch 71/150\n",
      "747/747 - 0s - loss: 0.4491 - accuracy: 0.8046\n",
      "Epoch 72/150\n",
      "747/747 - 0s - loss: 0.4573 - accuracy: 0.7952\n",
      "Epoch 73/150\n",
      "747/747 - 0s - loss: 0.4449 - accuracy: 0.8059\n",
      "Epoch 74/150\n",
      "747/747 - 0s - loss: 0.4452 - accuracy: 0.8005\n",
      "Epoch 75/150\n",
      "747/747 - 0s - loss: 0.4545 - accuracy: 0.8059\n",
      "Epoch 76/150\n",
      "747/747 - 0s - loss: 0.4480 - accuracy: 0.8032\n",
      "Epoch 77/150\n",
      "747/747 - 0s - loss: 0.4471 - accuracy: 0.7979\n",
      "Epoch 78/150\n",
      "747/747 - 0s - loss: 0.4503 - accuracy: 0.7925\n",
      "Epoch 79/150\n",
      "747/747 - 0s - loss: 0.4413 - accuracy: 0.8032\n",
      "Epoch 80/150\n",
      "747/747 - 0s - loss: 0.4521 - accuracy: 0.8112\n",
      "Epoch 81/150\n",
      "747/747 - 0s - loss: 0.4513 - accuracy: 0.7965\n",
      "Epoch 82/150\n",
      "747/747 - 0s - loss: 0.4319 - accuracy: 0.8126\n",
      "Epoch 83/150\n",
      "747/747 - 0s - loss: 0.4476 - accuracy: 0.8086\n",
      "Epoch 84/150\n",
      "747/747 - 0s - loss: 0.4549 - accuracy: 0.7979\n",
      "Epoch 85/150\n",
      "747/747 - 0s - loss: 0.4567 - accuracy: 0.7979\n",
      "Epoch 86/150\n",
      "747/747 - 0s - loss: 0.4472 - accuracy: 0.8032\n",
      "Epoch 87/150\n",
      "747/747 - 0s - loss: 0.4449 - accuracy: 0.8086\n",
      "Epoch 88/150\n",
      "747/747 - 0s - loss: 0.4464 - accuracy: 0.8126\n",
      "Epoch 89/150\n",
      "747/747 - 0s - loss: 0.4618 - accuracy: 0.7979\n",
      "Epoch 90/150\n",
      "747/747 - 0s - loss: 0.4436 - accuracy: 0.8126\n",
      "Epoch 91/150\n",
      "747/747 - 0s - loss: 0.4564 - accuracy: 0.8072\n",
      "Epoch 92/150\n",
      "747/747 - 0s - loss: 0.4599 - accuracy: 0.8005\n",
      "Epoch 93/150\n",
      "747/747 - 0s - loss: 0.4424 - accuracy: 0.8005\n",
      "Epoch 94/150\n",
      "747/747 - 0s - loss: 0.4594 - accuracy: 0.8046\n",
      "Epoch 95/150\n",
      "747/747 - 0s - loss: 0.4446 - accuracy: 0.8005\n",
      "Epoch 96/150\n",
      "747/747 - 0s - loss: 0.4635 - accuracy: 0.7952\n",
      "Epoch 97/150\n",
      "747/747 - 0s - loss: 0.4497 - accuracy: 0.8019\n",
      "Epoch 98/150\n",
      "747/747 - 0s - loss: 0.4482 - accuracy: 0.8046\n",
      "Epoch 99/150\n",
      "747/747 - 0s - loss: 0.4489 - accuracy: 0.8032\n",
      "Epoch 100/150\n",
      "747/747 - 0s - loss: 0.4524 - accuracy: 0.7979\n",
      "Epoch 101/150\n",
      "747/747 - 0s - loss: 0.4484 - accuracy: 0.8032\n",
      "Epoch 102/150\n",
      "747/747 - 0s - loss: 0.4564 - accuracy: 0.8086\n",
      "Epoch 103/150\n",
      "747/747 - 0s - loss: 0.4444 - accuracy: 0.7965\n",
      "Epoch 104/150\n",
      "747/747 - 0s - loss: 0.4584 - accuracy: 0.8059\n",
      "Epoch 105/150\n",
      "747/747 - 0s - loss: 0.4572 - accuracy: 0.8032\n",
      "Epoch 106/150\n",
      "747/747 - 0s - loss: 0.4467 - accuracy: 0.8072\n",
      "Epoch 107/150\n",
      "747/747 - 0s - loss: 0.4486 - accuracy: 0.7979\n",
      "Epoch 108/150\n",
      "747/747 - 0s - loss: 0.4405 - accuracy: 0.8072\n",
      "Epoch 109/150\n",
      "747/747 - 0s - loss: 0.4471 - accuracy: 0.8099\n",
      "Epoch 110/150\n",
      "747/747 - 0s - loss: 0.4557 - accuracy: 0.8139\n",
      "Epoch 111/150\n",
      "747/747 - 0s - loss: 0.4464 - accuracy: 0.8032\n",
      "Epoch 112/150\n",
      "747/747 - 0s - loss: 0.4445 - accuracy: 0.8072\n",
      "Epoch 113/150\n",
      "747/747 - 0s - loss: 0.4358 - accuracy: 0.8166\n",
      "Epoch 114/150\n",
      "747/747 - 0s - loss: 0.4351 - accuracy: 0.8193\n",
      "Epoch 115/150\n",
      "747/747 - 0s - loss: 0.4534 - accuracy: 0.7938\n",
      "Epoch 116/150\n",
      "747/747 - 0s - loss: 0.4421 - accuracy: 0.8032\n",
      "Epoch 117/150\n",
      "747/747 - 0s - loss: 0.4494 - accuracy: 0.8153\n",
      "Epoch 118/150\n",
      "747/747 - 0s - loss: 0.4377 - accuracy: 0.8139\n",
      "Epoch 119/150\n",
      "747/747 - 0s - loss: 0.4453 - accuracy: 0.8072\n",
      "Epoch 120/150\n",
      "747/747 - 0s - loss: 0.4411 - accuracy: 0.8166\n",
      "Epoch 121/150\n",
      "747/747 - 0s - loss: 0.4453 - accuracy: 0.7979\n",
      "Epoch 122/150\n",
      "747/747 - 0s - loss: 0.4415 - accuracy: 0.8112\n",
      "Epoch 123/150\n",
      "747/747 - 0s - loss: 0.4465 - accuracy: 0.8099\n",
      "Epoch 124/150\n",
      "747/747 - 0s - loss: 0.4469 - accuracy: 0.8032\n",
      "Epoch 125/150\n",
      "747/747 - 0s - loss: 0.4339 - accuracy: 0.8286\n",
      "Epoch 126/150\n",
      "747/747 - 0s - loss: 0.4493 - accuracy: 0.7965\n",
      "Epoch 127/150\n",
      "747/747 - 0s - loss: 0.4486 - accuracy: 0.8032\n",
      "Epoch 128/150\n",
      "747/747 - 0s - loss: 0.4449 - accuracy: 0.8086\n",
      "Epoch 129/150\n",
      "747/747 - 0s - loss: 0.4499 - accuracy: 0.7952\n",
      "Epoch 130/150\n",
      "747/747 - 0s - loss: 0.4411 - accuracy: 0.8032\n",
      "Epoch 131/150\n",
      "747/747 - 0s - loss: 0.4475 - accuracy: 0.8032\n",
      "Epoch 132/150\n",
      "747/747 - 0s - loss: 0.4424 - accuracy: 0.8059\n",
      "Epoch 133/150\n",
      "747/747 - 0s - loss: 0.4361 - accuracy: 0.8179\n",
      "Epoch 134/150\n",
      "747/747 - 0s - loss: 0.4513 - accuracy: 0.8072\n",
      "Epoch 135/150\n",
      "747/747 - 0s - loss: 0.4480 - accuracy: 0.7979\n",
      "Epoch 136/150\n",
      "747/747 - 0s - loss: 0.4353 - accuracy: 0.8019\n",
      "Epoch 137/150\n",
      "747/747 - 0s - loss: 0.4545 - accuracy: 0.7979\n",
      "Epoch 138/150\n",
      "747/747 - 0s - loss: 0.4515 - accuracy: 0.8059\n",
      "Epoch 139/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747/747 - 0s - loss: 0.4504 - accuracy: 0.7979\n",
      "Epoch 140/150\n",
      "747/747 - 0s - loss: 0.4511 - accuracy: 0.8153\n",
      "Epoch 141/150\n",
      "747/747 - 0s - loss: 0.4439 - accuracy: 0.8072\n",
      "Epoch 142/150\n",
      "747/747 - 0s - loss: 0.4449 - accuracy: 0.8139\n",
      "Epoch 143/150\n",
      "747/747 - 0s - loss: 0.4509 - accuracy: 0.8072\n",
      "Epoch 144/150\n",
      "747/747 - 0s - loss: 0.4510 - accuracy: 0.8032\n",
      "Epoch 145/150\n",
      "747/747 - 0s - loss: 0.4455 - accuracy: 0.8086\n",
      "Epoch 146/150\n",
      "747/747 - 0s - loss: 0.4406 - accuracy: 0.8032\n",
      "Epoch 147/150\n",
      "747/747 - 0s - loss: 0.4425 - accuracy: 0.8059\n",
      "Epoch 148/150\n",
      "747/747 - 0s - loss: 0.4491 - accuracy: 0.8112\n",
      "Epoch 149/150\n",
      "747/747 - 0s - loss: 0.4500 - accuracy: 0.8005\n",
      "Epoch 150/150\n",
      "747/747 - 0s - loss: 0.4520 - accuracy: 0.8099\n",
      "83/1 - 0s - loss: 0.7031 - accuracy: 0.7711\n",
      "0.8060240924358368\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=4, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_model, epochs=150, verbose=2, batch_size=20)\n",
    "scores = cross_val_score(estimator, scaled_features, classes, cv=10)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Do we have a winner?\n",
    "\n",
    "Which model, and which choice of hyperparameters, performed the best? Feel free to share your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logistic Regression: 0.80735\n",
    "# 2. Neural Network, epochs 150, batch size: 20: 0.806\n",
    "# 3. SVM, rbf kernel: 0.8012"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
